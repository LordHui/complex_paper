{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinear regression for KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dttm = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "dttm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randint(0x7fff_ffff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_state = np.random.RandomState(575_727_528)\n",
    "# random_state = np.random.RandomState()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KL-div for $\\mathbb{R}$-gaussian vi\n",
    "\n",
    "$$\n",
    "KL(\\mathcal{N}(w\\mid \\theta, \\alpha \\theta^2) \\|\n",
    "        \\tfrac1{\\lvert w \\rvert})\n",
    "    \\propto - \\tfrac12 \\log \\alpha\n",
    "      + \\mathbb{E}_{\\xi \\sim \\mathcal{N}(1, \\alpha)}\n",
    "        \\log{\\lvert \\xi \\rvert}\n",
    "%     = - \\tfrac12 \\log \\alpha\n",
    "%       + \\log{\\sqrt{\\alpha}}\n",
    "%       + \\tfrac12 \\mathbb{E}_{\\xi \\sim \\mathcal{N}(0, 1)}\n",
    "%         \\log{\\bigl\\lvert \\tfrac1{\\sqrt{\\alpha}} + \\xi \\bigr\\rvert^2}\n",
    "    = \\tfrac12 \\mathbb{E}_{\\xi \\sim \\mathcal{N}(0, 1)}\n",
    "        \\log{\\bigl\\lvert \\xi + \\tfrac1{\\sqrt{\\alpha}}\\bigr\\rvert^2}\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kldiv_real_mc(log_alpha, m=1e5):\n",
    "    kld, m = -0.5 * log_alpha, int(m)\n",
    "    for i, la in enumerate(tqdm.tqdm(log_alpha)):\n",
    "        eps = np.random.randn(m) * np.sqrt(np.exp(la))\n",
    "        kld[i] += np.log(abs(eps + 1)).mean(axis=-1)\n",
    "    return kld\n",
    "\n",
    "def kldiv_real_mc_reduced(log_alpha, m=1e5):\n",
    "    kld, m = np.zeros_like(log_alpha), int(m)\n",
    "    for i, la in enumerate(tqdm.tqdm(log_alpha)):\n",
    "        eps = np.random.randn(m)\n",
    "        kld[i] = np.log(abs(eps + np.exp(-0.5 * la))).mean(axis=-1)\n",
    "    return kld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose $q(z) = \\mathcal{CN}(\\theta, \\alpha \\lvert\\theta\\rvert^2, 0)$ and\n",
    "$p(z) \\propto \\lvert z\\rvert^{-\\beta}$. Each term in the sum\n",
    "$$\n",
    "\\begin{align}\n",
    "    KL(q\\|p)\n",
    "        &= \\mathbb{E}_{q(z)} \\log \\tfrac{q(z)}{p(z)}\n",
    "        = \\mathbb{E}_{q(z)} \\log q(z) - \\mathbb{E}_{q(z)} \\log p(z)\n",
    "%         \\\\\n",
    "%         &= - \\log \\bigl\\lvert \\pi e \\alpha \\lvert\\theta\\rvert^2 \\bigr\\rvert\n",
    "%         + \\beta \\mathbb{E}_{q(z)} \\log \\lvert z\\rvert\n",
    "%         + C\n",
    "%         \\\\\n",
    "%         &= - \\log \\pi e\n",
    "%         - \\log \\alpha \\lvert\\theta\\rvert^2\n",
    "%         + \\beta \\mathbb{E}_{\\varepsilon \\sim \\mathcal{CN}(1, \\alpha, 0)}\n",
    "%             \\log \\lvert \\theta \\rvert \\lvert \\varepsilon\\rvert\n",
    "%         + C\n",
    "%         \\\\\n",
    "%         &= - \\log \\pi e - \\log \\alpha\n",
    "%         + \\tfrac{\\beta - 2}2 \\log \\lvert \\theta \\rvert^2\n",
    "%         + \\beta \\mathbb{E}_{\\varepsilon \\sim \\mathcal{CN}(1, \\alpha, 0)}\n",
    "%             \\log \\lvert \\varepsilon\\rvert\n",
    "%         + C\n",
    "%         \\\\\n",
    "%         &= - \\log \\pi e - \\log \\alpha\n",
    "%         + \\tfrac{\\beta - 2}2 \\log \\lvert \\theta \\rvert^2\n",
    "%         + \\tfrac\\beta2 \\mathbb{E}_{z \\sim \\mathcal{CN}(0, \\alpha, 0)}\n",
    "%             \\log \\bigl\\lvert z + 1 \\bigr\\rvert^2\n",
    "%         + C\n",
    "        \\\\\n",
    "        &= - \\log \\pi e\n",
    "        + \\tfrac{\\beta - 2}2 \\log \\lvert \\theta \\rvert^2\n",
    "        + \\tfrac{\\beta-2}2 \\log\\alpha\n",
    "        + \\tfrac\\beta2 \\mathbb{E}_{z \\sim \\mathcal{CN}(0, 1, 0)}\n",
    "            \\log \\bigl\\lvert z + \\tfrac1{\\sqrt{\\alpha}} \\bigr\\rvert^2\n",
    "        + C\n",
    "        \\\\\n",
    "        &= - \\log \\pi e\n",
    "        + \\tfrac{\\beta - 2}2 \\log \\lvert \\theta \\rvert^2\n",
    "        + \\tfrac{\\beta - 2}2 \\log\\alpha\n",
    "        + \\tfrac\\beta2 \\mathbb{E}_{\\varepsilon \\sim \\mathcal{N}_2\\bigl(0, \\tfrac12 I\\bigr)}\n",
    "            \\log \\bigl((\\varepsilon_1 + \\tfrac1{\\sqrt{\\alpha}})^2 + \\varepsilon_2^2\\bigr)\n",
    "        + C\n",
    "\\end{align}\n",
    "\\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative kl-div divergence for $\\mathbb{C}$ gaussian and $\\beta=2$:\n",
    "$$\n",
    "KL(\\mathcal{N}^{\\mathbb{C}}(w\\mid \\theta, \\alpha \\lvert\\theta\\rvert^2, 0) \\|\n",
    "        \\tfrac1{\\lvert w \\rvert})\n",
    "    \\propto\n",
    "        - \\log\\alpha\n",
    "        + \\mathbb{E}_{\\xi \\sim \\mathcal{CN}(1, \\alpha, 0)}\n",
    "            \\log \\lvert \\xi \\rvert^2\n",
    "    = \\mathbb{E}_{z \\sim \\mathcal{CN}(0, 1, 0)}\n",
    "        \\log \\bigl\\lvert z + \\tfrac1{\\sqrt{\\alpha}} \\bigr\\rvert^2\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kldiv_cplx_mc(log_alpha, m=1e5):\n",
    "    kld, m = - log_alpha, int(m)\n",
    "    for i, la in enumerate(tqdm.tqdm(log_alpha)):\n",
    "        eps = np.random.randn(m) + 1j * np.random.randn(m)\n",
    "        eps *= np.sqrt(np.exp(la) / 2)\n",
    "        kld[i] += 2 * np.log(abs(eps + 1)).mean(axis=-1)\n",
    "    return kld\n",
    "\n",
    "def kldiv_cplx_mc_reduced(log_alpha, m=1e5):\n",
    "    kld, m, isq2 = np.zeros_like(log_alpha), int(m), 1/np.sqrt(2)\n",
    "    for i, la in enumerate(tqdm.tqdm(log_alpha)):\n",
    "        eps = isq2 * np.random.randn(m) + 1j * isq2 * np.random.randn(m)\n",
    "        kld[i] = 2 * np.log(abs(eps + np.exp(-0.5 * la))).mean(axis=-1)\n",
    "    return kld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact there is a \"simple\" expression for the expectation of $\n",
    "    \\log \\lvert z + \\mu \\rvert^2\n",
    "$ for $z\\sim \\mathcal{CN}(0, 1, 0)$ found here\n",
    "[The Expected Logarithm of a Noncentral Chi-Square Random Variable](http://moser-isi.ethz.ch/explog.html)\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $u \\sim \\mathcal{CN}(0, 1, 0)$ and $\\mu\\in \\mathbb{C}$ we have\n",
    "\n",
    "$$\n",
    "g(\\mu)\n",
    "    = \\mathbb{E} \\log \\lvert u + \\mu\\rvert^2\n",
    "    = \\log \\lvert \\mu \\rvert^2 - \\mathop{Ei}{(-\\lvert \\mu \\rvert^2)}\n",
    "    \\,, $$\n",
    "\n",
    "where $\n",
    "%     \\mathop{Ei}(x) = - \\int_{-x}^\\infty \\tfrac{e^{-t}}t dt\n",
    "    \\mathop{Ei}(x) = \\int_{-\\infty}^x \\tfrac{e^u}u du\n",
    "$.\n",
    "Thus for $z \\sim \\mathcal{CN}(0, \\alpha, 0)$, $\\alpha > 0$, we get\n",
    "\n",
    "$$\n",
    "\\mathbb{E} \\log \\lvert z + 1\\rvert^2\n",
    "%     = \\mathbb{E} \\log \\bigl\\lvert \\sqrt\\alpha u + 1\\bigr\\rvert^2\n",
    "    = \\mathbb{E} \\log \\alpha \\bigl\\lvert u + \\tfrac1{\\sqrt\\alpha} \\bigr\\rvert^2\n",
    "%     = \\log \\alpha + g\\bigl(\\tfrac1{\\sqrt\\alpha}\\bigr)\n",
    "%     = \\log \\alpha + \\log \\tfrac1\\alpha - \\mathop{Ei}{(-\\tfrac1\\alpha)}\n",
    "    = - \\mathop{Ei}{(-\\tfrac1\\alpha)}\n",
    "    \\,. $$\n",
    "\n",
    "Therefore\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    KL(q\\|p)\n",
    "        &= \\mathbb{E}_{q(z)} \\log \\tfrac{q(z)}{p(z)}\n",
    "        = \\mathbb{E}_{q(z)} \\log q(z) - \\mathbb{E}_{q(z)} \\log p(z)\n",
    "        \\\\\n",
    "        &= - \\log \\pi e - \\log \\alpha\n",
    "        + \\tfrac{\\beta - 2}2 \\log \\lvert \\theta \\rvert^2\n",
    "        + \\tfrac\\beta2 \\mathbb{E}_{z \\sim \\mathcal{CN}(0, \\alpha, 0)}\n",
    "            \\log \\bigl\\lvert z + 1 \\bigr\\rvert^2\n",
    "        + C\n",
    "        \\\\\n",
    "        &= - \\log \\pi e - \\log \\alpha\n",
    "        + (\\beta - 2) \\log \\lvert \\theta \\rvert\n",
    "        - \\tfrac\\beta2 \\mathop{Ei}{(-\\tfrac1\\alpha)}\n",
    "        + C\n",
    "\\end{align}\n",
    "\\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $\\beta = 2$ we get\n",
    "$$\n",
    "KL(q\\|p)\n",
    "    = C - \\log \\pi e - \\log \\alpha\n",
    "    - \\mathop{Ei}{\\bigl(-\\tfrac1\\alpha \\bigr)}\n",
    "\\,. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_div_exact(log_alpha):\n",
    "    return - log_alpha - expi(- np.exp(- log_alpha))  # - np.euler_gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differentiable exponential integral for torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from scipy.special import expi\n",
    "\n",
    "class ExpiFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        ctx.save_for_backward(x)\n",
    "\n",
    "        x_cpu = x.data.cpu().numpy()\n",
    "        output = expi(x_cpu, dtype=x_cpu.dtype)\n",
    "        return torch.from_numpy(output).to(x.device)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x = ctx.saved_tensors[-1]\n",
    "        return grad_output * torch.exp(x) / x\n",
    "\n",
    "torch_expi = ExpiFunction.apply\n",
    "\n",
    "input = torch.randn(20, 20).to(torch.double)\n",
    "assert torch.autograd.gradcheck(torch_expi, input.requires_grad_(True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loky backend seems to correctly deal with `np.random`:\n",
    "[Random state within joblib.Parallel](https://joblib.readthedocs.io/en/latest/auto_examples/parallel_random_state.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "def par_kldiv_real_mc(log_alpha, m=1e5):\n",
    "    def _kldiv_one(log_alpha):\n",
    "        eps = np.random.randn(int(m)) * np.sqrt(np.exp(log_alpha))\n",
    "        return - 0.5 * log_alpha + np.log(abs(eps + 1)).mean(axis=-1)\n",
    "    kldiv_one = joblib.delayed(_kldiv_one)\n",
    "\n",
    "    par_ = joblib.Parallel(n_jobs=-1, backend=\"loky\", verbose=0)\n",
    "    return np.array(par_(kldiv_one(la) for la in tqdm.tqdm(log_alpha)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute (or load from cache) the MC estiamte of the negative Kullback-Leibler divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import joblib\n",
    "\n",
    "filename = \"neg kl-div mc 20190516-134609.gz\"\n",
    "if os.path.exists(filename):\n",
    "    # load from cache\n",
    "    with gzip.open(filename, \"rb\") as fin:\n",
    "        cache = joblib.load(fin)\n",
    "\n",
    "    ## HERE!\n",
    "    neg_kl_real_mc, neg_kl_cplx_mc = cache[\"real\"], cache[\"cplx\"]\n",
    "    alpha = cache[\"alpha\"]\n",
    "    log_alpha = np.log(alpha)\n",
    "\n",
    "else:\n",
    "    alpha = np.logspace(-8, 8, num=4096)\n",
    "    log_alpha = np.log(alpha)\n",
    "\n",
    "    # get an MC estimate of the negative kl-divergence\n",
    "    neg_kl_real_mc = -kldiv_real_mc(log_alpha, m=1e7)\n",
    "    neg_kl_cplx_mc = -kldiv_cplx_mc(log_alpha, m=1e7)\n",
    "\n",
    "    filename = f\"neg kl-div mc {dttm}.gz\"\n",
    "    with gzip.open(filename, \"wb\", compresslevel=5) as fout:\n",
    "        joblib.dump({\n",
    "            \"m\" : 1e7,\n",
    "            \"alpha\" : alpha,\n",
    "            \"real\": neg_kl_real_mc,\n",
    "            \"cplx\": neg_kl_cplx_mc,\n",
    "        }, fout)\n",
    "# end if\n",
    "\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "100%|██████████| 513/513 [16:54<00:00,  1.93s/it]\n",
    "100%|██████████| 513/513 [39:25<00:00,  4.56s/it]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "100%|██████████| 4096/4096 [22:08<00:00,  3.25it/s]\n",
    "100%|██████████| 4096/4096 [56:05<00:00,  1.21it/s]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative kl-div approximation from [arxiv:1701.05369](https://arxiv.org/pdf/1701.05369.pdf)\n",
    "\n",
    "$$\n",
    "    - KL(\\mathcal{N}(w\\mid \\theta, \\alpha \\theta^2) \\|\n",
    "            \\tfrac1{\\lvert w \\rvert})\n",
    "        \\approx\n",
    "            k_1 \\sigma(k_2 + k_3 \\log \\alpha) + C\n",
    "            - k_4 \\log (1 + e^{-\\log \\alpha})\n",
    "        \\bigg\\vert_{C,\\, k_4 = -k_1,\\, \\tfrac12}\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "\n",
    "def np_neg_kldiv_approx(k, log_alpha):\n",
    "    k1, k2, k3, k4 = k\n",
    "    C = 0  # -k1\n",
    "\n",
    "    sigmoid = expit(k2 + k3 * log_alpha)\n",
    "    softplus = - k4 * np.logaddexp(0, -log_alpha)\n",
    "    return k1 * sigmoid + softplus + C\n",
    "\n",
    "def tr_neg_kldiv_approx(k, log_alpha):\n",
    "    k1, k2, k3, k4 = k\n",
    "    C = 0  # -k1\n",
    "\n",
    "    sigmoid = torch.sigmoid(k2 + k3 * log_alpha)\n",
    "    softplus = - k4 * F.softplus(- log_alpha)\n",
    "    return k1 * sigmoid + softplus + C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x \\mapsto \\log(1 + e^x)$ is softplus and needs different\n",
    " compute paths depending on the sign of $x$:\n",
    " $$ x\\mapsto \\log(1+e^{-\\lvert x\\rvert}) + \\max{\\{x, 0\\}} \\,. $$\n",
    "\n",
    "\n",
    "$$\n",
    "    \\log\\alpha - \\log(1 + e^{\\log\\alpha})\n",
    "        = \\log\\tfrac{\\alpha}{1 + \\alpha}\n",
    "        = - \\log\\tfrac{1 + \\alpha}{\\alpha}\n",
    "        = - \\log(1 + \\tfrac1\\alpha)\n",
    "        = - \\log(1 + e^{-\\log\\alpha})\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a curve to the MC estimate: level-grad fused function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_neg_kl_cplx_mc = torch.from_numpy(neg_kl_cplx_mc)\n",
    "tr_neg_kl_real_mc = torch.from_numpy(neg_kl_real_mc)\n",
    "tr_log_alpha = torch.from_numpy(log_alpha)\n",
    "\n",
    "def fused_mse(k, log_alpha, target):  # torch\n",
    "    tr_k = torch.from_numpy(k).requires_grad_(True)\n",
    "\n",
    "    approx = tr_neg_kldiv_approx(tr_k, log_alpha)\n",
    "\n",
    "    loss = F.mse_loss(approx, target, reduction=\"mean\")\n",
    "    loss.backward()\n",
    "\n",
    "    return loss.item(), tr_k.grad.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the mc estimate against the exact value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid = (- kl_div_exact(log_alpha)) - neg_kl_cplx_mc\n",
    "plt.plot(log_alpha, resid)\n",
    "\n",
    "abs(resid).mean(), resid.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's find the optimal nonlinear regresson fit using L-BFGS:\n",
    "$$\n",
    "\\frac12 \\sum_i \\bigl(\n",
    "    y_i - f(\\log \\alpha_i, \\theta)\n",
    "\\bigr)^2 \\longrightarrow \\min_\\theta\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize.lbfgsb import fmin_l_bfgs_b\n",
    "\n",
    "k_real = fmin_l_bfgs_b(fused_mse, np.r_[0.5, 0., 1., 0.5],\n",
    "                       bounds=((None, None), (None, None), (None, None), (0.5, 0.5)),\n",
    "                       args=(tr_log_alpha, tr_neg_kl_real_mc))[0]\n",
    "\n",
    "k_cplx = fmin_l_bfgs_b(fused_mse, np.r_[0.5, 0., 1., 1.],\n",
    "                       bounds=((None, None), (None, None), (None, None), (1.0, 1.0)),\n",
    "                       args=(tr_log_alpha, tr_neg_kl_cplx_mc))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit coefficients from the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_real_1701_05369 = np.r_[0.63576, 1.87320, 1.48695, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k_real_1701_05369, k_real, k_cplx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "```text\n",
    "(array([0.63576, 1.8732 , 1.48695, 0.5    ]),\n",
    " array([0.63567313, 1.88114543, 1.49136378, 0.5       ]),\n",
    " array([0.57810091, 1.45926293, 1.36525956, 1.        ]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_kl_real_1701_05369 = np_neg_kldiv_approx(k_real_1701_05369, log_alpha)\n",
    "neg_kl_real_approx = np_neg_kldiv_approx(k_real, log_alpha)\n",
    "neg_kl_cplx_approx = np_neg_kldiv_approx(k_cplx, log_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 5))\n",
    "ax = fig.add_subplot(111, xlabel=r\"$\\log\\alpha$\", ylabel=\"-kld\")\n",
    "\n",
    "ax.plot(log_alpha, neg_kl_real_mc - neg_kl_real_1701_05369, label=r\"$\\mathbb{R}$ - arXiv:1701.05369\", alpha=0.5)\n",
    "ax.plot(log_alpha, neg_kl_real_mc - neg_kl_real_approx, label=r\"$\\mathbb{R}$ - lbfgs\", alpha=0.5)\n",
    "ax.plot(log_alpha, neg_kl_cplx_mc - neg_kl_cplx_approx, label=r\"$\\mathbb{C}$ - lbfgs\")\n",
    "\n",
    "ax.axhline(0., c=\"k\", zorder=-10)\n",
    "ax.legend(ncol=3)\n",
    "ax.set_title(\"Regression residuals of the MC estimate of the KL-div for ARD\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 5))\n",
    "ax = fig.add_subplot(111, xlabel=r\"$\\log\\alpha$\", ylabel=\"-kld\")\n",
    "\n",
    "ax.plot(log_alpha, -kl_div_exact(log_alpha), c=\"k\", label=r\"$\\mathbb{C}$ - exact\", lw=2)\n",
    "\n",
    "ax.plot(log_alpha, neg_kl_real_mc, label=r\"$\\mathbb{R}$\")\n",
    "ax.plot(log_alpha, neg_kl_cplx_mc, label=r\"$\\mathbb{C}$\")\n",
    "\n",
    "ax.legend(ncol=3)\n",
    "ax.set_title(\"the MC estimate of the KL-div for ARD\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, really close in unform norm ($\\|\\cdot\\|_\\infty$ on $C^1(\\mathbb{R})$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(neg_kl_real_mc - neg_kl_real_1701_05369).max(), \\\n",
    "abs(neg_kl_real_mc - neg_kl_real_approx).max(), \\\n",
    "abs(neg_kl_cplx_mc - neg_kl_cplx_approx).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 5))\n",
    "ax = fig.add_subplot(111, xlabel=r\"$\\log\\alpha$\", ylabel=\"-kld\")\n",
    "\n",
    "ax.plot(log_alpha, neg_kl_cplx_mc, label=r\"$\\mathbb{C}$ - mc\")\n",
    "ax.plot(log_alpha, neg_kl_cplx_approx, label=r\"$\\mathbb{C}$ - approx\")\n",
    "\n",
    "ax.plot(log_alpha, neg_kl_real_mc, label=r\"$\\mathbb{R}$ - mc\")\n",
    "ax.plot(log_alpha, neg_kl_real_approx, label=r\"$\\mathbb{R}$ - approx\")\n",
    "\n",
    "# ax.set_xlim(0, 20)\n",
    "# ax.set_ylim(0.55, 0.6)\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact kl-div grad for $\\mathbb{R}$-gaussian (draft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note this paper here\n",
    "[Moments of the log non-central chi-square distribution](https://arxiv.org/pdf/1503.06266.pdf)\n",
    "correctly notices that on\n",
    "[p. 2446 of Lapidoth, Moser (2003)](http://moser-isi.ethz.ch/docs/papers/alap-smos-2003-3.pdf)\n",
    "there is a missing $\\log{2}$ term. In the following analysis resembles the logic of this paper. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $(z_i)_{i=1}^m \\sim \\mathcal{N}(0, 1)$ iid and $\n",
    "  (\\mu_i)_{i=1}^m \\in \\mathbb{R}\n",
    "$. The random variable $\n",
    "  W = \\sum_i (\\mu_i + z_i)^2\n",
    "$ is said to be $\\chi^2_m(\\lambda)$ distributed (noncentral $\\chi^2$)\n",
    "with noncentrality parameter $\\lambda = \\sum_i \\mu_i^2$.\n",
    "\n",
    "Consider the mgf of $W$:\n",
    "\n",
    "$$\n",
    "M_W(t)\n",
    "    = \\mathbb{E}(e^{Wt})\n",
    "    = \\prod_i \\mathbb{E}(e^{(\\mu_i + z_i)^2 t})\n",
    "    \\,, $$\n",
    "\n",
    "by independence. Now for $z \\sim \\mathcal{N}(\\mu, 1)$\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(e^{z^2 t})\n",
    "    = \\tfrac1{\\sqrt{2\\pi}}\n",
    "    \\int_{-\\infty}^{+\\infty}\n",
    "        e^{z^2 t} e^{-\\tfrac{(z-\\mu)^2}2}\n",
    "    dz\n",
    "    \\,. $$\n",
    "\n",
    "Now, for $t < \\tfrac12$\n",
    "\n",
    "$$\n",
    "z^2 t - \\tfrac{(z - \\mu)^2}2\n",
    "    = - \\tfrac12 (1 - 2t) z^2 + z \\mu - \\tfrac{\\mu^2}2\n",
    "%     = - \\tfrac12 (1 - 2t) \\bigl(\n",
    "%         z^2 - 2 z \\tfrac\\mu{1 - 2t} + \\tfrac{\\mu^2}{1 - 2t}\n",
    "%     \\bigr)\n",
    "%     = - \\tfrac12 (1 - 2t) \\bigl( z - \\tfrac\\mu{1 - 2t} \\bigr)^2\n",
    "%     - \\tfrac12 (1 - 2t) \\bigl(\n",
    "%         \\tfrac{\\mu^2}{1 - 2t}\n",
    "%         - \\tfrac{\\mu^2}{(1 - 2t)^2}\n",
    "%     \\bigr)\n",
    "%     = - \\tfrac12 (1 - 2t) \\bigl( z - \\tfrac\\mu{1 - 2t} \\bigr)^2\n",
    "%     - \\tfrac{\\mu^2}2 \\bigl(\n",
    "%         \\tfrac{1 - 2t}{1 - 2t} - \\tfrac1{1 - 2t}\n",
    "%     \\bigr)\n",
    "%     = - \\tfrac12 (1 - 2t) \\bigl( z - \\tfrac\\mu{1 - 2t} \\bigr)^2\n",
    "%     + \\tfrac{\\mu^2}2 \\tfrac{2t}{1 - 2t}\n",
    "    = - \\tfrac12 (1 - 2t) \\bigl( z - \\tfrac\\mu{1 - 2t} \\bigr)^2\n",
    "    + \\mu^2 \\tfrac{t}{1 - 2t}\n",
    "    \\,, $$\n",
    "\n",
    "whence\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(e^{z^2 t})\n",
    "    = \\tfrac1{\\sqrt{2\\pi}}\n",
    "    \\int_{-\\infty}^{+\\infty}\n",
    "        e^{z^2 t} e^{-\\tfrac{(z-\\mu)^2}2}\n",
    "    dz\n",
    "%     = e^{\\mu^2 \\tfrac{t}{1 - 2t}}\n",
    "%     \\tfrac1{\\sqrt{2\\pi}}\n",
    "%     \\int_{-\\infty}^{+\\infty}\n",
    "%         e^{- \\tfrac12 (1 - 2t) \\bigl( z - \\tfrac\\mu{1 - 2t} \\bigr)^2}\n",
    "%     dz\n",
    "    = e^{\\mu^2 \\tfrac{t}{1 - 2t}}\n",
    "    \\tfrac1{\\sqrt{2\\pi}}\n",
    "    \\int_{-\\infty}^{+\\infty}\n",
    "        e^{- \\tfrac12 (1 - 2t) z^2}\n",
    "    dz\n",
    "%     = [u = \\sqrt{1 - 2t} z]\n",
    "%     = e^{\\mu^2 \\tfrac{t}{1 - 2t}}\n",
    "%     \\tfrac1{\\sqrt{2\\pi}}\n",
    "%     \\int_{-\\infty}^{+\\infty}\n",
    "%         e^{- \\tfrac12 u^2}\n",
    "%     \\tfrac{du}{\\sqrt{1 - 2t}}\n",
    "    = \\tfrac{\n",
    "        \\exp{\\{\\mu^2 \\tfrac{t}{1 - 2t}\\}}\n",
    "    }{\\sqrt{1 - 2t}}\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore \n",
    "\n",
    "$$\n",
    "M_W(t)\n",
    "    = \\mathbb{E}(e^{Wt})\n",
    "    = \\prod_i \\tfrac{\n",
    "        e^{\\mu_i^2 \\tfrac{t}{1 - 2t}}\n",
    "    }{\\sqrt{1 - 2t}}\n",
    "%     = e^{\\lambda \\tfrac{t}{1 - 2t}}\n",
    "%     (1 - 2t)^{-\\tfrac{m}2}\n",
    "%     = e^{\\lambda \\tfrac{- t}{2t - 1}}\n",
    "%     (1 - 2t)^{-\\tfrac{m}2}\n",
    "%     = e^{\\tfrac\\lambda2 \\tfrac{1 - 2t - 1}{2t - 1}}\n",
    "%     (1 - 2t)^{-\\tfrac{m}2}\n",
    "%     = e^{- \\tfrac\\lambda2 (1 + \\tfrac1{2t - 1})}\n",
    "%     (1 - 2t)^{-\\tfrac{m}2} \n",
    "    = e^{- \\tfrac\\lambda2} e^{\\tfrac\\lambda2 \\tfrac1{1 - 2t}}\n",
    "    (1 - 2t)^{-\\tfrac{m}2}\n",
    "    \\,. $$\n",
    "\n",
    "Expanding the exponential as infinte series:\n",
    "\n",
    "$$\n",
    "M_W(t)\n",
    "    = e^{- \\tfrac\\lambda2} e^{\\tfrac\\lambda2 \\tfrac1{1 - 2t}}\n",
    "    (1 - 2t)^{-\\tfrac{m}2}\n",
    "%     = e^{- \\tfrac\\lambda2} (1 - 2t)^{-\\tfrac{m}2}\n",
    "%     \\sum_{n \\geq 0} \\tfrac{\\bigl(\\tfrac\\lambda2\\bigr)^n}{n! (1 - 2t)^n}\n",
    "    = \\sum_{n \\geq 0} \\tfrac{e^{- \\tfrac\\lambda2} \\bigl(\\tfrac\\lambda2\\bigr)^n}{n!}\n",
    "        (1 - 2t)^{-\\tfrac{2n + m}2}\n",
    "    = \\sum_{n \\geq 0} \\tfrac{e^{- \\tfrac\\lambda2} \\bigl(\\tfrac\\lambda2\\bigr)^n}{n!}\n",
    "        \\mathbb{E}_{x \\sim \\chi^2_{m + 2n}}(e^{x t})\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus (really? this is how we derive this?!) the density of a non-central $\\chi^2_m(\\lambda)$ is given by\n",
    "\n",
    "$$\n",
    "f_W(x)\n",
    "    = e^{- \\tfrac\\lambda2} \\sum_{n \\geq 0} \\tfrac{\\bigl(\\tfrac\\lambda2\\bigr)^n}{n!}\n",
    "        \\tfrac{\n",
    "            x^{n + \\tfrac{m}2 - 1} e^{-\\tfrac{x}2}\n",
    "        }{\n",
    "            2^{n + \\tfrac{m}2} \\Gamma(n + \\tfrac{m}2)\n",
    "        }\n",
    "%     = e^{- \\tfrac\\lambda2} \\sum_{n \\geq 0} \\tfrac{\\bigl(\\tfrac\\lambda2\\bigr)^n}{n!}\n",
    "%         \\tfrac{\n",
    "%             \\bigl(\\tfrac{x}2\\bigr)^{n + \\tfrac{m}2 - 1} e^{-\\tfrac{x}2}\n",
    "%         }{\n",
    "%             2 \\Gamma(n + \\tfrac{m}2)\n",
    "%         }\n",
    "    = \\frac12 e^{- \\tfrac{x + \\lambda}2} \\bigl(\\tfrac{x}2\\bigr)^{\\tfrac{m}2 - 1}\n",
    "    \\sum_{n \\geq 0} \\tfrac{\n",
    "            \\bigl(\\tfrac{x \\lambda}4\\bigr)^n\n",
    "        }{\n",
    "            n! \\Gamma(n + \\tfrac{m}2)\n",
    "        }\n",
    "%     = \\frac12 e^{- \\tfrac{x + \\lambda}2}\n",
    "%     \\bigl(\\tfrac{x}\\lambda\\bigr)^{\\tfrac{m}4 - \\tfrac12}\n",
    "%     \\bigl(\\tfrac{x \\lambda}4\\bigr)^{\\tfrac{m}4 - \\tfrac12}\n",
    "%     \\sum_{n \\geq 0} \\tfrac{\n",
    "%             \\bigl(\\tfrac{x \\lambda}4\\bigr)^n\n",
    "%         }{\n",
    "%             n! \\Gamma(n + \\tfrac{m}2)\n",
    "%         }\n",
    "%     = \\frac12 e^{- \\tfrac{x + \\lambda}2}\n",
    "%     \\bigl(\\tfrac{x}\\lambda\\bigr)^{\\tfrac{m}4 - \\tfrac12}\n",
    "%     \\bigl(\\tfrac{\\sqrt{x \\lambda}}2\\bigr)^{\\tfrac{m}2 - 1}\n",
    "%     \\sum_{n \\geq 0} \\tfrac{\n",
    "%             \\bigl(\\tfrac{x \\lambda}4\\bigr)^n\n",
    "%         }{\n",
    "%             n! \\Gamma(n + \\tfrac{m}2)\n",
    "%         }\n",
    "    = \\frac12 e^{- \\tfrac{x + \\lambda}2}\n",
    "    \\bigl(\\tfrac{x}\\lambda\\bigr)^{\\tfrac{m - 2}4}\n",
    "    I_{\\bigl(\\tfrac{m}2 - 1\\bigr)}(\\sqrt{x \\lambda})\n",
    "\\,, $$\n",
    "\n",
    "where $I_k$ is the modified Bessel function of the first kind\n",
    "\n",
    "$$\n",
    "I_k(s)\n",
    "    = \\Bigl(\\frac{s}2\\Bigr)^k\n",
    "        \\sum_{n \\geq 0} \\tfrac{\n",
    "            \\bigl( \\tfrac{s}2 \\bigr)^{2n}\n",
    "        }{n! \\Gamma(n + k + 1)}\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected logarithm of $W$ is\n",
    "$$\n",
    "\\mathbb{E}_{W\\sim \\chi^2_m(\\lambda)} \\log W\n",
    "    = \\int_0^\\infty f_W(x) \\log x dx\n",
    "%     = \\frac12 e^{- \\tfrac\\lambda2}\n",
    "%     \\int_0^\\infty \\sum_{n \\geq 0} \\biggl(\n",
    "%         \\tfrac{\\bigl(\\tfrac\\lambda2\\bigr)^n}{n!}\n",
    "%         \\tfrac{e^{- \\tfrac{x}2}}{\\Gamma(n + \\tfrac{m}2)}\n",
    "%         \\bigl(\\tfrac{x}2\\bigr)^{n + \\tfrac{m}2 - 1}\n",
    "%     \\biggr) \\log x dx\n",
    "%     = [\\text{ absolute summability and Fubini, or any other conv. thm for integrals of non-negative integrals}]\n",
    "%     = e^{- \\tfrac\\lambda2} \\sum_{n \\geq 0}\n",
    "%         \\tfrac{\\bigl(\\tfrac\\lambda2\\bigr)^n}{n!}\n",
    "%         \\tfrac1{\\Gamma(n + \\tfrac{m}2)}\n",
    "%     \\int_0^\\infty\n",
    "%         e^{- \\tfrac{x}2} \\bigl(\\tfrac{x}2\\bigr)^{n + \\tfrac{m}2 - 1}\n",
    "%         (\\log2 + \\log \\tfrac{x}2) \\tfrac{dx}2\n",
    "%     = [u = \\tfrac{x}2]\n",
    "%     = e^{- \\tfrac\\lambda2} \\sum_{n \\geq 0}\n",
    "%         \\tfrac{\\bigl(\\tfrac\\lambda2\\bigr)^n}{n!}\n",
    "%     \\tfrac1{\\Gamma(n + \\tfrac{m}2)}\n",
    "%     \\int_0^\\infty\n",
    "%         e^{- u} u^{n + \\tfrac{m}2 - 1}\n",
    "%         (\\log2 + \\log u) du\n",
    "%     = [\\text{definitions:} \\Gamma, \\psi]\n",
    "%     = e^{- \\tfrac\\lambda2} \\sum_{n \\geq 0}\n",
    "%         \\tfrac{\\bigl(\\tfrac\\lambda2\\bigr)^n}{n!}\n",
    "%     (\\psi(n + \\tfrac{m}2) + \\log2)\n",
    "    = \\log{2}\n",
    "    + e^{- \\tfrac\\lambda2} \\sum_{n \\geq 0}\n",
    "        \\tfrac{\\bigl(\\tfrac\\lambda2\\bigr)^n}{n!}\n",
    "        \\psi(n + \\tfrac{m}2)\n",
    "  \\,, $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out the expectation $\n",
    "  \\mathbb{E}_{z \\sim \\mathcal{N}(0, 1)}\n",
    "    \\log{\\bigl\\lvert z + \\tfrac1{\\sqrt{\\alpha}} \\bigr\\rvert^2}\n",
    "$ is equal to $\n",
    "  \\log{2} + g_1(\\tfrac1{2\\alpha})\n",
    "$ where\n",
    "\n",
    "$$\n",
    "g_m(x)\n",
    "%     = e^{-x} \\sum_{n \\geq 0} \\frac{x^n}{n! \\Gamma(n + \\tfrac{m}2)}\n",
    "%         \\int_0^\\infty e^{-t} t^{n + \\tfrac{m}2-1} \\log{t} dt\n",
    "% e^{-x} \\sum_{n=0}^{\\infty} \\frac{x^n}{n!} \\psi(n + m / 2)\n",
    "    = e^{-x} \\sum_{n \\geq 0} \\frac{x^n}{n!} \\psi(n + \\tfrac{m}2)\n",
    "    \\,, $$\n",
    "\n",
    "and $\\psi(x)$ is the digamma function, i.e. $\n",
    "    \\psi(x) = \\tfrac{d}{dx} \\Gamma(x)\n",
    "$. The digamma function has the following useful properties:\n",
    "$\n",
    "    \\psi(z+1) = \\psi(z) + \\tfrac1z\n",
    "$ and $\n",
    "    \\psi(\\tfrac12) = -\\gamma - 2\\log 2\n",
    "$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differentiating the series within $g_m$ yields:\n",
    "$$\n",
    "\\frac{d}{d x} g_m(x)\n",
    "    = e^{-x} \\sum_{n\\geq 1} \\frac{x^{n-1}}{(n-1)!} \\psi(n + \\tfrac{m}2) - g_m(x)\n",
    "%     = e^{-x} \\sum_{n\\geq 0} \\frac{x^n}{n!} \\psi(n + \\tfrac{m}2 + 1) - g_m(x)\n",
    "%     = e^{-x} \\sum_{n\\geq 0} \\frac{x^n}{n!} (\n",
    "%         \\psi(n + \\tfrac{m}2 + 1) - \\psi(n + \\tfrac{m}2)\n",
    "%     )\n",
    "%     = e^{-x} \\sum_{n\\geq 0} \\frac{x^n}{n!} \\tfrac1{n + \\tfrac{m}2}\n",
    "    = e^{-x} \\sum_{n\\geq 0} \\frac{x^n}{n!} (n + \\tfrac{m}2)^{-1}\n",
    "    = \\cdots\n",
    "    \\,. $$\n",
    "\n",
    "We can differentiate the series in $g_m(x)$, since the sum converges everywhere\n",
    "on $\\mathbb{R}$. Indeed, it is a power series featuring nonegative coefficients,\n",
    "which is dominated by $\n",
    "  \\sum_{n \\geq 1} \\tfrac{x^n}{n!} \\log{(n+\\tfrac{m}2)}\n",
    "$, because $\\psi(x)\\leq \\log x - \\tfrac1{2x}$. By the ratio test, the dominating\n",
    "series has infinite radius:\n",
    "\n",
    "$$\n",
    "\\lim_{n\\to\\infty}\n",
    "    \\biggl\\lvert\n",
    "        \\frac{\n",
    "            n! x^{n+1} \\log{(n + 1 + \\tfrac{m}2)}\n",
    "        }{\n",
    "            x^n \\log{(n + \\tfrac{m}2)} (n+1)!\n",
    "        }\n",
    "    \\biggr\\rvert\n",
    "    = \\lim_{n\\to\\infty}\n",
    "        \\lvert x \\rvert \n",
    "        \\biggl\\lvert\n",
    "            \\frac{\n",
    "                \\log{(n + 1 + \\tfrac{m}2)}\n",
    "            }{\n",
    "                \\log{(n + \\tfrac{m}2)} (n+1)\n",
    "            }\n",
    "        \\biggr\\rvert\n",
    "%     = \\lim_{n\\to\\infty}\n",
    "%         \\lvert x \\rvert \n",
    "%         \\biggl\\lvert\n",
    "%             \\frac{n + \\tfrac{m}2}{\n",
    "%                 (n + 1 + \\tfrac{m}2)((n+1) + (n + \\tfrac{m}2) \\log{(n + \\tfrac{m}2)})\n",
    "%             }\n",
    "%         \\biggr\\rvert\n",
    "    = 0 < 1\n",
    "    \\,. $$\n",
    "\n",
    "Since\n",
    "$$\n",
    "\\frac{\\log{x + a + 1}}{x \\log{x + a}}\n",
    "    \\sim \\frac{\\log{x+1}}{(x-a) \\log{x}}\n",
    "    \\sim \\frac{\\log{x+1}}{x \\log{x}}\n",
    "    \\sim \\frac{\\tfrac1{x+1}}{1 + \\log{x}}\n",
    "    \\to 0\n",
    "    \\,. $$\n",
    "\n",
    "A theroem from calculus states, that the formal series derivative (integral)\n",
    "coincides with the derivative (integral) of the function, corresponding to\n",
    "the power series (everywhere on the convergence region). And the convergence\n",
    "regions of derivative (integral) concide with the region of the original\n",
    "power series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, observe that $\n",
    "    e^t  = \\sum_{n\\geq 0} \\tfrac{t^n}{n!}\n",
    "$ on $\\mathbb{R}$, for $\\alpha\\neq 0$ we have $\n",
    "    \\int_0^x t^{\\alpha-1} dt\n",
    "        = \\tfrac{x^\\alpha}{\\alpha}\n",
    "$ and that\n",
    "\n",
    "$$\n",
    "\\sum_{n\\geq 0} \\int_0^x \\frac{t^{n+\\alpha-1}}{n!} dt\n",
    "    = \\int_0^x \\sum_{n\\geq 0} \\frac{t^{n+\\alpha-1}}{n!} dt\n",
    "    = \\int_0^x t^{\\alpha-1} e^t dt\n",
    "    \\,. $$\n",
    "\n",
    "by (MCT) on $\n",
    "    ([0, x], \\mathcal{B}([0, x]), dx)\n",
    "$ whence\n",
    "\n",
    "$$\n",
    "\\cdots\n",
    "    = x^{-\\tfrac{m}2} e^{-x} \\sum_{n\\geq 0}\n",
    "        \\frac{x^{n + \\tfrac{m}2}}{n!} (n + \\tfrac{m}2)^{-1}\n",
    "%     = x^{-\\tfrac{m}2} e^{-x} \\sum_{n\\geq 0}\n",
    "%         \\frac1{n!} \\int_0^x t^{n + \\tfrac{m}2 - 1} dt\n",
    "%     = x^{-\\tfrac{m}2} e^{-x}\n",
    "%         \\int_0^x t^{\\tfrac{m}2 - 1} \\sum_{n\\geq 0} \\frac{t^n}{n!} dt\n",
    "    = x^{-\\tfrac{m}2} e^{-x}\n",
    "        \\int_0^x t^{\\tfrac{m}2 - 1} e^t dt\n",
    "    = \\cdots\n",
    "    \\,. $$\n",
    "\n",
    "Using $u^2 = t$ on $[0, \\infty]$ we get $2u du = dt$ and \n",
    "\n",
    "$$\n",
    "\\int_0^x t^{\\tfrac{m}2 - 1} e^t  dt\n",
    "%     = \\int_0^{\\sqrt{x}} u^{m - 2} e^{u^2} 2 u du\n",
    "    = 2 \\int_0^{\\sqrt{x}} u^{m - 1} e^{u^2} du\n",
    "    \\,.$$\n",
    "\n",
    "Therefore\n",
    "\n",
    "$$\n",
    "\\frac{d}{d x} g_m(x)\n",
    "%     = x^{-\\tfrac{m}2} e^{-x}\n",
    "%         \\int_0^x t^{\\tfrac{m}2 - 1} e^t dt\n",
    "    = 2 x^{-\\tfrac{m}2} e^{-x}\n",
    "        \\int_0^{\\sqrt{x}} u^{m - 1} e^{u^2} du\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $m=1$\n",
    "\n",
    "$$\n",
    "g_1(x)\n",
    "    = - \\gamma - 2 \\log{2}\n",
    "    + e^{-x} \\sum_{n\\geq 0} \\frac{x^n}{n!} \\sum_{p=1}^n \\frac2{2p - 1}\n",
    "    \\,, $$\n",
    "\n",
    "and the derivative can be computed thus \n",
    "\n",
    "$$\n",
    "\\frac{d}{d x} g_1(x)\n",
    "    = 2 \\tfrac{F(\\sqrt{x})}{\\sqrt{x}}\n",
    "    \\,, $$\n",
    "\n",
    "using Dawson's integral, $\n",
    "    F\\colon \\mathbb{R} \\to \\mathbb{R}\n",
    "    \\colon x \\mapsto e^{-x^2} \\int_0^x e^{u^2} du\n",
    "$, which exists as a special function (in `scipy`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in SGD (and specifically in SGVB) we are concerned more with\n",
    "the gradient field, induced by the potential (which is the loss\n",
    "function), rather than the value itself. Thus, as far as the regularizing\n",
    "penalty term is concerned, which is used to regularize the loss\n",
    "objective, we can essentially ignore it's forward pass value (level)\n",
    "and just compute its gradient (subgradient, normal cone) with respect\n",
    "the parameter of interest in sgd. (unless it is a part of a constraint,\n",
    "ie. downstream computation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative wrt $\\alpha$ is \n",
    "$$\n",
    "\\tfrac{d}{d \\alpha} g_1(\\tfrac1{2\\alpha})\n",
    "    = -\\tfrac1{2 \\alpha^2} g_1'(\\tfrac1{2\\alpha})\n",
    "%     = -\\tfrac1{2 \\alpha^2} 2 \\tfrac{F(\\sqrt{\\tfrac1{2\\alpha}})}{\\sqrt{\\tfrac1{2\\alpha}}}\n",
    "%     = -\\tfrac1{2 \\alpha^2} 2 \\tfrac{F(\\tfrac1{\\sqrt{2\\alpha}})}{\\tfrac1{\\sqrt{2\\alpha}}}\n",
    "    = -\\tfrac1{\\alpha} \\sqrt{\\tfrac2{\\alpha}} F(\\tfrac1{\\sqrt{2\\alpha}})\n",
    "    \\,. $$\n",
    "\n",
    "Since $\\alpha$ is nonegative, it it typically parametereized through its\n",
    "logarithm and computed when needed. Thus in particular the gradient of\n",
    "the divergence penalty w.r.t $\\log \\alpha$ is\n",
    "$$\n",
    "\\frac{d}{d\\log \\alpha}\n",
    "    \\tfrac12 \\mathbb{E}_{z \\sim \\mathcal{N}(0,1)}\n",
    "    \\log \\lvert z + \\tfrac1{\\sqrt{\\alpha}} \\rvert^2\n",
    "    = \\frac12 \\frac{d\\alpha}{d\\log \\alpha}\n",
    "        \\frac{d}{d\\alpha} \\bigl(\\mathbb{E}\\cdots \\bigr)\n",
    "    = - \\frac\\alpha2 \\tfrac1{\\alpha}\n",
    "        \\sqrt{\\tfrac2{\\alpha}} F(\\tfrac1{\\sqrt{2\\alpha}})\n",
    "    = - \\tfrac1{\\sqrt{2\\alpha}} F(\\tfrac1{\\sqrt{2\\alpha}})\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import dawsn\n",
    "\n",
    "def kl_real_deriv(log_alpha):\n",
    "    tmp = np.exp(- 0.5 * (log_alpha + np.log(2)))\n",
    "    return -dawsn(tmp) * tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $\\beta = 2$ we get\n",
    "$$\n",
    "KL(q\\|p)\n",
    "    = C - \\log \\pi e - \\log \\alpha\n",
    "    - \\mathop{Ei}{\\bigl(-\\tfrac1\\alpha \\bigr)}\n",
    "\\,. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_cplx_deriv(log_alpha):\n",
    "    return -1 + np.exp(-np.exp(-log_alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{d}{d y} \\mathop{Ei}(-e^{-y})\n",
    "    = \\frac{d}{d x} \\mathop{Ei}(x) \\bigg\\vert_{x=-e^{-y}}\n",
    "    \\frac{d(-e^{-y})}{d y}\n",
    "    = \\frac{e^x}{x} \\bigg\\vert_{x=-e^{-y}} e^{-y}\n",
    "    = - e^{-e^{-y}}\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use autograd to get the derivative of the approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_kldiv_approx_deriv(log_alpha, k):\n",
    "    k, x = map(torch.from_numpy, (k, log_alpha))\n",
    "\n",
    "    x.requires_grad_(True)\n",
    "    kldiv = tr_neg_kldiv_approx(k, x).sum()\n",
    "    grad, = torch.autograd.grad(kldiv, x, grad_outputs=torch.tensor(1.).to(x))\n",
    "\n",
    "    return log_alpha, grad.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's estimate the derivative wrt $\\log\\alpha$ using symmetric differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symm_diff(x, y):\n",
    "    \"\"\"Symmetric difference derivative approximation.\n",
    "\n",
    "    Assumes x_i is sorted (axis=0), y_i = y(x_i).\n",
    "    \"\"\"\n",
    "    return x[1:-1], (y[2:] - y[:-2]) /(x[2:] - x[:-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Darken a given colour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import to_rgb\n",
    "from colorsys import rgb_to_hls, hls_to_rgb\n",
    "\n",
    "def darker(color, a=0.5):\n",
    "    \"\"\"Adapted from this stackoverflow question_.\n",
    "    .. _question: https://stackoverflow.com/questions/37765197/\n",
    "    \"\"\"\n",
    "\n",
    "    h, l, s = rgb_to_hls(*to_rgb(color))\n",
    "    return hls_to_rgb(h, max(0, min(a * l, 1)), s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the `numerical` derivative of the MC estimate, the exact derivative and\n",
    "the derivative of the fit approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 5))\n",
    "ax = fig.add_subplot(111, xlabel=r\"$\\log\\alpha$\", ylabel=\"-kld\")\n",
    "\n",
    "# plot kl-real\n",
    "line, = ax.plot(*neg_kldiv_approx_deriv(log_alpha, k_real_1701_05369),\n",
    "                label=r\"$\\partial {KL}_\\mathbb{R}$-approx\",\n",
    "                linestyle=\"-\", c=\"red\")\n",
    "\n",
    "color, zorder = darker(line.get_color(), .75), line.get_zorder()\n",
    "ax.plot(log_alpha, -kl_real_deriv(log_alpha),\n",
    "        label=r\"$\\partial {KL}_\\mathbb{R}$-exact\",\n",
    "        linestyle=\"--\", c=color, zorder=zorder + 5, lw=3, alpha=0.5)\n",
    "\n",
    "color = darker(line.get_color(), 1.5)\n",
    "ax.plot(*symm_diff(log_alpha, neg_kl_real_mc),\n",
    "        label=r\"$\\Delta {KL}_\\mathbb{R}$-MC\",\n",
    "        c=color, zorder=zorder - 5, alpha=0.5)\n",
    "\n",
    "\n",
    "# plot kl-cplx\n",
    "line, = ax.plot(*neg_kldiv_approx_deriv(log_alpha, k_cplx),\n",
    "                label=r\"$\\partial {KL}_\\mathbb{C}$-approx\",\n",
    "                linestyle=\"-\", c=\"blue\")\n",
    "\n",
    "color, zorder = darker(line.get_color(), .75), line.get_zorder()\n",
    "ax.plot(log_alpha, -kl_cplx_deriv(log_alpha),\n",
    "        label=r\"$\\partial {KL}_\\mathbb{C}$-exact\",\n",
    "        linestyle=\"--\", c=color, zorder=zorder + 5, lw=3, alpha=0.5)\n",
    "\n",
    "color = darker(line.get_color(), 1.5)\n",
    "ax.plot(*symm_diff(log_alpha, neg_kl_cplx_mc),\n",
    "        label=r\"$\\Delta {KL}_\\mathbb{C}$-MC\",\n",
    "        c=color, zorder=zorder - 5, alpha=0.5)\n",
    "\n",
    "# ax.axhline(0., c=\"k\", zorder=-10, alpha=0.15)\n",
    "# ax.axhline(1., c=\"k\", zorder=-10, alpha=0.15)\n",
    "ax.legend(ncol=2)\n",
    "\n",
    "ax.set_xlim(-8, +8)\n",
    "\n",
    "fig.savefig(\"./assets/grad_log.pdf\", dpi=300, format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolute difference for the exact and approximation's derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 5))\n",
    "ax = fig.add_subplot(111, xlabel=r\"$\\log\\alpha$\", ylabel=\"-kld\")\n",
    "\n",
    "ax.plot(log_alpha, abs(\n",
    "        kldiv_approx_deriv(log_alpha, k_real_1701_05369)\n",
    "        - (-kl_real_deriv(log_alpha))\n",
    "    ), label=r\"$\\partial {KL}_\\mathbb{R}$: approx - exact\")\n",
    "\n",
    "ax.plot(log_alpha, abs(\n",
    "        kldiv_approx_deriv(log_alpha, k_cplx)\n",
    "        - (-kl_cplx_deriv(log_alpha))\n",
    "    ), label=r\"$\\partial {KL}_\\mathbb{C}$: approx - exact\")\n",
    "\n",
    "ax.axhline(0., c=\"k\", zorder=-10)\n",
    "ax.legend(ncol=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 5))\n",
    "ax = fig.add_subplot(111, xlabel=r\"$\\log\\alpha$\", ylabel=\"-kld\")\n",
    "\n",
    "# mid_log_alpha = (log_alpha[1:] + log_alpha[:-1]) / 2\n",
    "# d_log = log_alpha[1:] - log_alpha[:-1]\n",
    "\n",
    "ax.plot(*symm_diff(log_alpha, neg_kl_real_mc), alpha=0.5,\n",
    "        label=r\"$\\partial {KL}_\\mathbb{R}$-MC symm.d\")\n",
    "ax.plot(log_alpha, -kl_real_deriv(log_alpha), label=r\"$\\partial {KL}_\\mathbb{R}$-exact\")\n",
    "\n",
    "ax.plot(*symm_diff(log_alpha, neg_kl_cplx_mc), alpha=0.5,\n",
    "        label=r\"$\\partial {KL}_\\mathbb{C}$-MC symm.d\")\n",
    "ax.plot(log_alpha, -kl_cplx_deriv(log_alpha), label=r\"$\\partial {KL}_\\mathbb{C}$-exact\")\n",
    "\n",
    "ax.axhline(0., c=\"k\", zorder=-10)\n",
    "ax.legend(ncol=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assert False, \"\"\"STOP!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import erf, expit\n",
    "\n",
    "x = np.linspace(-10, 10, num=513)\n",
    "plt.plot(x, 1 - expit(x))\n",
    "plt.plot(x, 1 - (erf(x) + 1) / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a complex random vector $z \\in \\mathbb{C}^d$\n",
    "$$\n",
    "    z \\sim \\mathcal{CN}_d \\bigl(\\theta, K, C \\bigr)\n",
    "        \\Leftrightarrow\n",
    "        \\begin{pmatrix}\\Re z \\\\ \\Im z\\end{pmatrix}\n",
    "            \\sim \\mathcal{N}_{2 d} \\biggl(\n",
    "                \\begin{pmatrix}\\Re \\theta \\\\ \\Im \\theta \\end{pmatrix},\n",
    "                \\tfrac12 \\begin{pmatrix}\n",
    "                     \\Re (K + C) & - \\Im (K - C) \\\\\n",
    "                     \\Im (K + C) & \\Re (K - C)\n",
    "                \\end{pmatrix}\n",
    "            \\biggr)\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $x \\sim \\mathcal{N}_{2n}\\Bigl( \\mu, \\Sigma\\Bigr)$ with $x = (x_1, x_2)$, then $z = x_1 + i x_2$\n",
    "is a complex gaussian random vector,  $z \\sim \\mathcal{CN}_n(\\mu_1 + i \\mu_2, K, C)$ with\n",
    "$$\n",
    "\\begin{align}\n",
    "    K &= \\Sigma_{11} + \\Sigma_{22} + i (\\Sigma_{21} - \\Sigma_{12})\n",
    "        \\,, \\\\\n",
    "    C &= \\Sigma_{11} - \\Sigma_{22} + i (\\Sigma_{21} + \\Sigma_{12})\n",
    "        \\,.\n",
    "\\end{align}\n",
    "$$\n",
    "Note that $\\Sigma_{12} = \\Sigma_{21}^\\top$, and $\\Sigma_{11}, \\Sigma_{22} \\succeq 0$ imply that\n",
    "$K\\succeq 0$, $K^H = K$, $C = C^\\top$, and $\\overline{\\Gamma} \\succeq \\overline{C} \\Gamma^{-1} C $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $A\\in \\mathbb{C}^{n\\times d}$ and $b\\in \\mathbb{C}^n$, then\n",
    "$$\n",
    "    A z + b \\sim \\mathcal{CN}_n \\bigl(A \\theta + b, A K A^H, A C A^\\top \\bigr)\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed,\n",
    "$$\n",
    "    \\mathbb{E} Az + b = A \\mathbb{E} z + b = A \\theta + b\n",
    "    \\,, $$\n",
    "and for $\\xi = Az - A\\theta$ we have\n",
    "$$\n",
    "    \\mathbb{E} A (z-\\theta)(z-\\theta)^H A^H = A K A^H\n",
    "    \\,,\n",
    "    \\mathbb{E} A (z-\\theta)(z-\\theta)^\\top A^\\top = A C A^\\top\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A complex vector is called _proper_ (or circularly-symmetric) iff the pseudo-covariance\n",
    "matrix $C$ vanishes. This means that the corresponding blocks in the double-vector real\n",
    "representation obey $\\Sigma_{11} = \\Sigma_{22}$ and $\\Sigma_{21} = - \\Sigma_{12}$. This\n",
    "last condition implies that $\\Sigma_{12}$ is skew-symmetric: $\\Sigma_{12} = -\\Sigma_{12}^\\top$.\n",
    "This also means that the real and imaginary components of each element of $z$ are\n",
    "uncorrelated, since skew-symmetry means that its main diagonal is zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output of a random complex-linear function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider $y = Wx = (I \\otimes x^\\top) \\mathop{vec}(W)$ for\n",
    "$$\n",
    "    \\mathop{vec}(W) \\sim \\mathcal{CN}_{[d_1\\times d_0]}\n",
    "        \\Bigl(\\mathop{vec} \\theta, K, C\\Bigr)\n",
    "    \\,, $$\n",
    "for $K \\in \\mathbb{C}^{[d_1\\times d_0]\\times [d_1\\times d_0]}$ diagonal $K = \\mathop{diag}(k_\\omega)$.\n",
    "\n",
    "Since $K = K^H$ we must have $k_\\omega = \\overline{k_\\omega}$, whence $k_\\omega \\in \\mathbb{R}$\n",
    "and $k_\\omega \\geq 0$. The relation matrix $C$ is also diagonal with $c_\\omega \\in \\mathbb{C}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that for $A = I \\otimes x^\\top$ we have $A^H = I \\otimes \\bar{x}$\n",
    "and $A^\\top = I \\otimes x$ both $[d_1\\times d_0]\\times [d_1\\times 1]$ matrices.\n",
    "$$\n",
    "    A K A^H\n",
    "        = \\sum_{i=1}^{d_1} \\sum_{j=1}^{d_0}\n",
    "            A (e_i \\otimes e_j) k_{ij} (e_i \\otimes e_j)^\\top A^H\n",
    "        = \\sum_{i=1}^{d_1} \\sum_{j=1}^{d_0} e_i e_i^\\top x_j k_{ij} \\bar{x}_j\n",
    "        = \\sum_{i=1}^{d_1} e_i e_i^\\top \\Bigl\\{ \\sum_{j=1}^{d_0} k_{ij} x_j \\bar{x}_j \\Bigr\\}\n",
    "    \\,, $$\n",
    "and\n",
    "$$\n",
    "    A C A^\\top\n",
    "        = \\sum_{i=1}^{d_1} \\sum_{j=1}^{d_0}\n",
    "            A (e_i \\otimes e_j) c_{ij} (e_i \\otimes e_j)^\\top A^\\top\n",
    "        = \\sum_{i=1}^{d_1} \\sum_{j=1}^{d_0} e_i e_i^\\top x_j c_{ij} x_j\n",
    "        = \\sum_{i=1}^{d_1} e_i e_i^\\top \\Bigl\\{ \\sum_{j=1}^{d_0} c_{ij} x_j x_j \\Bigr\\}\n",
    "    \\,, $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore\n",
    "$$\n",
    "    y \\sim \\mathcal{CN}_{d_1}\\Bigl(\n",
    "        \\theta x, \\sum_{i=1}^{d_1} e_i e_i^\\top \\Bigl\\{ \\sum_{j=1}^{d_0} k_{ij} x_j \\bar{x}_j \\Bigr\\},\n",
    "        \\sum_{i=1}^{d_1} e_i e_i^\\top \\Bigl\\{ \\sum_{j=1}^{d_0} c_{ij} x_j x_j \\Bigr\\}\n",
    "    \\Bigr)\n",
    "    = \\bigotimes_{i=1}^{d_1}\n",
    "        \\mathcal{CN}\\Bigl(\n",
    "            \\sum_{j=1}^{d_0} \\theta_{ij} x_j,\n",
    "            \\sum_{j=1}^{d_0} k_{ij} x_j \\bar{x}_j,\n",
    "            \\sum_{j=1}^{d_0} c_{ij} x_j x_j\n",
    "        \\Bigr)\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's suppose that the complex random vector $\\mathop{vec}W$ is proper, i.e. $C = 0$.\n",
    "Then $y$ is itself proper, has independent components and\n",
    "$$\n",
    "    y_i \\sim \\mathcal{CN}\\Bigl(\n",
    "            \\sum_{j=1}^{d_0} \\theta_{ij} x_j,\n",
    "            \\sum_{j=1}^{d_0} k_{ij} \\lvert x_j \\rvert^2, 0\n",
    "        \\Bigr)\n",
    "    \\,. $$\n",
    "In a form, more aligned with `the reparametrization trick`, the expression for the\n",
    "output is\n",
    "$$\n",
    "    y_i\n",
    "        = \\sum_{j=1}^{d_0} \\theta_{ij} x_j\n",
    "        + \\sqrt{\\sum_{j=1}^{d_0} k_{ij} \\lvert x_j \\rvert^2}\n",
    "            \\varepsilon_i\n",
    "    \\,,\n",
    "    \\varepsilon_i \\sim \\mathcal{CN}\\Bigl(\n",
    "            0, 1, 0\n",
    "        \\Bigr)\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that if $z \\sim \\mathcal{CN}(\\mu, \\gamma, 0)$ for $\\gamma \\in \\mathbb{C}$, then\n",
    "$\\gamma = \\gamma^H = \\bar{\\gamma}$, $\\gamma\\in \\mathbb{R}$ and $\\gamma \\geq 0$:\n",
    "$$\n",
    "    \\begin{pmatrix}\\Re z \\\\ \\Im z\\end{pmatrix}\n",
    "    = \\begin{pmatrix}\\Re \\mu \\\\ \\Im \\mu \\end{pmatrix}\n",
    "    + \\sqrt{\\gamma} \\varepsilon\n",
    "    \\,,\n",
    "        \\varepsilon \\sim \\mathcal{N}_{2} \\bigl( 0, \\tfrac12 I\\bigr)\n",
    "    \\,,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reparametrization $z_\\omega = \\theta_\\omega \\varepsilon_\\omega$ for\n",
    "$\\varepsilon_\\omega \\sim \\mathcal{CN}(1, \\alpha_\\omega, 0)$.\n",
    "\n",
    "$$\n",
    "    z_\\omega\n",
    "        = \\theta_\\omega \\varepsilon_\\omega\n",
    "    \\,,\n",
    "    \\varepsilon_\\omega\n",
    "        \\sim \\mathcal{CN}(1, \\alpha_\\omega, 0)\n",
    "    \\Leftrightarrow\n",
    "    z_\\omega\n",
    "        \\sim \\mathcal{CN}(\\theta_\\omega, \\alpha_\\omega \\lvert \\theta_\\omega\\rvert^2, 0)\n",
    "    \\Leftrightarrow\n",
    "    z_\\omega\n",
    "        = \\theta_\\omega + \\sigma_\\omega \\varepsilon_\\omega\n",
    "    \\,, \\sigma_\\omega^2\n",
    "        = \\alpha_\\omega \\lvert \\theta_\\omega\\rvert^2\n",
    "    \\,,\n",
    "    \\varepsilon_\\omega\n",
    "        \\sim \\mathcal{CN}(0, 1, 0)\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy and divergences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entropy of a generic gaussian random vector $x\\sim q(x) = \\mathcal{N}_d(x\\mid\\mu ,\\Sigma)$\n",
    "is\n",
    "$$\n",
    "    \\mathbb{E}_{x\\sim q} \\log q(x)\n",
    "        = - \\tfrac{d}2\\log 2\\pi - \\tfrac12 \\log\\det\\Sigma\n",
    "        - \\tfrac12 \\mathbb{E}_{x\\sim q}\\mathop{tr}{\\Sigma^{-1} (x-\\mu)(x-\\mu)^\\top}\n",
    "        = - \\tfrac12 \\log\\det 2\\pi e \\Sigma\n",
    "    \\,. $$\n",
    "\n",
    "Therefore the entropy of a gaussian complex random vector $z\\sim \\mathcal{CN}_d(\\theta, K, C)$\n",
    "is exaclty the entropy of the $2d$ double-real gaussian vector with a special\n",
    "convariance structure: for $z\\sim q(z) = \\mathcal{CN}_d(z\\mid \\theta, K, C)$\n",
    "$$\n",
    "    \\mathbb{E}_{z\\sim q} \\log q(z)\n",
    "        = - \\tfrac12 \\log\\det \\pi e \\begin{pmatrix}\n",
    "                 \\Re (K + C) & \\Im (- K + C) \\\\\n",
    "                 \\Im (K + C) & \\Re (K - C)\n",
    "            \\end{pmatrix}\n",
    "        = - \\tfrac{2d}2 \\log\\pi e\n",
    "        - \\tfrac12 \\log\\det \\begin{pmatrix}\n",
    "                 \\Re (K + C) & \\Im (- K + C) \\\\\n",
    "                 \\Im (K + C) & \\Re (K - C)\n",
    "            \\end{pmatrix}\n",
    "    \\,. $$\n",
    "If $C=0$, i.e. the complex vector is proper, then\n",
    "$$\n",
    "    \\det \\begin{pmatrix}\n",
    "         \\Re (K + C) & \\Im (- K + C) \\\\\n",
    "         \\Im (K + C) & \\Re (K - C)\n",
    "    \\end{pmatrix}\n",
    "        = \\det \\begin{pmatrix}\n",
    "             \\Re K & - \\Im K \\\\\n",
    "             \\Im K & \\Re K\n",
    "        \\end{pmatrix}\n",
    "        = \\det \\hat{K}\n",
    "        = \\det K \\overline{\\det K}\n",
    "        = \\lvert \\det K \\rvert^2\n",
    "        \\,, $$\n",
    "whence the differential entropy becomes\n",
    "$$\n",
    "    \\mathbb{E}_{z\\sim q} \\log q(z)\n",
    "        = - \\log (\\pi e)^d\\lvert \\det K\\rvert\n",
    "        = - \\log \\lvert \\det \\pi e K\\rvert\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $q$ be any distribution on $z$: a product with block terms, or even dirac $\\delta_{z_*}$\n",
    "distributions (for MLE), a mixture or whatever. $q$ may depend on anything, even $\\theta$!\n",
    "\n",
    "Then\n",
    "$$\n",
    "    \\log p(D; \\theta)\n",
    "        % = \\mathbb{E}_{z\\sim q} \\log p(D; \\theta)\n",
    "        % = \\mathbb{E}_{z\\sim q} \\log \\tfrac{p(D, z; \\theta)}{p(z\\mid D; \\theta)}\n",
    "        = \\underbrace{\n",
    "            \\mathbb{E}_{z\\sim q} \\log \\tfrac{p(D, z; \\theta)}{q(z)}\n",
    "        }_{\\mathfrak{L}(\\theta, q)}\n",
    "        + \\underbrace{\n",
    "            \\mathbb{E}_{z\\sim q} \\log \\tfrac{q(z)}{p(z\\mid D; \\theta)}\n",
    "        }_{KL(q \\| p(\\cdot \\mid D; \\theta))}\n",
    "        = \\mathbb{E}_{z\\sim q} \\log p(D\\mid z; \\theta)\n",
    "        - \\mathbb{E}_{z\\sim q} \\log \\tfrac{q(z)}{p(z)}\n",
    "        + \\mathbb{E}_{z\\sim q} \\log \\tfrac{q(z)}{p(z\\mid D; \\theta)}\n",
    "    \\,, $$\n",
    "is constant w.r.t. $q$ at any $\\theta$. Since KL-divergence is nonnegative (from\n",
    "Jensen's inequality), the Evidence Lower Bound $\\mathfrak{L}(\\theta, q)$ bound the\n",
    "original log-likelihood $\\log p(D; \\theta)$ from below.\n",
    "\n",
    "Let's maximize the ELBO with respec to $q$ and $\\theta$. However, since the whole\n",
    "right hand side is constant with respect to $q$, maximization of $\\mathfrak{L}(\\theta, q)$\n",
    "w.r.t. $q \\in \\mathcal{F}$ (holding $\\theta$ fixed) is equivalent to minimizing\n",
    "$KL(q \\| p(\\cdot \\mid D; \\theta))$ w.r.t. $q$. This is the E-step of the EM.\n",
    "\n",
    "The M-step is maximizing the ELBO over $\\theta$ holding $q$ fixed. Note that some of\n",
    "$q$ may be `offlaid` to the M-step from the E-step!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that the prior $p(z)$ and the variational distribution are fully factorized:\n",
    "$p(z) = \\otimes_{\\omega} p(z_\\omega)$, $q(z) = \\otimes_{\\omega} q(z_\\omega)$. Then\n",
    "$$\n",
    "    \\mathbb{E}_{z\\sim q} \\log \\tfrac{q(z)}{p(z)}\n",
    "        = \\sum_\\omega \\mathbb{E}_{z\\sim q} \\log \\tfrac{q(z_\\omega)}{p(z_\\omega)}\n",
    "        = \\sum_\\omega \\mathbb{E}_{q(z_\\omega)} \\log \\tfrac{q(z_\\omega)}{p(z_\\omega)}\n",
    "    \\,. $$\n",
    "<span style=\"color:red\">**NOTE**</span> we treat $p$ and $q$ as symbols, represending\n",
    "the density of the argument random variable w.r.t. some carrier measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    z_\\omega\n",
    "        = \\theta_\\omega \\varepsilon_\\omega\n",
    "    \\,,\n",
    "    \\varepsilon_\\omega\n",
    "        \\sim \\mathcal{CN}(1, \\alpha_\\omega, 0)\n",
    "    \\Leftrightarrow\n",
    "    z_\\omega\n",
    "        \\sim \\mathcal{CN}(\\theta_\\omega, \\alpha_\\omega \\lvert \\theta_\\omega\\rvert^2, 0)\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose $q(z) = \\mathcal{CN}(\\theta, \\alpha \\lvert\\theta\\rvert^2, 0)$ and\n",
    "$p(z) \\propto \\lvert z\\rvert^{-\\beta}$. Each term in the sum\n",
    "$$\n",
    "\\begin{align}\n",
    "    KL(q\\|p)\n",
    "        &= \\mathbb{E}_{q(z)} \\log \\tfrac{q(z)}{p(z)}\n",
    "        = \\mathbb{E}_{q(z)} \\log q(z) - \\mathbb{E}_{q(z)} \\log p(z)\n",
    "        \\\\\n",
    "        &= - \\log \\bigl\\lvert \\pi e \\alpha \\lvert\\theta\\rvert^2 \\bigr\\rvert\n",
    "        + \\beta \\mathbb{E}_{q(z)} \\log \\lvert z\\rvert\n",
    "        + C\n",
    "        \\\\\n",
    "        &= - \\log \\pi e\n",
    "        - \\log \\alpha \\lvert\\theta\\rvert^2\n",
    "        + \\beta \\mathbb{E}_{\\varepsilon \\sim \\mathcal{CN}(1, \\alpha, 0)}\n",
    "            \\log \\lvert \\theta \\rvert \\lvert \\varepsilon\\rvert\n",
    "        + C\n",
    "        \\\\\n",
    "        &= - \\log \\pi e - \\log \\alpha\n",
    "        + \\tfrac{\\beta - 2}2 \\log \\lvert \\theta \\rvert^2\n",
    "        + \\beta \\mathbb{E}_{\\varepsilon \\sim \\mathcal{CN}(1, \\alpha, 0)}\n",
    "            \\log \\lvert \\varepsilon\\rvert\n",
    "        + C\n",
    "        \\\\\n",
    "        &= - \\log \\pi e - \\log \\alpha\n",
    "        + \\tfrac{\\beta - 2}2 \\log \\lvert \\theta \\rvert^2\n",
    "        + \\tfrac\\beta2 \\mathbb{E}_{z \\sim \\mathcal{CN}(0, \\alpha, 0)}\n",
    "            \\log \\bigl\\lvert z + 1 \\bigr\\rvert^2\n",
    "        + C\n",
    "        \\\\\n",
    "        &= - \\log \\pi e - \\log \\alpha\n",
    "        + \\tfrac{\\beta - 2}2 \\log \\lvert \\theta \\rvert^2\n",
    "        + \\tfrac\\beta2 \\mathbb{E}_{\\varepsilon \\sim \\mathcal{N}_2\\bigl(0, \\tfrac\\alpha2 I\\bigr)}\n",
    "            \\log \\bigl((\\varepsilon_1 + 1)^2 + \\varepsilon_2^2\\bigr)\n",
    "        + C\n",
    "\\end{align}\n",
    "\\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $\\beta = 2$ the parameter $\\theta$ vanishes from the divergence,\n",
    "and it becomes just the expectation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this expression to the real variational dropout: for\n",
    "$q(z) = \\mathcal{N}(\\theta, \\alpha \\theta^2)$ and $p(z) \\propto \\lvert z\\rvert^{-1}$\n",
    "we have\n",
    "$$\n",
    "\\begin{align}\n",
    "    KL(q\\|p)\n",
    "        &= \\mathbb{E}_{q(z)} \\log \\tfrac{q(z)}{p(z)}\n",
    "        = \\mathbb{E}_{q(z)} \\log q(z) - \\mathbb{E}_{q(z)} \\log p(z)\n",
    "        \\\\\n",
    "        &= - \\tfrac12 \\log 2 \\pi e - \\tfrac12 \\log \\alpha \\theta^2\n",
    "        + \\mathbb{E}_{\\xi \\sim \\mathcal{N}(0, \\alpha)}\n",
    "            \\log \\bigl\\lvert \\theta (\\xi + 1) \\bigr\\rvert\n",
    "        + C\n",
    "        \\\\\n",
    "        &= - \\tfrac12 \\log 2 \\pi e - \\tfrac12 \\log \\alpha\n",
    "        + \\tfrac12 \\mathbb{E}_{\\xi \\sim \\mathcal{N}(0, \\alpha)}\n",
    "            \\log (\\xi + 1)^2\n",
    "        + C\n",
    "\\end{align}\n",
    "\\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Complex-Valued Random Variable](https://www.mins.ee.ethz.ch/teaching/wirelessIT/handouts/miscmath1.pdf)\n",
    "\n",
    "[Caution: The Complex Normal Distribution!](http://www.ee.imperial.ac.uk/wei.dai/PaperDiscussion/Material2014/ReadingGroup_May.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A complex Gaussian random vector $z\\in \\mathbb{C}^d$ is completely determiend by the\n",
    "distribution of $\\begin{pmatrix}\\Re z \\\\ \\Im z\\end{pmatrix} \\in \\mathbb{R}^{[2\\times d]}$\n",
    "or equivalently a $\\mathbb{C}^{[2\\times d]}$ vector:\n",
    "$$\n",
    "    \\begin{pmatrix}z \\\\ \\bar{z} \\end{pmatrix}\n",
    "        = \\underbrace{\\begin{pmatrix}I & iI \\\\ I & -iI \\end{pmatrix}}_{\\Xi}\n",
    "        \\begin{pmatrix}\\Re z \\\\ \\Im z\\end{pmatrix}\n",
    "    \\,. $$\n",
    "For a rv $z$ this vector is called the augmented $z$ vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the transjugation (Hermitian transpose) of $\\Xi$ is \n",
    "$$\n",
    "    \\Xi^H\n",
    "        = \\begin{pmatrix}I & I \\\\ -iI & iI \\end{pmatrix}\n",
    "    \\,, $$\n",
    "and $ \\Xi^H \\Xi = \\Xi \\Xi^H = \\begin{pmatrix} 2I & 0 \\\\ 0 & 2I \\end{pmatrix}$.\n",
    "Therefore, $\\Xi^{-H} = \\tfrac12 \\Xi$ and $\\Xi^{-1} = \\tfrac12 \\Xi^H$.\n",
    "Thus\n",
    "$$\n",
    "    \\begin{pmatrix}\\Re z \\\\ \\Im z\\end{pmatrix}\n",
    "        = \\Xi^{-1} \\begin{pmatrix}z \\\\ \\bar{z} \\end{pmatrix}\n",
    "        = \\tfrac12 \\Xi^H \\begin{pmatrix}z \\\\ \\bar{z} \\end{pmatrix}\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the complex covariance of the augmented $z$:\n",
    "$$\n",
    "    \\mathbb{E}\n",
    "        \\begin{pmatrix}z \\\\ \\bar{z} \\end{pmatrix}\n",
    "        \\begin{pmatrix}z \\\\ \\bar{z} \\end{pmatrix}^H\n",
    "        = \\begin{pmatrix}\n",
    "            \\mathbb{E} z \\bar{z}^\\top & \\mathbb{E} z z^\\top \\\\\n",
    "            \\mathbb{E} \\bar{z} \\bar{z}^\\top & \\mathbb{E} \\bar{z} z^\\top\n",
    "        \\end{pmatrix}\n",
    "        = \\begin{pmatrix} K & C \\\\ \\bar{C} & \\bar{K} \\end{pmatrix}\n",
    "    \\,, $$\n",
    "since effectively $z^H = (\\bar{z})^\\top = \\overline{\\bigl(z^\\top\\bigr)}$.\n",
    "\n",
    "Since $$\n",
    "    \\bigl(z z^\\top\\bigr)^H\n",
    "        = \\overline{\\bigl(z z^\\top\\bigr)}^\\top\n",
    "        = \\bigl(\\bar{z} \\bar{z}^\\top\\bigr)^\\top\n",
    "        = \\bar{z}^{\\top\\top} \\bar{z}^\\top\n",
    "        = \\bar{z} \\bar{z}^\\top\n",
    "    \\,, $$\n",
    "we have $\\bar{C} = C^H$. This implies the following constraints on $K$ (the\n",
    "covariance) and $C$ (the pseudo-covariance): $K \\succeq 0$, $K = K^H$ and\n",
    "$C=C^\\top$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This complex covariance matrix corresponds to the following real-vector covariance:\n",
    "$$\n",
    "    \\mathbb{E}\n",
    "        \\begin{pmatrix}\\Re z \\\\ \\Im z\\end{pmatrix}\n",
    "        \\begin{pmatrix}\\Re z \\\\ \\Im z\\end{pmatrix}^\\top\n",
    "        = \\mathbb{E}\n",
    "            \\begin{pmatrix}\\Re z \\\\ \\Im z\\end{pmatrix}\n",
    "            \\begin{pmatrix}\\Re z \\\\ \\Im z\\end{pmatrix}^H\n",
    "        = \\Xi^{-1}\n",
    "            \\begin{pmatrix} K & C \\\\ \\bar{C} & \\bar{K} \\end{pmatrix}\n",
    "        \\Xi^{-H}\n",
    "        = \\tfrac14 \\Xi^H \\begin{pmatrix}\n",
    "            K & C \\\\ \\bar{C} & \\bar{K}\n",
    "        \\end{pmatrix} \\Xi  % \\begin{pmatrix}I & iI \\\\ I & -iI \\end{pmatrix}\n",
    "%         = \\tfrac14 \\begin{pmatrix}I & I \\\\ -iI & iI \\end{pmatrix}\n",
    "%         \\begin{pmatrix}\n",
    "%             K + C & i (K - C) \\\\ \\bar{C} + \\bar{K} & i(\\bar{C} - \\bar{K})\n",
    "%         \\end{pmatrix}\n",
    "%         = \\tfrac14 \\begin{pmatrix}\n",
    "%              K + \\bar{K} + C + \\bar{C}\n",
    "%                  & i (K - \\bar{K} - (C - \\bar{C}))\n",
    "%                      \\\\\n",
    "%              - i (K - \\bar{K} + C - \\bar{C})\n",
    "%                  & K + \\bar{K} - (C + \\bar{C})\n",
    "%         \\end{pmatrix}\n",
    "        = \\tfrac12 \\begin{pmatrix}\n",
    "             \\Re (K + C) & - \\Im (K - C) \\\\\n",
    "             \\Im (K + C) & \\Re (K - C)\n",
    "        \\end{pmatrix}\n",
    "        = \\tfrac12 \\begin{pmatrix}\n",
    "             \\Re (K + C) & \\Im (\\overline{K - C}) \\\\\n",
    "             \\Im (K + C) & \\Re (\\overline{K - C})\n",
    "        \\end{pmatrix}\n",
    "    \\,, $$\n",
    "since $- \\Im z = \\Im \\bar{z}$ and $\\Re z = \\Re \\bar{z}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\Im (K+C)^\\top\n",
    "    = \\Im (K^\\top+C^\\top)\n",
    "    = \\Im (\\bar{K} + C)\n",
    "    = \\Im \\bar{K} + \\Im C\n",
    "    = -\\Im K + \\Im C\n",
    "    = -\\Im (K - C)\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\Re (K-C)^\\top\n",
    "    = \\Re (K^\\top - C^\\top)\n",
    "    = \\Re (\\bar{K} - C)\n",
    "    = \\Re \\bar{K} - \\Re C\n",
    "    = \\Re K - \\Re C\n",
    "    = \\Re (K - C)\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red;\">**TODO**</span>\n",
    "Futhermore, the whole matrix must be complex positive semi-definite.\n",
    "$$\n",
    "    \\begin{pmatrix}a \\\\ b \\end{pmatrix}^\\top\n",
    "        \\begin{pmatrix}\n",
    "             \\Re (K + C) & - \\Im (K - C) \\\\\n",
    "             \\Im (K + C) & \\Re (K - C)\n",
    "        \\end{pmatrix}\n",
    "    \\begin{pmatrix}a \\\\ b \\end{pmatrix}\n",
    "        = \\tfrac14 \\begin{pmatrix}a \\\\ b \\end{pmatrix}^H \\Xi^H\n",
    "            \\begin{pmatrix}\n",
    "                K & C \\\\ \\bar{C} & \\bar{K}\n",
    "            \\end{pmatrix}\n",
    "        \\Xi \\begin{pmatrix}a \\\\ b \\end{pmatrix}\n",
    "        = \\tfrac14 \\begin{pmatrix}a + i b \\\\ a - i b \\end{pmatrix}^H\n",
    "            \\begin{pmatrix}\n",
    "                K & C \\\\ \\bar{C} & \\bar{K}\n",
    "            \\end{pmatrix}\n",
    "        \\begin{pmatrix}a + i b \\\\ a - i b \\end{pmatrix}\n",
    "        = \\tfrac14 \\begin{pmatrix}z \\\\ \\bar{z} \\end{pmatrix}^H\n",
    "            \\begin{pmatrix}\n",
    "                K & C \\\\ \\bar{C} & \\bar{K}\n",
    "            \\end{pmatrix}\n",
    "        \\begin{pmatrix}z \\\\ \\bar{z} \\end{pmatrix}\n",
    "%         = \\tfrac14 \\begin{pmatrix}z \\\\ \\bar{z} \\end{pmatrix}^H\n",
    "%             \\begin{pmatrix}\n",
    "%                 K z + C \\bar{z} \\\\ \\bar{C} z + \\bar{K} \\bar{z}\n",
    "%             \\end{pmatrix}\n",
    "%         = \\tfrac14 \\bigl(\n",
    "%                 z^H K z + z^H C \\bar{z} + \\bar{z}^H \\bar{C} z + \\bar{z}^H \\bar{K} \\bar{z}\n",
    "%         \\bigr)\n",
    "        = \\tfrac14 \\bigl(\n",
    "                z^H K z + z^H C \\bar{z} + \\overline{z^H C \\bar{z}} + \\overline{z^H K z}\n",
    "        \\bigr)\n",
    "        = \\tfrac12 \\Re (z^H K z) + \\tfrac12 \\Re (z^H C \\bar{z})\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $A$ and $B$ be conforming $\\mathbb{C}$-valued matrices. Let\n",
    "$$\n",
    "    \\hat{\\cdot}\n",
    "    \\colon \\mathbb{C}^{n \\times m} \\to \\mathbb{R}^{2n \\times 2m}\n",
    "    \\colon A \\mapsto \\begin{pmatrix}\n",
    "            \\Re A & - \\Im A \\\\\n",
    "            \\Im A & \\Re A \\\\\n",
    "        \\end{pmatrix}\n",
    "    \\,. $$\n",
    "$$\n",
    "\\begin{align}\n",
    "    AB = \\Re A \\Re B - \\Im A \\Im B + i (\\Re A \\Im B + \\Im A \\Re B)\n",
    "    &\\Leftrightarrow\n",
    "        \\begin{pmatrix}\n",
    "            \\Re A & - \\Im A \\\\\n",
    "            \\Im A & \\Re A \\\\\n",
    "        \\end{pmatrix}\n",
    "        \\begin{pmatrix}\n",
    "            \\Re B & - \\Im B \\\\\n",
    "            \\Im B & \\Re B \\\\\n",
    "        \\end{pmatrix}\n",
    "        = \\begin{pmatrix}\n",
    "            \\Re A \\Re B - \\Im A \\Im B & - (\\Re A \\Im B + \\Im A \\Re B) \\\\\n",
    "            \\Im A \\Re B + \\Re A \\Im B & \\Re A \\Re B - \\Im A \\Im B\\\\\n",
    "        \\end{pmatrix}\n",
    "        \\,, \\\\\n",
    "    \\widehat{AB} &= \\hat{A} \\hat{B}\n",
    "        \\,, \\\\\n",
    "    \\widehat{A^H} &= \\hat{A}^\\top\n",
    "        \\,, \\\\\n",
    "    I_{2n}\n",
    "        = \\widehat{I}\n",
    "        = \\widehat{A A^{-1}}\n",
    "        = \\hat{A} \\widehat{A^{-1}}\n",
    "        &\\Rightarrow\n",
    "        \\widehat{A^{-1}} = (\\hat{A})^{-1}\n",
    "        \\,, \\\\\n",
    "    \\det \\hat{A}\n",
    "        = \\det \\begin{pmatrix}\n",
    "            I & i I \\\\\n",
    "            0 & I \\\\\n",
    "        \\end{pmatrix}\n",
    "        \\begin{pmatrix}\n",
    "            \\Re A & - \\Im A \\\\\n",
    "            \\Im A & \\Re A \\\\\n",
    "        \\end{pmatrix}\n",
    "        \\begin{pmatrix}\n",
    "            I & - i I \\\\\n",
    "            0 & I \\\\\n",
    "        \\end{pmatrix}\n",
    "%         = \\det \\begin{pmatrix}\n",
    "%             \\Re A + i \\Im A & - \\Im A + i \\Re A \\\\\n",
    "%             \\Im A & \\Re A \\\\\n",
    "%         \\end{pmatrix}\n",
    "%         \\begin{pmatrix}\n",
    "%             I & - i I \\\\\n",
    "%             0 & I \\\\\n",
    "%         \\end{pmatrix}\n",
    "        &= \\det \\begin{pmatrix}\n",
    "            A & i A \\\\\n",
    "            \\Im A & \\Re A \\\\\n",
    "        \\end{pmatrix}\n",
    "        \\begin{pmatrix}\n",
    "            I & - i I \\\\\n",
    "            0 & I \\\\\n",
    "        \\end{pmatrix}\n",
    "        = \\det \\begin{pmatrix}\n",
    "            A & 0 \\\\\n",
    "            \\Im A & \\bar{A} \\\\\n",
    "        \\end{pmatrix}\n",
    "        = \\det A \\det \\bar{A}\n",
    "        = \\det A \\overline{\\det A}\n",
    "        \\,.\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On vectors the hat-operator works by treating vectors as one-column matrices.\n",
    "$$\n",
    "    \\widehat{u^H v}\n",
    "        = \\widehat{u^H} \\hat{v}\n",
    "        = \\hat{u}^\\top \\hat{v}\n",
    "        = \\begin{pmatrix}\n",
    "            \\Re u^\\top & \\Im u^\\top \\\\\n",
    "            - \\Im u^\\top & \\Re u^\\top\n",
    "        \\end{pmatrix} \\begin{pmatrix}\n",
    "            \\Re v & - \\Im v \\\\\n",
    "            \\Im v & \\Re v\n",
    "        \\end{pmatrix}\n",
    "        = \\begin{pmatrix}\n",
    "            \\Re u^\\top \\Re v + \\Im u^\\top \\Im v\n",
    "                & - (\\Re u^\\top \\Im v - \\Im u^\\top \\Re v) \\\\\n",
    "            \\Re u^\\top \\Im v - \\Im u^\\top \\Re v\n",
    "                & \\Im u^\\top \\Im v + \\Re u^\\top \\Re v\n",
    "        \\end{pmatrix}\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $Q\\in \\mathbb{C}^{n\\times n}$ is postive semidefinite, then $z^H Q z \\geq 0$ for all $z\\in \\mathbb{C}^n$.\n",
    "Then\n",
    "$$\n",
    "    0 \\leq z^H Q z\n",
    "        = \\Re z^H Q z\n",
    "        \\equiv \\begin{pmatrix}\n",
    "            \\Re z^H Q z & 0 \\\\\n",
    "            0 & \\Re z^H Q z\n",
    "        \\end{pmatrix}\n",
    "        = \\widehat{z^H Q z}\n",
    "        = \\widehat{z^H} \\widehat{Q z}\n",
    "        = \\hat{z}^\\top \\hat{Q} \\hat{z}\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\mathbb{E}_{\\xi \\sim \\mathcal{N}(0, \\alpha)}\n",
    "        \\log (\\xi + 1)^2\n",
    "            = \\log \\alpha\n",
    "            + \\mathbb{E}_{\\xi \\sim \\mathcal{N}(0, 1)}\n",
    "                \\log (\\xi + \\tfrac1{\\sqrt\\alpha})^2\n",
    "            = \\log \\alpha + g\\bigl(\\tfrac1{\\sqrt\\alpha}\\bigr)\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    g(\\mu)\n",
    "        = \\mathbb{E}_{\\xi \\sim \\mathcal{N}(0, 1)} \\log (\\xi + \\mu)^2\n",
    "\\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "    \\int_{-\\infty}^\\infty\n",
    "        \\tfrac1{\\sqrt{2\\pi}} e^{-\\tfrac12x^2}\n",
    "        \\log \\lvert x + \\mu\\rvert dx\n",
    "        &= \\int_{-\\infty}^{-\\mu}\n",
    "            \\tfrac1{\\sqrt{2\\pi}} e^{-\\tfrac12x^2}\n",
    "            \\log (- x - \\mu) dx\n",
    "        + \\int_{-\\mu}^\\infty\n",
    "            \\tfrac1{\\sqrt{2\\pi}} e^{-\\tfrac12x^2}\n",
    "            \\log (x + \\mu) dx\n",
    "        \\\\\n",
    "        &= \\int_\\mu^\\infty\n",
    "            \\tfrac1{\\sqrt{2\\pi}} e^{-\\tfrac12x^2}\n",
    "            \\log (x - \\mu) dx\n",
    "        + \\int_{-\\mu}^\\infty\n",
    "            \\tfrac1{\\sqrt{2\\pi}} e^{-\\tfrac12x^2}\n",
    "            \\log (x + \\mu) dx\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supppose $x \\sim \\chi^2_k$, the cumulant function of $y = \\log x$ is\n",
    "$$\n",
    "    K(t)\n",
    "        = \\log \\mathbb{E} e^{ty}\n",
    "        = \\log \\mathbb{E} x^t\n",
    "    \\,, $$\n",
    "which is known to be\n",
    "$$\n",
    "    K(t)\n",
    "        = t \\log2 + \\log\\Gamma\\bigl(\\tfrac{k}2 + t\\bigr) - \\log \\Gamma\\bigl(\\tfrac{k}2\\bigr)\n",
    "    \\,. $$\n",
    "Now\n",
    "$$\n",
    "    \\mathbb{E} y^k\n",
    "        != \\tfrac{d^k}{dt^k} K(t) \\big\\vert_{t=0}\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
