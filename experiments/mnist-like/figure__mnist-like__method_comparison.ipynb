{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report and plots `fig:mnist-like__method_comparison`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of a single experiment:\n",
    "\n",
    "* load models: at the end of `dense`, just prior to `fine-tune`, just after `fine-tune`\n",
    "  * the model that existed just before the `fine-tune` stage is recovered from `sparsify`\n",
    "  and the sparsity threshold, specified in the experiment\n",
    "* get each model's compression rate and accuracy on `test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch any one of the given keys from a dict, prioritizing from left to right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cplxpaper.auto.reports.utils import dict_get_one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the results, pickled sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cplxpaper.auto.reports.utils import restore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the report constructed on the grid of experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cplxpaper.auto.parameter_grid import reconstruct_grid\n",
    "\n",
    "def build_report(filename):\n",
    "    report = tqdm.tqdm(restore(filename), desc=\"analyzing report data\")\n",
    "    workers, results = zip(*report)    \n",
    "    if not results:\n",
    "        return {}, []\n",
    "\n",
    "    # compute the grid and flatten the manifests\n",
    "    experiments, options, *results = zip(*results)\n",
    "    full_grid, flat_options = reconstruct_grid(options)\n",
    "\n",
    "    return full_grid, [*zip(experiments, flat_options, *results)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decide on the target folder and computation cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_name = \"figure__mnist-like__method_comparison\"\n",
    "\n",
    "report_target = os.path.normpath(os.path.abspath(os.path.join(\n",
    "    \"../../assets\", report_name\n",
    ")))\n",
    "\n",
    "os.makedirs(report_target, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the score from the scorers' output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(score):\n",
    "    # something is horribly wrong if this fails...\n",
    "    assert score[\"pre-fine-tune\"][\"sparsity\"] == score[\"post-fine-tune\"][\"sparsity\"]\n",
    "\n",
    "    metrics = {k: dict_get_one(v, \"pooled_average_precision\", \"accuracy\")\n",
    "               for k, v in score.items()}\n",
    "\n",
    "    n_zer, n_par = map(sum, zip(*score[\"pre-fine-tune\"][\"sparsity\"].values()))\n",
    "    return {\n",
    "        **metrics,\n",
    "        \"compression\": n_par / (n_par - n_zer)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = \"appendix__\"  # \"appendix__cmp__\"  # \"legacy__\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "reports = [\n",
    "    # \"legacy__\"\n",
    "    \"./grids_joint/report__trade-off.pk\",             # VD R/C only experiments (compatible)\n",
    "    # \"appendix__\" and \"appendix__cmp__\"\n",
    "    \"./grids-20200213/report__trade-off.pk\",          # full experiment on all datasets\n",
    "    \n",
    "    # unused\n",
    "    \"./grids-20200213/grids-20200210__trade-off.pk\",  # full expriment on fashionmnist only\n",
    "]\n",
    "\n",
    "comparable_pairs = {\n",
    "    \"raw\": (\"R\"  , \"C/2\"),  # (raw) based on the number of parameters (see VC dim of complex hyperplane)\n",
    "    \"fft\": (\"R*2\", \"C\"  ),  # (fft) ditto\n",
    "#     \"raw\": (\"R\"  , \"C\"  ),  # (fft, raw) just for the sake of it\n",
    "#     \"fft\": (\"R\"  , \"C\"  ),  # \n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incomparable models in the appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PREFIX == \"appendix__\":\n",
    "    reports = [\n",
    "        \"./grids-20200213/report__trade-off.pk\"\n",
    "    ]\n",
    "\n",
    "    comparable_pairs = {\"raw\": (\"R\"  , \"C\"  ), \"fft\": (\"R\"  , \"C\"  )}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparable models in the appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PREFIX == \"appendix__cmp__\":\n",
    "    reports = [\n",
    "        \"./grids-20200213/report__trade-off.pk\"\n",
    "    ]\n",
    "\n",
    "    comparable_pairs = {\"raw\": (\"R\"  , \"C/2\"), \"fft\": (\"R*2\", \"C\"  )}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R-C plots in the main text with VD experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PREFIX == \"legacy__\":\n",
    "    reports = [\n",
    "        \"./grids_joint/report__trade-off.pk\"  # R-C comparison\n",
    "    ]\n",
    "\n",
    "    comparable_pairs = {\"raw\": (\"R\"  , \"C\"  ), \"fft\": (\"R\"  , \"C\"  )}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate several grids and join them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "output, joint_grid = [], defaultdict(set)\n",
    "for report in reports:\n",
    "    grid, results = build_report(report)\n",
    "    output.extend(results)\n",
    "    for k, v in grid.items():\n",
    "        joint_grid[k].update(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alter the recovered grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = set(field for field in joint_grid\n",
    "           if not any(map(field.__contains__, {\n",
    "                # service fields\n",
    "                \"__name__\", \"__timestamp__\", \"__version__\", \"device\",\n",
    "\n",
    "                # ignore global model class settings\n",
    "                \"model__cls\",\n",
    "\n",
    "                # upcast is a service variable, which only complex models have\n",
    "                #  and it is usually mirrored in `features` settings.\n",
    "                \"__upcast\"\n",
    "            })))\n",
    "\n",
    "grid.update({\n",
    "    \"stages__sparsify__model__cls\",\n",
    "    \"threshold\"  # ensure threshold is included\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index by the experiment **grid--folder** and prepare fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments, options, *rest = zip(*output)\n",
    "\n",
    "# experiment paths are absolute!\n",
    "master_index = pd.Index(experiments, name=\"experiment\", dtype=str)\n",
    "master_index = master_index.str.replace(os.path.commonpath(experiments), \"*\")\n",
    "\n",
    "master_index = master_index.str.rsplit(\"/\", 1, expand=True)\n",
    "master_index.rename([\"grid\", \"experiment\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradually construct the table of options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = pd.DataFrame(index=master_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign proper tags to models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cplxpaper.auto.reports.utils import get_model_tag\n",
    "\n",
    "grid = [k for k in grid if not k.startswith((\n",
    "    \"model__\",\n",
    "    \"stages__sparsify__model__\"\n",
    "))]\n",
    "\n",
    "parameters = parameters.join(pd.DataFrame([\n",
    "    *map(get_model_tag, options)\n",
    "], index=master_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deal with features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cplxpaper.auto.reports.utils import get_features_tag\n",
    "\n",
    "grid = [k for k in grid if not k.startswith(\"features__\")]\n",
    "\n",
    "parameters = parameters.join(pd.DataFrame([\n",
    "    *map(get_features_tag, options)\n",
    "], index=master_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle dataset family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cplxpaper.auto.reports.utils import get_dataset_tag\n",
    "\n",
    "grid = [k for k in grid if not k.startswith(\"datasets__\")]\n",
    "\n",
    "parameters = parameters.join(pd.DataFrame([\n",
    "    *map(get_dataset_tag, options)\n",
    "], index=master_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other fields' preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only the essential experiment parameters should have remained by now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = parameters.join(pd.DataFrame([\n",
    "    {g: opt[g] for g in grid} for opt in options\n",
    "], index=master_index))\n",
    "\n",
    "grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now collect the metrics. We need:\n",
    "* **accuracy** performance on `dense`, `pre-fine-tune` and `post-fine-tune`\n",
    "* **compression rate** from a `fine-tune` stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, *tail = rest\n",
    "assert not tail\n",
    "\n",
    "metrics = pd.DataFrame([\n",
    "    get_score(dict_get_one(score, \"test\", \"test-256\")) for score in scores\n",
    "], index=master_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the tables and rename unfotunate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main = parameters.join(metrics).rename(columns={\n",
    "    \"stages__sparsify__objective__kl_div\": \"kl_div\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group by all fileds except for `kl_div` coefficient:\n",
    "* `model`, `kind`, `method`, `dataset` and `threshold`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([f for f in parameters.columns if \"kl_div\" not in f])\n",
    "fields = [\n",
    "    'dataset',\n",
    "#     'method',\n",
    "    'model',  # models are plotted together\n",
    "    'features',\n",
    "#     'kind',  # use kind for joint plotting\n",
    "    'threshold',\n",
    "]\n",
    "grouper = df_main.groupby(fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A service plotting function to darkern the specified colour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FormatStrFormatter, FuncFormatter\n",
    "\n",
    "\n",
    "def darker(color, a=0.5):\n",
    "    \"\"\"Adapted from this stackoverflow question_.\n",
    "    .. _question: https://stackoverflow.com/questions/37765197/\n",
    "    \"\"\"\n",
    "    from matplotlib.colors import to_rgb\n",
    "    from colorsys import rgb_to_hls, hls_to_rgb\n",
    "\n",
    "    h, l, s = rgb_to_hls(*to_rgb(color))\n",
    "    return hls_to_rgb(h, max(0, min(a * l, 1)), s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The common trade-off plotting procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FormatStrFormatter, FuncFormatter\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "def plot_performance_compression_plot(group, data):\n",
    "    \"\"\"Produce the performance compression plot.\n",
    "\n",
    "    Things tried\n",
    "    ------------\n",
    "    Tried saturation contrasting (poor), used quiver (arrow heads are confusing)\n",
    "    superimposed on to scatter (not good), using marker styles (bad), good idea\n",
    "    was to swap C1 and C2 above so that related models (that have similar performance)\n",
    "    have contrasting colours. used quiver alone (poor). Finally decided to use plain\n",
    "    lines. Hopefully this conveys the that pre/post fine-tune may differ.\n",
    "    \"\"\"\n",
    "    title = \"Trade-off on {dataset} for {model} ({features}) ($\\\\tau = {threshold}$)\".format(**group)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 5), dpi=300)\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_ylabel(\"accuracy\")\n",
    "    ax.set_xlabel(\"compression\")\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(lambda x, p: f\"$\\\\times${int(x):d}\"))\n",
    "    \n",
    "    ax.set_ylim(ylim_pairs[group[\"dataset\"], group[\"model\"]])\n",
    "    ax.set_xlim(1, 1.5e3)\n",
    "\n",
    "    # grid and the adequacy zone\n",
    "    ax.grid(axis='x', which=\"major\", c=\"k\", alpha=0.1, zorder=-20)\n",
    "    ax.axvspan(50, 500, color=\"k\", alpha=0.05, zorder=-10)\n",
    "\n",
    "    # draw the scatter plot of compression-accuracy pairs\n",
    "    for (kind, method), df in data.groupby([\"kind\", \"method\"]):\n",
    "        label = f\"{kind} {method}\"  # .format(**group, kind=kind)\n",
    "        color = kind_method_color[kind, method]\n",
    "\n",
    "        # draw the `dense` min-max band and median\n",
    "        patch = ax.axhline(\n",
    "            df[\"dense\"].median(), color=darker(color, 1.5),\n",
    "            alpha=0.75, lw=1, zorder=-10)\n",
    "        ax.axhspan(\n",
    "            df[\"dense\"].min(), df[\"dense\"].max(), color=darker(color, 1.7),\n",
    "            alpha=0.15, lw=0, zorder=-15)\n",
    "\n",
    "        # performance jump using line collection and final endpoint scatter\n",
    "        c = df['compression']\n",
    "        z, a = df['post-fine-tune'], df['pre-fine-tune']\n",
    "        ax.add_collection(LineCollection(\n",
    "            np.array([*zip(zip(c, a), zip(c, z))]),\n",
    "            colors=[darker(color, 0.5)], lw=1, alpha=0.125, zorder=+5\n",
    "        ))\n",
    "        ax.scatter(c, z, c=\"k\", edgecolor=[color], lw=1, s=5,\n",
    "                   marker=\"o\", label=label, alpha=1.0, zorder=+10)\n",
    "\n",
    "    ax.legend(ncol=2, loc=\"lower left\")\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method colour coding scheme:\n",
    "* fft and raw features are never mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kind_method_color = {  # fft | raw\n",
    "    # tab10 colours are paired! use this to keep similar models distinguishable\n",
    "    (\"R*2\", \"VD\"   ): \"C0\", (\"R\"  , \"VD\"   ): \"C0\",\n",
    "    (\"C\"  , \"VD\"   ): \"C2\", (\"C/2\", \"VD\"   ): \"C2\",\n",
    "\n",
    "    (\"R*2\", \"ARD\"): \"C1\", (\"R\"  , \"ARD\"): \"C1\",\n",
    "    (\"C\"  , \"ARD\"): \"C3\", (\"C/2\", \"ARD\"): \"C3\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y-axis limits for clearer picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylim_pairs = {\n",
    "    (\"emnist_letters\", 'TwoLayerDenseModel'): (0.75, 0.87),\n",
    "    (\"emnist_letters\", 'SimpleConvModel'): (0.87, 0.91),\n",
    "    (\"fashionmnist\", 'TwoLayerDenseModel'): (0.81, 0.87),\n",
    "    (\"fashionmnist\", 'SimpleConvModel'): (0.84, 0.90),\n",
    "    (\"kmnist\", 'TwoLayerDenseModel'): (0.70, 0.875),\n",
    "    (\"kmnist\", 'SimpleConvModel'): (0.85, 0.945),\n",
    "\n",
    "    (\"mnist\", 'TwoLayerDenseModel'): (0.93, 0.98),\n",
    "    (\"mnist\", 'SimpleConvModel'): (0.975, 0.995),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot for all groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for key, df in tqdm.tqdm(grouper, desc=\"populating plots\"):\n",
    "    group, df = dict(zip(fields, key)), df.drop(columns=fields)\n",
    "\n",
    "    # comparability filter\n",
    "    pair = comparable_pairs[group[\"features\"]]\n",
    "    df = df.loc[df.kind.apply(pair.__contains__)]\n",
    "\n",
    "    fig = plot_performance_compression_plot(group, df)\n",
    "    fig.patch.set_alpha(1.0)\n",
    "\n",
    "    filename = (\n",
    "        PREFIX + \"{model}__{dataset}__{features}__{threshold}\"\n",
    "    ).format(**group)\n",
    "    fig.savefig(os.path.join(report_target, filename + \".pdf\"), dpi=300)\n",
    "\n",
    "#     plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is inside?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main.loc[(\n",
    "    df_main.dataset==\"mnist\"\n",
    ")&(\n",
    "    df_main.model==\"SimpleConvModel\"\n",
    ")&(\n",
    "    df_main.features==\"raw\"\n",
    ")].sort_values([\"kind\", \"model\", \"method\", \"kl_div\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_main.groupby([\"model\", \"kind\", \"features\", \"method\", \"kl_div\"])['compression'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = {k: df.xs(k, level=\"kind\") for k, d in df.groupby([\"kind\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_fft = mod[\"C\"].loc[:, 'fft'] / mod[\"R*2\"].loc[:, 'fft']\n",
    "ratio_fft.groupby([\"model\", \"method\"]).plot()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_raw = mod[\"C/2\"].loc[:, 'raw'] / mod[\"R\"].loc[:, 'raw']\n",
    "ratio_raw.groupby([\"model\", \"method\"]).plot()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_raw = mod[\"C\"].loc[:, 'raw'] / mod[\"R\"].loc[:, 'raw']\n",
    "ratio_raw.groupby([\"model\", \"method\"]).plot()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_fft = mod[\"C\"].loc[:, 'fft'] / mod[\"R\"].loc[:, 'fft']\n",
    "ratio_fft.groupby([\"model\", \"method\"]).plot()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (df_main.loc[(\n",
    "    df_main.dataset==\"mnist\"\n",
    ")&(\n",
    "    df_main.model==\"SimpleConvModel\"\n",
    ")&(\n",
    "    df_main.method==\"VD\"\n",
    ")&(\n",
    "    df_main.kl_div > .01\n",
    ")&(\n",
    "    df_main.kl_div < .1\n",
    ")]\\\n",
    ".groupby([\"kl_div\", \"model\", \"method\", \"features\", \"kind\"]).mean()\\\n",
    ".sort_values([\"kl_div\", \"model\", \"method\", \"features\", \"kind\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_main.groupby([\"kl_div\", \"model\", \"method\", \"features\", \"kind\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_main.loc[(\n",
    "    df_main.dataset==\"mnist\"\n",
    ")&(\n",
    "    df_main.model==\"SimpleConvModel\"\n",
    ")&(\n",
    "    df_main.method==\"VD\"\n",
    ")&(\n",
    "    df_main.kl_div > .1\n",
    ")]\\\n",
    ".groupby([\"kl_div\", \"model\", \"method\", \"features\", \"kind\"]).mean()\\\n",
    ".sort_values([\"kl_div\", \"model\", \"method\", \"features\", \"kind\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp = df_main.compression\n",
    "\n",
    "df = df_main.loc[(90 <= cmp) & (cmp <= 200)]\n",
    "df = df.sort_values([\"method\", \"kl_div\"])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[os.path.join(*exp) for exp in df.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
