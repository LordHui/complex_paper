{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report and plots `fig:mnist-like__trade-off`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File-based caching, and snapshot loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cplxpaper.auto.utils import file_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the results, pickled sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def restore(filename):\n",
    "    with open(filename, \"rb\") as storage:\n",
    "        while True:\n",
    "            try:\n",
    "                #  Pickle streams are entirely self-contained\n",
    "                # unpickling will unpickle one entire object at a time.\n",
    "                yield pickle.load(storage)\n",
    "\n",
    "            except EOFError:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate every experiment from the grid of experiments.\n",
    "* calls `evaluate_experiemnt(...)` defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cplxpaper.auto.parameter_grid import reconstruct_grid\n",
    "\n",
    "def build_report(filename):\n",
    "    objects = tqdm.tqdm(restore(filename), desc=\"analyzing report data\")\n",
    "\n",
    "    results = []\n",
    "    for (worker, (experiment, options, scores)) in objects:\n",
    "        results.append((experiment, options, scores))\n",
    "    \n",
    "    if not results:\n",
    "        return {}, []\n",
    "\n",
    "    # compute the grid and flatten the manifests\n",
    "    experiments, options, *results = zip(*results)\n",
    "    full_grid, flat_options = reconstruct_grid(options)\n",
    "\n",
    "    return full_grid, [*zip(experiments, flat_options, *results)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load models from an experiment:\n",
    "* load the models at the end of `dense` and `fine-tune` stages\n",
    "* recover the model that existed just before the `fine-tune` stage\n",
    "from `sparsify` and the sparsity threshold, specified in the experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate a single experiment:\n",
    "* load models: at the end of `dense`, just prior to `fine-tune`, just after `fine-tune`\n",
    "* get each model's compression rate and accuracy on `test`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decide on the target folder and computation cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_name = \"figure__mnist-like__trade-off\"\n",
    "\n",
    "report_target = os.path.normpath(os.path.abspath(os.path.join(\n",
    "    \"../../assets\", report_name\n",
    ")))\n",
    "\n",
    "os.makedirs(report_target, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report-specific procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch any one of the given keys from a dict, prioritizing from left to right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_get_one(d, *keys):\n",
    "    for k in keys:\n",
    "        if k in d:\n",
    "            return d[k]\n",
    "\n",
    "    raise KeyError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the score from the scorers' output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(score):\n",
    "    # something is horribly wrong if this fails...\n",
    "    assert score[\"pre-fine-tune\"][\"sparsity\"] == score[\"post-fine-tune\"][\"sparsity\"]\n",
    "\n",
    "    metrics = {k: dict_get_one(v, \"pooled_average_precision\", \"accuracy\")\n",
    "               for k, v in score.items()}\n",
    "\n",
    "    n_zer, n_par = map(sum, zip(*score[\"pre-fine-tune\"][\"sparsity\"].values()))\n",
    "    return {\n",
    "        **metrics,\n",
    "        \"compression\": n_par / (n_par - n_zer)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read grids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = \"trabelsi2\"\n",
    "reports = [\n",
    "    \"./test__trade-off.pk\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate several grids and join them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "output, joint_grid = [], defaultdict(set)\n",
    "for report in reports:\n",
    "    grid, results = build_report(report)\n",
    "    output.extend(results)\n",
    "    for k, v in grid.items():\n",
    "        joint_grid[k].update(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alter the recovered grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = set(field for field in joint_grid\n",
    "           if not any(map(field.__contains__, {\n",
    "                # service fields\n",
    "                \"__name__\", \"__timestamp__\", \"__version__\", \"device\",\n",
    "\n",
    "                # ignore global model class settings\n",
    "                \"model__cls\",\n",
    "\n",
    "                # upcast is a service variable, which only complex models have\n",
    "                #  and it is usually mirrored in `features` settings.\n",
    "                \"__upcast\"\n",
    "            })))\n",
    "\n",
    "grid.update({\n",
    "    \"stages__sparsify__model__cls\",\n",
    "    \"threshold\"  # ensure threshold is included\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index by the experiment **grid--folder** and prepare fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments, options, *rest = zip(*output)\n",
    "\n",
    "# experiment paths are absolute!\n",
    "master_index = pd.Index(experiments, name=\"experiment\", dtype=str)\n",
    "master_index = master_index.str.replace(os.path.commonpath(experiments), \"*\")\n",
    "\n",
    "master_index = master_index.str.rsplit(\"/\", 1, expand=True)\n",
    "master_index.rename([\"grid\", \"experiment\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradually construct the table of options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = pd.DataFrame(index=master_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign proper tags to models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cplxpaper.auto.reports.utils import get_model_tag\n",
    "\n",
    "grid = [k for k in grid if not k.startswith((\n",
    "    \"model__\",\n",
    "    \"stages__sparsify__model__\"\n",
    "))]\n",
    "\n",
    "parameters = parameters.join(pd.DataFrame([\n",
    "    *map(get_model_tag, options)\n",
    "], index=master_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deal with features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cplxpaper.auto.reports.utils import get_features\n",
    "\n",
    "grid = [k for k in grid if not k.startswith(\"features__\")]\n",
    "\n",
    "parameters = parameters.join(pd.DataFrame([\n",
    "    *map(get_features, options)\n",
    "], index=master_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle dataset family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cplxpaper.auto.reports.utils import get_dataset\n",
    "\n",
    "grid = [k for k in grid if not k.startswith(\"datasets__\")]\n",
    "\n",
    "parameters = parameters.join(pd.DataFrame([\n",
    "    *map(get_dataset, options)\n",
    "], index=master_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other fields' preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only the essential experiment parameters should have remained by now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = parameters.join(pd.DataFrame([\n",
    "    {g: opt[g] for g in grid} for opt in options\n",
    "], index=master_index))\n",
    "\n",
    "grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now collect the metrics. We need:\n",
    "* **accuracy** performance on `dense`, `pre-fine-tune` and `post-fine-tune`\n",
    "* **compression rate** from a `fine-tune` stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, *tail = rest\n",
    "assert not tail\n",
    "\n",
    "metrics = pd.DataFrame([\n",
    "    get_score(score[\"test\"]) for score in scores\n",
    "], index=master_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the tables and rename unfotunate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main = parameters.join(metrics).rename(columns={\n",
    "    \"stages__sparsify__objective__kl_div\": \"kl_div\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group by all fileds except for `kl_div` coefficient:\n",
    "* `model`, `kind`, `method`, `dataset` and `threshold`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([f for f in parameters.columns if \"kl_div\" not in f])\n",
    "fields = [\n",
    "    'dataset',\n",
    "    'method',\n",
    "#     'model',  # models are plotted together\n",
    "    'features',\n",
    "#     'kind',  # use kind for joint plotting\n",
    "    'threshold',\n",
    "]\n",
    "grouper = df_main.groupby(fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A service plotting function to darkern the specified colour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FormatStrFormatter, FuncFormatter\n",
    "\n",
    "\n",
    "def darker(color, a=0.5):\n",
    "    \"\"\"Adapted from this stackoverflow question_.\n",
    "    .. _question: https://stackoverflow.com/questions/37765197/\n",
    "    \"\"\"\n",
    "    from matplotlib.colors import to_rgb\n",
    "    from colorsys import rgb_to_hls, hls_to_rgb\n",
    "\n",
    "    h, l, s = rgb_to_hls(*to_rgb(color))\n",
    "    return hls_to_rgb(h, max(0, min(a * l, 1)), s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The common trade-off plotting procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FormatStrFormatter, FuncFormatter\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "def plot_performance_compression_plot(group, data):\n",
    "    \"\"\"Produce the performance compression plot.\n",
    "\n",
    "    Things tried\n",
    "    ------------\n",
    "    Tried saturation contrasting (poor), used quiver (arrow heads are confusing)\n",
    "    superimposed on to scatter (not good), using marker styles (bad), good idea\n",
    "    was to swap C1 and C2 above so that related models (that have similar performance)\n",
    "    have contrasting colours. used quiver alone (poor). Finally decided to use plain\n",
    "    lines. Hopefully this conveys the that pre/post fine-tune may differ.\n",
    "    \"\"\"\n",
    "    title = \"Trade-off on {dataset} ({features}) by {method} ($\\\\tau = {threshold}$)\".format(**group)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 5), dpi=300)\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_ylabel(\"accuracy\")\n",
    "    ax.set_xlabel(\"compression\")\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(lambda x, p: f\"$\\\\times${int(x):d}\"))\n",
    "    \n",
    "    ax.set_ylim(ylim_pairs[group[\"dataset\"]])\n",
    "    ax.set_xlim(1, 1.5e3)\n",
    "\n",
    "    # grid and the adequacy zone\n",
    "    ax.grid(axis='x', which=\"major\", c=\"k\", alpha=0.1, zorder=-20)\n",
    "    ax.axvspan(50, 500, color=\"k\", alpha=0.05, zorder=-10)\n",
    "\n",
    "    # draw the scatter plot of compression-accuracy pairs\n",
    "    for (kind, model), df in data.groupby([\"kind\", \"model\"]):\n",
    "        label = f\"{kind} {model}\"  # .format(**group, kind=kind)\n",
    "        color = kind_model_color[kind, model]\n",
    "        \n",
    "        # draw the `dense` min-max band and median\n",
    "        patch = ax.axhline(\n",
    "            df[\"dense\"].median(), color=darker(color, 1.5),\n",
    "            alpha=0.75, lw=1, zorder=-10)\n",
    "        ax.axhspan(\n",
    "            df[\"dense\"].min(), df[\"dense\"].max(), color=darker(color, 1.7),\n",
    "            alpha=0.15, lw=0, zorder=-15)\n",
    "\n",
    "        # performance jump using line collection and final endpoint scatter\n",
    "        c = df['compression']\n",
    "        z, a = df['post-fine-tune'], df['pre-fine-tune']\n",
    "        ax.add_collection(LineCollection(\n",
    "            np.array([*zip(zip(c, a), zip(c, z))]),\n",
    "            colors=[darker(color, 0.5)], lw=1, alpha=0.5, zorder=+5\n",
    "        ))\n",
    "        ax.scatter(c, z, c=\"k\", edgecolor=[color], lw=1, s=5,\n",
    "                   marker=\"o\", label=label, alpha=1.0, zorder=+10)\n",
    "\n",
    "    \n",
    "    ax.legend(ncol=2, loc=\"lower left\")\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model colour coding scheme:\n",
    "* fft and raw features are never mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kind_model_color = {  # fft | raw\n",
    "    # tab10 colours are paired! use this to keep similar models distinguishable\n",
    "    (\"R*2\", \"SimpleConvModel\"   ): \"C0\", (\"R\"  , \"SimpleConvModel\"   ): \"C0\",\n",
    "    (\"C\"  , \"SimpleConvModel\"   ): \"C1\", (\"C/2\", \"SimpleConvModel\"   ): \"C1\",\n",
    "\n",
    "    (\"R*2\", \"TwoLayerDenseModel\"): \"C2\", (\"R\"  , \"TwoLayerDenseModel\"): \"C2\",\n",
    "    (\"C\"  , \"TwoLayerDenseModel\"): \"C3\", (\"C/2\", \"TwoLayerDenseModel\"): \"C3\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the allowed comparison pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparable_pairs = {\n",
    "#     \"raw\": (\"R\"  , \"C/2\"),  # (raw) based on the number of parameters (see VC dim of complex hyperplane)\n",
    "#     \"fft\": (\"R*2\", \"C\"  ),  # (fft) ditto\n",
    "    \"raw\": (\"R\"  , \"C\"  ),  # (fft, raw) just for the sake of it\n",
    "    \"fft\": (\"R\"  , \"C\"  ),  # \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y-axis limits for clearer picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylim_pairs = {\n",
    "    \"mnist\": (0.93, 0.995),\n",
    "    \"kmnist\": (0.65, 0.945),\n",
    "    \"fashionmnist\": (0.80, 0.90),\n",
    "    \"emnist_letters\": (0.75, 0.91),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot for all groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for key, df in tqdm.tqdm(grouper, desc=\"populating plots\"):\n",
    "    group, df = dict(zip(fields, key)), df.drop(columns=fields)\n",
    "\n",
    "    # comparability filter\n",
    "    pair = comparable_pairs[group[\"features\"]]\n",
    "    df = df.loc[df.kind.apply(pair.__contains__)]\n",
    "\n",
    "    fig = plot_performance_compression_plot(group, df)\n",
    "    fig.patch.set_alpha(1.0)\n",
    "\n",
    "    filename = (\n",
    "        PREFIX + \"{method}__{dataset}__{features}__{threshold}\"\n",
    "    ).format(**group)\n",
    "#     fig.savefig(os.path.join(report_target, filename + \".pdf\"), dpi=300)\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ME LIKE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is inside?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cmp = df_main.compression\n",
    "\n",
    "df = df_main.loc[(90 <= cmp) & (cmp <= 200)]\n",
    "df = df.sort_values([\"method\", \"kl_div\"])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[os.path.join(*exp) for exp in df.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
