{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report and plots `fig:mnist-like__trade-off`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import tqdm\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File-based caching, and snapshot loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cplxpaper.auto import auto\n",
    "\n",
    "from cplxpaper.auto.utils import file_cache\n",
    "from cplxpaper.auto.utils import load_stage_snapshot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment manifest loader and completion checker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cplxpaper.auto.utils import load_manifest, verify_experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch any one of the given keys from a dict, prioritizing from left to right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_get_one(d, *keys):\n",
    "    for k in keys:\n",
    "        if k in d:\n",
    "            return d[k]\n",
    "\n",
    "    raise KeyError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_ = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model, stored in the given snapshot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(snapshot, errors=\"ignore\"):\n",
    "    \"\"\"Recover the model from the snapshot.\"\"\"\n",
    "    if errors not in (\"ignore\", \"raise\"):\n",
    "        raise ValueError(f\"`errors` must be either 'ignore' or 'raise'.\")\n",
    "\n",
    "    if any(k not in snapshot for k in [\"options\", \"stage\", \"model\"]):\n",
    "        if errors == \"raise\":\n",
    "            raise ValueError(\"Bad snapshot.\")\n",
    "        return torch.nn.Module()\n",
    "\n",
    "    options = snapshot[\"options\"]\n",
    "    _, settings = snapshot[\"stage\"]\n",
    "\n",
    "    model = auto.get_model(options[\"model\"], **settings[\"model\"])\n",
    "    model.to(device=torch.device(\"cpu\"))\n",
    "    model.load_state_dict(snapshot[\"model\"])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load models from an experiment:\n",
    "* load the models at the end of `dense` and `fine-tune` stages\n",
    "* recover the model that existed just before the `fine-tune` stage\n",
    "from `sparsify` and the sparsity threshold, specified in the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_experiment(folder):\n",
    "    options = load_manifest(folder)\n",
    "\n",
    "    # load 'dense'\n",
    "    models = {\"dense\": load_model(load_stage_snapshot(\"dense\", folder))}\n",
    "\n",
    "    # \"post-fine-tune\"\n",
    "    snapshot = load_stage_snapshot(\"fine-tune\", folder)\n",
    "    models[\"post-fine-tune\"] = load_model(snapshot)\n",
    "\n",
    "    # \"pre-fine-tune\": load model from `fine-tune` and deploy the masks\n",
    "    #  and weights onto it from `sparsify` using the prescribed threshold.\n",
    "    state_dict, masks = auto.state_dict_with_masks(\n",
    "        load_model(load_stage_snapshot(\"sparsify\", folder)),\n",
    "        hard=True, threshold=options[\"threshold\"])\n",
    "\n",
    "    models[\"pre-fine-tune\"] = load_model(snapshot)\n",
    "    models[\"pre-fine-tune\"].load_state_dict(state_dict, strict=False)\n",
    "\n",
    "    return options, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate every experiment from the grid of experiments.\n",
    "* calls `evaluate_experiemnt(...)` defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cplxpaper.auto.parameter_grid import reconstruct_grid\n",
    "\n",
    "def evaluate_grid(grid):\n",
    "    grid = os.path.abspath(os.path.normpath(grid))\n",
    "    grid, _, filenames = next(os.walk(grid))\n",
    "\n",
    "    filenames = tqdm.tqdm(filenames, desc=\"analyzing grid\")\n",
    "    \n",
    "    results = []\n",
    "    for name, ext in map(os.path.splitext, filenames):\n",
    "        if ext != \".json\" or name.startswith(\".\"):\n",
    "            continue\n",
    "\n",
    "        experiment = os.path.join(grid, name)\n",
    "        if not verify_experiment(experiment):\n",
    "            continue\n",
    "\n",
    "        results.append((experiment, *evaluate_experiment(experiment)))\n",
    "    \n",
    "    if not results:\n",
    "        return {}, []\n",
    "\n",
    "    # compute the grid and flatten the manifests\n",
    "    experiments, options, *results = zip(*results)\n",
    "    full_grid, flat_options = reconstruct_grid(options)\n",
    "\n",
    "    return full_grid, [*zip(experiments, flat_options, *results)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decide on the target folder and computation cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_name = \"figure__mnist-like__trade-off\"\n",
    "\n",
    "report_target = os.path.normpath(os.path.abspath(os.path.join(\n",
    "    \"../../assets\", report_name\n",
    ")))\n",
    "\n",
    "os.makedirs(report_target, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report-specific procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dirty hack to avoid loading the same dataset over and over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(None)\n",
    "def _get_datasets(key):\n",
    "    return auto.get_datasets(pickle.loads(key))\n",
    "\n",
    "\n",
    "def get_datasets(datasets):\n",
    "    return _get_datasets(pickle.dumps(datasets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the test feed for later evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cplxpaper.mnist.performance import MNISTBasePerformance\n",
    "\n",
    "def get_scorer(options, **kwargs):\n",
    "    feeds = auto.get_feeds(\n",
    "        get_datasets(options[\"datasets\"]),\n",
    "        kwargs, options[\"features\"],\n",
    "        {\"test\": options[\"feeds\"][\"test\"]}\n",
    "    )\n",
    "\n",
    "    return MNISTBasePerformance(feeds[\"test\"], threshold=options[\"threshold\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate a single experiment:\n",
    "* load models: at the end of `dense`, just prior to `fine-tune`, just after `fine-tune`\n",
    "* get each model's compression rate and accuracy on `test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cplxpaper.mnist.performance import MNISTBasePerformance\n",
    "\n",
    "@file_cache(f\"./cache__{report_name}.pk\")\n",
    "def evaluate_experiment(folder):\n",
    "    assert False, folder\n",
    "    device = torch.device(device_)\n",
    "\n",
    "    # get the models and the scorer\n",
    "    options, models = load_experiment(folder)\n",
    "    scorer = get_scorer(options, device=device)\n",
    "\n",
    "    # score each model on device\n",
    "    scores = []\n",
    "    for name, model in models.items():\n",
    "        model.to(device)\n",
    "        scores.append((name, scorer(model.eval())))\n",
    "        model.cpu()\n",
    "\n",
    "    return options, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To debug\n",
    "```python\n",
    "@file_cache(f\"./cache__{report_name}.pk\")\n",
    "def evaluate_experiment(folder):\n",
    "    print(folder)\n",
    "    assert False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the score from the scorers' output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(score):\n",
    "    # something is horribly wrong if this fails...\n",
    "    assert score[\"pre-fine-tune\"][\"sparsity\"] == score[\"post-fine-tune\"][\"sparsity\"]\n",
    "\n",
    "    metrics = {k: dict_get_one(v, \"pooled_average_precision\", \"accuracy\")\n",
    "               for k, v in score.items()}\n",
    "\n",
    "    n_zer, n_par = map(sum, zip(*score[\"pre-fine-tune\"][\"sparsity\"].values()))\n",
    "    return {\n",
    "        **metrics,\n",
    "        \"compression\": n_par / (n_par - n_zer)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read grids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    PREFIX = \"legacy__\"\n",
    "    grids = [\n",
    "        \"./grids_joint/legacy__mnist-like__00\",\n",
    "        \"./grids_joint/legacy__mnist-like__01\",\n",
    "        \"./grids_joint/legacy__mnist-like__02\",\n",
    "        \"./grids_joint/legacy__mnist-like__03\",\n",
    "        \"./grids_joint/legacy__mnist-like__04\",\n",
    "    ]\n",
    "\n",
    "else:\n",
    "    PREFIX = \"\"\n",
    "    grids = [\n",
    "        \"./grids/mnist-like__real-vs-cplx__00\",\n",
    "        \"./grids/mnist-like__real-vs-cplx__01\",\n",
    "        \"./grids/mnist-like__real-vs-cplx__02\",\n",
    "        \"./grids/mnist-like__real-vs-cplx__03\",\n",
    "#         \"./grids/mnist-like__real-vs-cplx__04\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate several grids and join them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "output, joint_grid = [], defaultdict(set)\n",
    "for grid in grids:\n",
    "    grid, results = evaluate_grid(grid)\n",
    "    output.extend(results)\n",
    "    for k, v in grid.items():\n",
    "        joint_grid[k].update(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alter the recovered grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = set(field for field in joint_grid\n",
    "           if not any(map(field.__contains__, {\n",
    "                # service fields\n",
    "                \"__name__\", \"__timestamp__\", \"__version__\", \"device\",\n",
    "\n",
    "                # ignore global model class settings\n",
    "                \"model__cls\",\n",
    "\n",
    "                # upcast is a service variable, which only complex models have\n",
    "                #  and it is usually mirrored in `features` settings.\n",
    "                \"__upcast\"\n",
    "            })))\n",
    "\n",
    "grid.update({\n",
    "    \"stages__sparsify__model__cls\",\n",
    "    \"threshold\"  # ensure threshold is included\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index by the experiment **grid--folder** and prepare fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments, options, *rest = zip(*output)\n",
    "\n",
    "# experiment paths are absolute!\n",
    "master_index = pd.Index(experiments, name=\"experiment\", dtype=str)\n",
    "master_index = master_index.str.replace(os.path.commonpath(experiments) + \"/\", \"\")\n",
    "\n",
    "master_index = master_index.str.rsplit(\"/\", 1, expand=True)\n",
    "master_index.rename([\"grid\", \"experiment\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradually construct the table of options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = pd.DataFrame(index=master_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign proper tags to models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = [k for k in grid if not k.startswith((\n",
    "    \"model__\",\n",
    "    \"stages__sparsify__model__\"\n",
    "))]\n",
    "\n",
    "def get_model_tag(opt):\n",
    "    # extract the class name\n",
    "    cls = opt[\"stages__sparsify__model__cls\"]\n",
    "    cls = re.sub(\"^<class '.*?\\.models\\.(.*?)'>$\", r\"\\1\", cls)\n",
    "\n",
    "    # get the model kind: real/complex\n",
    "    if not cls.startswith((\"real.\", \"complex.\")):\n",
    "        raise ValueError(\"Unknown model type.\")\n",
    "\n",
    "    if cls.startswith(\"real.\"):\n",
    "        kind, cls = \"R\", cls[5:]\n",
    "    elif cls.startswith(\"complex.\"):\n",
    "        kind, cls = \"C\", cls[8:]\n",
    "\n",
    "    # handle real `double` and cplx `half`\n",
    "    if kind == \"R\" and opt.get(\"model__double\", False):\n",
    "        kind = kind + \"*2\"\n",
    "    elif kind == \"C\" and opt.get(\"model__half\", False):\n",
    "        kind = kind + \"/2\"\n",
    "\n",
    "    # get method\n",
    "    if not cls.endswith((\"VD\", \"ARD\")):\n",
    "        raise ValueError(\"Unknown Bayesian method.\")\n",
    "\n",
    "    if cls.endswith(\"VD\"):\n",
    "        method, cls = \"VD\", cls[:-2]\n",
    "    elif cls.endswith(\"ARD\"):\n",
    "        method, cls = \"ARD\", cls[:-3]\n",
    "\n",
    "    return {\"model\": cls, \"kind\": kind, \"method\": method}\n",
    "\n",
    "parameters = parameters.join(pd.DataFrame([\n",
    "    *map(get_model_tag, options)\n",
    "], index=master_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deal with features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = [k for k in grid if not k.startswith(\"features__\")]\n",
    "\n",
    "def get_features(opt):\n",
    "    cls = opt[\"features__cls\"]\n",
    "    cls = re.sub(\"^<class '.*?\\.feeds\\.(.*?)'>$\", r\"\\1\", cls).lower()\n",
    "    \n",
    "    if cls == \"feedfourierfeatures\":\n",
    "        features = \"fft\"\n",
    "\n",
    "    elif cls == \"feedrawfeatures\":\n",
    "        features = \"raw\"\n",
    "    else:\n",
    "        raise ValueError(\"Unknown input features.\")\n",
    "\n",
    "    return {\"features\": features}\n",
    "\n",
    "parameters = parameters.join(pd.DataFrame([\n",
    "    *map(get_features, options)\n",
    "], index=master_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle dataset family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = [k for k in grid if not k.startswith(\"datasets__\")]\n",
    "\n",
    "def get_dataset(opt):\n",
    "    cls = dict_get_one(opt, \"datasets__musicnet-test-128__cls\", \"datasets__test__cls\")\n",
    "    assert cls is not None\n",
    "    cls = re.sub(\"^<class '.*?\\.(?:mnist|musicnet)\\.dataset\\.(.*?)'>$\", r\"\\1\", cls).lower()\n",
    "\n",
    "    return {\"dataset\": cls.replace(\"_test\", \"\")}\n",
    "\n",
    "parameters = parameters.join(pd.DataFrame([\n",
    "    *map(get_dataset, options)\n",
    "], index=master_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other fields' preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only the essential experiment parameters should have remained by now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = parameters.join(pd.DataFrame([\n",
    "    {g: opt[g] for g in grid} for opt in options\n",
    "], index=master_index))\n",
    "\n",
    "grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now collect the metrics. We need:\n",
    "* **accuracy** performance on `dense`, `pre-fine-tune` and `post-fine-tune`\n",
    "* **compression rate** from a `fine-tune` stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, *tail = rest\n",
    "assert not tail\n",
    "\n",
    "metrics = pd.DataFrame([\n",
    "    get_score(dict(score)) for score in scores\n",
    "], index=master_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the tables and rename unfotunate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main = parameters.join(metrics).rename(columns={\n",
    "    \"stages__sparsify__objective__kl_div\": \"kl_div\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group by all fileds except for `kl_div` coefficient:\n",
    "* `model`, `kind`, `method`, `dataset` and `threshold`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([f for f in parameters.columns if \"kl_div\" not in f])\n",
    "fields = [\n",
    "    'dataset',\n",
    "    'method',\n",
    "#     'model',  # models are plotted together\n",
    "    'features',\n",
    "#     'kind',  # use kind for joint plotting\n",
    "    'threshold',\n",
    "]\n",
    "grouper = df_main.groupby(fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A service plotting function to darkern the specified colour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FormatStrFormatter, FuncFormatter\n",
    "\n",
    "\n",
    "def darker(color, a=0.5):\n",
    "    \"\"\"Adapted from this stackoverflow question_.\n",
    "    .. _question: https://stackoverflow.com/questions/37765197/\n",
    "    \"\"\"\n",
    "    from matplotlib.colors import to_rgb\n",
    "    from colorsys import rgb_to_hls, hls_to_rgb\n",
    "\n",
    "    h, l, s = rgb_to_hls(*to_rgb(color))\n",
    "    return hls_to_rgb(h, max(0, min(a * l, 1)), s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model colour coding scheme:\n",
    "* fft and raw features are never mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kind_model_color = {  # fft | raw\n",
    "    # tab10 colours are paired! use this to keep similar models distinguishable\n",
    "    (\"R*2\", \"SimpleConvModel\"   ): \"C0\", (\"R\"  , \"SimpleConvModel\"   ): \"C0\",\n",
    "    (\"C\"  , \"SimpleConvModel\"   ): \"C1\", (\"C/2\", \"SimpleConvModel\"   ): \"C1\",\n",
    "\n",
    "    (\"R*2\", \"TwoLayerDenseModel\"): \"C2\", (\"R\"  , \"TwoLayerDenseModel\"): \"C2\",\n",
    "    (\"C\"  , \"TwoLayerDenseModel\"): \"C3\", (\"C/2\", \"TwoLayerDenseModel\"): \"C3\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y-axis limits for clearer picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylim_pairs = {\n",
    "    \"mnist\": (0.93, 0.995),\n",
    "    \"kmnist\": (0.65, 0.945),\n",
    "    \"fashionmnist\": (0.80, 0.90),\n",
    "    \"emnist_letters\": (0.75, 0.91),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The common trade-off plotting procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FormatStrFormatter, FuncFormatter\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "def plot_performance_compression_plot(param, data):\n",
    "    \"\"\"Produce the performance compression plot.\n",
    "\n",
    "    Things tried\n",
    "    ------------\n",
    "    Tried saturation contrasting (poor), used quiver (arrow heads are confusing)\n",
    "    superimposed on to scatter (not good), using marker styles (bad), good idea\n",
    "    was to swap C1 and C2 above so that related models (that have similar performance)\n",
    "    have contrasting colours. used quiver alone (poor). Finally decided to use plain\n",
    "    lines. Hopefully this conveys the that pre/post fine-tune may differ.\n",
    "    \"\"\"\n",
    "    filename = (\n",
    "        PREFIX + \"{method}__{dataset}__{features}__{threshold}\"\n",
    "    ).format(**group)\n",
    "\n",
    "    title = \"Trade-off on {dataset} ({features}) by {method} ($\\\\tau = {threshold}$)\".format(**group)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 3.5), dpi=300)\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_ylabel(\"accuracy\")\n",
    "    ax.set_xlabel(\"compression\")\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(lambda x, p: f\"$\\\\times${int(x):d}\"))\n",
    "    \n",
    "    ax.set_ylim(ylim_pairs[group[\"dataset\"]])\n",
    "    ax.set_xlim(1, 1.5e3)\n",
    "\n",
    "    # grid and the adequacy zone\n",
    "    ax.grid(axis='x', which=\"major\", c=\"k\", alpha=0.1, zorder=-20)\n",
    "    ax.axvspan(50, 500, color=\"k\", alpha=0.05, zorder=-10)\n",
    "\n",
    "    # draw the scatter plot of compression-accuracy pairs\n",
    "    for (kind, model), df in data.groupby([\"kind\", \"model\"]):\n",
    "        label = f\"{kind} {model}\"  # .format(**group, kind=kind)\n",
    "        color = kind_model_color[kind, model]\n",
    "        \n",
    "        # draw the `dense` min-max band and median\n",
    "        patch = ax.axhline(\n",
    "            df[\"dense\"].median(), color=darker(color, 1.5),\n",
    "            alpha=0.75, lw=1, zorder=-10)\n",
    "        ax.axhspan(\n",
    "            df[\"dense\"].min(), df[\"dense\"].max(), color=darker(color, 1.7),\n",
    "            alpha=0.15, lw=0, zorder=-15)\n",
    "\n",
    "        # performance jump using line collection and final endpoint scatter\n",
    "        c = df['compression']\n",
    "        z, a = df['post-fine-tune'], df['pre-fine-tune']\n",
    "        ax.add_collection(LineCollection(\n",
    "            np.array([*zip(zip(c, a), zip(c, z))]),\n",
    "            colors=[darker(color, 0.5)], lw=1, alpha=0.5, zorder=+5\n",
    "        ))\n",
    "        ax.scatter(c, z, c=\"k\", edgecolor=[color], lw=1, s=5,\n",
    "                   marker=\"o\", label=label, alpha=1.0, zorder=+10)\n",
    "\n",
    "    \n",
    "    ax.legend(ncol=2, loc=\"lower left\")\n",
    "    return os.path.join(report_target, filename), fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot for all groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for key, df in tqdm.tqdm(grouper, desc=\"populating plots\"):\n",
    "    df = df.drop(columns=fields)\n",
    "    group = dict(zip(fields, key))\n",
    "\n",
    "    filename, fig = plot_performance_compression_plot(group, df)\n",
    "    fig.patch.set_alpha(1.0)\n",
    "    fig.savefig(filename + \".pdf\", dpi=300)\n",
    "\n",
    "#     plt.show()\n",
    "    plt.close()\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is inside?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp = df_main.compression\n",
    "\n",
    "df = df_main.loc[(90 <= cmp) & (cmp <= 200)]\n",
    "df = df.sort_values([\"method\", \"kl_div\"])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[os.path.join(*exp) for exp in df.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
