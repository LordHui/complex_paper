{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Analysis: MusicNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import json\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully flatten the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cplxpaper.auto.parameter_grid import flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load performance results from each snapshot in the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cplxpaper.auto.utils import load_snapshot\n",
    "\n",
    "def from_snapshots(*snapshots):\n",
    "    results, options = {}, {}\n",
    "    for snapshot in sorted(snapshots):\n",
    "        name = os.path.basename(snapshot)\n",
    "        snapshot = load_snapshot(snapshot)\n",
    "\n",
    "        options = snapshot['options']\n",
    "        stage, settings = snapshot['stage']\n",
    "\n",
    "        results[name] = stage, snapshot['performance']\n",
    "\n",
    "    return results, options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load experiment from its snapshots or from cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "\n",
    "\n",
    "def load_experiment(folder, cache=\"cache.pk\", reload=False):\n",
    "    if isinstance(cache, str):\n",
    "        cache = os.path.join(folder, cache)\n",
    "\n",
    "    assert cache is None or isinstance(cache, str)\n",
    "\n",
    "    snapshots = []\n",
    "    folder, _, filenames = next(os.walk(folder))\n",
    "    for filename in sorted(filenames):\n",
    "        if re.match(r\"^\\d+.*\\.gz$\", filename) is not None:\n",
    "            snapshots.append(filename)\n",
    "\n",
    "    # load scorer results from the snapshots or from cache\n",
    "    scores, options = {}, {}\n",
    "    if cache is not None and os.path.exists(cache) and not reload:\n",
    "        with open(cache, \"rb\") as fin:\n",
    "            scores, options = pickle.load(fin)\n",
    "\n",
    "    # reload from originals if anything is missing (use SHA-digest)\n",
    "    if any(s not in scores for s in snapshots):\n",
    "        snapshots = [os.path.join(folder, s) for s in snapshots]\n",
    "        scores, options = from_snapshots(*snapshots)\n",
    "        if cache is not None:\n",
    "            with open(cache, \"wb\") as fout:\n",
    "                pickle.dump((scores, options), fout)\n",
    "\n",
    "    return scores, options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the name of the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cplxpaper.auto.utils import get_class\n",
    "\n",
    "dataset_name = \"MusicNet\"\n",
    "metric_name = \"pooled_average_precision\"\n",
    "\n",
    "# sources, scorer_name = [\"\"\"./runs/grid_trabelsi_legacy/\"\"\"], \"test_256\"  # obsolete\n",
    "\n",
    "sources = [\n",
    "#     \"\"\"./runs/grid_cplx_fine_kl_div/\"\"\",\n",
    "#     \"\"\"./runs/grid_cplx_fine_kl_div_v2/\"\"\",\n",
    "    \"\"\"./runs/grid_cplx_fine_kl_div_v3_fast/\"\"\",\n",
    "]\n",
    "scorer_name = \"test\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather model performance summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def performance_summary(scores):\n",
    "    out = {}\n",
    "    for stage, results in scores.values():\n",
    "        # Collect performance metrics..\n",
    "        score = results[scorer_name]\n",
    "\n",
    "        # ... aggregate sparsity and metrics.\n",
    "        n_zer, n_par = map(sum, zip(*score[\"sparsity\"].values()))\n",
    "        out[stage] = {\n",
    "            \"score\": score[metric_name],\n",
    "            \"n_zer\": int(n_zer), \"n_par\": int(n_par)\n",
    "        }\n",
    "\n",
    "    return pd.DataFrame.from_dict(out, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect results and reconstruct the grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "grid_options = defaultdict(set)\n",
    "ignore = {\"__name__\", \"__timestamp__\", \"__version__\", \"device\"}\n",
    "\n",
    "results = []\n",
    "for source in sources:\n",
    "    source, experiments, manifests = next(os.walk(source))\n",
    "    for experiment in tqdm.tqdm(experiments):\n",
    "        match = re.match(r\"^.*?\\[(\\d+)\\]-(\\d+)$\", experiment)\n",
    "        if not match:\n",
    "            continue\n",
    "\n",
    "        replication, exp_no = map(int, match.groups())\n",
    "\n",
    "        # load scorer results from the snapshots\n",
    "        scores, options = load_experiment(\n",
    "            os.path.join(source, experiment),\n",
    "            cache='cache.pk', reload=False)\n",
    "\n",
    "        if not options:\n",
    "            continue\n",
    "\n",
    "        flat = flatten(options)\n",
    "        for k, v in flat.items():\n",
    "            if k not in ignore:\n",
    "                grid_options[k].add(v)\n",
    "\n",
    "        results.append((\n",
    "            experiment,\n",
    "            performance_summary(scores),\n",
    "            flat\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments, scores, manifests = zip(*results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalize the grid variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pick all keys which have more than one unique value\n",
    "#  and drop any model specs (added manually)\n",
    "full_grid = [k for k, v in grid_options.items()\n",
    "             if len(v) > 1 and \"model__cls\" not in k]\n",
    "\n",
    "manual = [\"stages__sparsify__model__cls\"]\n",
    "for field in manual:\n",
    "    if len(grid_options[field]) > 0:\n",
    "        full_grid.append(field)\n",
    "\n",
    "# upcast is a service variable, which only complex models have\n",
    "#  and it i usually mirrored in `features` settings.\n",
    "full_grid = [g for g in full_grid if not g.endswith(\"__upcast\")]\n",
    "\n",
    "main_grid = [g for g in full_grid if not g.endswith('__kl_div')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the report spreadsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [{k: opt.get(k, None) for k in full_grid} for opt in manifests]\n",
    "params = pd.DataFrame.from_dict(dict(zip(experiments, params)), orient=\"index\")\n",
    "\n",
    "scores = pd.concat(dict(zip(experiments, scores)), axis=0, names=[\"expno\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = scores.unstack(-1)\n",
    "df.columns = df.columns.to_flat_index().map('-'.join)\n",
    "\n",
    "df = params.join(df).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace({\n",
    "        # identify models by the sparsify stage model\n",
    "        \"stages__sparsify__model__cls\": {\"^<class '.*?\\.models\\.(.*?)'>$\": r\"\\1\"},\n",
    "        \"features__cls\": {\"^<class '.*?\\.feeds\\.(.*?)'>$\": r\"\\1\"}\n",
    "    }, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index([*main_grid, \"stages__sparsify__objective__kl_div\", \"index\"], append=False, drop=True).sort_index(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[\"complex.DeepConvNetVD\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "summary, stage = {}, \"fine-tune\"\n",
    "summary, stage = {}, \"sparsify\"\n",
    "\n",
    "if main_grid:\n",
    "    groups = ((k, g.loc[k]) for k, g in df.groupby(axis=0, level=main_grid))\n",
    "\n",
    "else:\n",
    "    groups = [(\n",
    "        (dataset_name,), df\n",
    "    )]\n",
    "\n",
    "for k, g in groups:\n",
    "    score_before = g[\"score-dense\"].mean(), g[\"score-dense\"].std()\n",
    "    f_score, n_par, n_zer = g[f\"score-{stage}\"], g[f\"n_par-{stage}\"], g[f\"n_zer-{stage}\"]\n",
    "\n",
    "    curve = pd.concat([n_zer / n_par, f_score], axis=1)\n",
    "    curve_mean = curve.mean(level=0).to_numpy()\n",
    "    curve_std = curve.std(level=0).to_numpy()\n",
    "\n",
    "    curve = curve.to_numpy()\n",
    "    order = curve[:, 0].argsort()\n",
    "\n",
    "    summary[k] = score_before, curve[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from matplotlib.ticker import FormatStrFormatter, FuncFormatter\n",
    "\n",
    "# dttm = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "figurename = os.path.basename(os.path.normpath(source))\n",
    "\n",
    "filename = os.path.join(\n",
    "    \"../../assets\", f\"{figurename}__{stage}.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce a plot (bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(14, 5))\n",
    "fig.patch.set_alpha(1.0)\n",
    "\n",
    "for name, (dense, curve) in summary.items():\n",
    "    m, s = dense\n",
    "    spr, scr = curve.T\n",
    "    pts = ax.scatter(1 / (1 - spr), scr, label=name, s=15)\n",
    "\n",
    "    color = pts.get_facecolor()[0]\n",
    "    ax.axhspan(m - 1.96 * s, m + 1.96 * s, alpha=0.1, color=color, zorder=-99)\n",
    "\n",
    "ax.legend(loc=\"lower left\", ncol=1)\n",
    "\n",
    "ax.set_title(f\"Average Precision - Compression trade-off on MusicNet\")\n",
    "\n",
    "ax.set_ylabel(\"Average Precision\")\n",
    "ax.set_xlabel(r\"$\\times$ compressed\")\n",
    "\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlim(0.9, 2e3)\n",
    "ax.xaxis.set_major_formatter(FuncFormatter(lambda x, p: f\"{int(x):d}\"))\n",
    "ax.yaxis.set_major_formatter(FuncFormatter(lambda x, p: f\"{x:.0%}\"))\n",
    "\n",
    "# ax.axvspan(50, 500, color=\"k\", alpha=0.05, zorder=-10)  # for Trablesi et al. with k=3\n",
    "ax.axvspan(40, 300, color=\"k\", alpha=0.05, zorder=-10)  # for Trablesi et al. with k=6\n",
    "ax.axhline(0.726, color=\"k\", alpha=0.5, lw=1, zorder=-11)\n",
    "\n",
    "fig.savefig(filename, dpi=300, transparent=False)\n",
    "\n",
    "plt.show()\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m = (n_zer / n_par).mean(level=0)\n",
    "s = (n_zer / n_par).std(level=0)\n",
    "m.plot(label=\"sparsity\")\n",
    "plt.fill_between(m.index, m-1.96*s, m+1.96*s, alpha=0.25)\n",
    "plt.gca().set_xscale(\"log\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "\n",
    "plt.twinx()\n",
    "m = (g[\"score-dense\"]).mean(level=0)\n",
    "s = (g[\"score-dense\"]).std(level=0)\n",
    "m.plot(c=\"C1\", label=\"dense\")\n",
    "plt.fill_between(m.index, m-1.96*s, m+1.96*s, alpha=0.25, color=\"C1\")\n",
    "\n",
    "m = (g[\"score-fine-tune\"]).mean(level=0)\n",
    "s = (g[\"score-fine-tune\"]).std(level=0)\n",
    "m.plot(c=\"C2\", label=\"fine-tune\")\n",
    "plt.fill_between(m.index, m-1.96*s, m+1.96*s, alpha=0.25, color=\"C2\")\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
