Quantization and FP arithmetic
------------------------------
V. Vanhoucke, and M.Z. Mao, “Improving the speed of neural networks on CPUs,” In: Proc. Deep Learning and Unsupervised Feature Learning NIPS Workshop, 2011.
[12] consider throughput optimization: locality, loop unrolling, fixed-point arithmetic

M. Courbariaux, J.P. David, and Y. Bengio, “Training deep neural networks with low precision multiplications,” arXiv preprint arXiv:1412.7024, 2014.
[15] post-train fixed point arithmetic conversion

M. Courbariaux, Y. Bengio, and J.P. David, “Binaryconnect: Training deep neural networks with binary weights during propagations,” In: Advances in Neural Information Processing Systems, pp. 3123–3131, 2015.
[16] binary quantizaton during forward and backward

W. Chen, J.T. Wilson, S. Tyree, K.Q. Weinberger, and Y. Chen, “Compressing neural networks with the hashing trick,” arXiv preprint arXiv: 1504.04788, 2015.
[13] weight-agnostic hashing into groups sharing the same value within a layer: hash->bin->shared weight. Hashing allows NOT to store bin indices

Y. Gong, L. Liu, M. Yang, and L. Bourdev, “Compressing deep convolutional networks using vector quantization,” arXiv preprint arXiv:1412.6115, 2014.
[14] post-training quantization based on k-means clustering


Knowledge distillation
----------------------
B.B. Sau, and V.N. Balasubramanian, “Deep model compression: distilling knowledge from noisy teachers,” arXiv: 1610.09650, 2016.
[19] logit perturbation to simulate the effect of mutliple teachers when training a student


Network pruning
---------------
F. Seide, G. Li, and D. Yu, “Conversational speech transcription using context-dependent deep neural networks,” in Proceedings of Annual Conference of the International Speech Communication Association (Interspeech), 2011, pp. 437–440.
[7] \tau threshold-based magnitude pruning (hard (\ell_0) or soft (\ell_1) thresholding) with safeguard against reappearance of very weak weights at half the \tau

Y.LeCun,J.S.Denker,andS.A.Solla,“Optimal brain damage,” Proceedings of Advances in Neural Information Processing Systems (NIPS), vol. 2, pp. 598–605, 1990.
[11] use diagonal hessian approximation to (1) get parameter ``saliency'' at local optimium, then (2) rank parameters and prune, then (3) redo step 1
    zhu-gupta use ranking and sparsity targeting
    saliency [p.~602 OBD recipe~4) is very similar to \tfrac1\alpha in VarDropout
        HYPOTHESIS: relevance \propto \tfrac1\alpha = \tfrac{\mu^2}{\sigma^2} and \sigma^2 \approx \tfrac1{\partial^2_{ww} loss} at optimum

S. Han, H. Mao, and W. J. Dally, “Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding,” arXiv preprint arXiv:1510.00149, 2015a.
[11] prune-qunatize(-zip) to reduce storage and energy requirements to nets in inference mode
    (1) threshold-based magnitude pruning (train->prune->fine-tune)
    (2) quantize with k-means and fix cluster affinity
    (3) further fine-tune shared centroids using SGD (shared-scatter on forward, grad-sums on backward)
    * show that P+Q is much better than P alone

S. Anwar, and W. Sung, “Compact deep convolutional neural networks with coarse pruning,” arXiv:1610.09639, 2016.
[17] feature pruning: (1) zero random path through the net, (2) check its impact on validation, (3) pick the least worst one

S. Han, J. Pool, J. Tran, and W. Dally, “Learning both weights and connections for efficient neural network,” In: Advances in Neural Information Processing Systems, pp. 1135–1143, 2015b.
[18] investigate differnet pruning strategies with fine-tuning (\ell_1, \ell_2), emphasize importance of the latter


Low-rank matrix decomposition
-----------------------------
M. Denil, B. Shakibi, L. Dinh, M. Ranzato, and N. Freitas, “Predicting parameters in deep learning,” Neural Information Processing Systems. 2013:2148-2156.
[20] exploit weight redundancy and smoothness to predict weigths based on a random subsample and to get low-rank approximation thereof.
    Compression - dictionary (rbf or empirical stats) + subsample of weights (factors + loadings)

P. Nakkiran, R. Alvarez, R. Prabhavalkar, and C. Parada, “Compressing Deep Neural Networks Using a Rank- Constrained Topology,” Proc of Annual Conference of the International Speech Communication Association. 2015:1473-1477.
[21] use explicit rank-k parameterization of weights

E. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fergus, “Exploiting linear structure within convolutional networks for efficient evaluation,” In: Advances in Neural Information Processing Systems, pp. 1269–1277, 2014.
[22] data-driven low rank matrix and tensor decompositions to speed up eval
    biclustering followed by tensor svd, followed by a flash fine-tune stage
