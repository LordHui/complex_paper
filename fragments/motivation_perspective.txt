Motivation and perspective
\citep{wu_compressing_2019}

\citep{trabelsi_deep_2018}
* (C ≥ R) speech spectrum prediction, music transcription
    * point out 4 times growth in flops for C in appendix 6.1

\citep{gaudet_deep_2018}
* (Q ≥ R + budget) apply to image classification and segmentation, point out n_floats reduction due to structured input-output
    * sucessor to Trabelsi. constructs the basic blocks for hyper-complex neural nets
      * they use `sf` 8, 16, 32 for Q, C, and R, respectively, hence the number of parameters is as in the table
      * same budget intermediate output shepes (floats)

\citep{zhang_complex-valued_2017}
* (C ≥ R + dof budget) terrain surface classification and target identification from radar imagery (naturally cplx)

\citep{popa_complex-valued_2017}
* (C ≥ R + dof budget) show improvement in performance with complex-valued neural networks
* mentions applications in fMRI (naturally cplx)

\citep{reichert_neuronal_2014}
* provide motivation form biology alluded to by Trabelsi
* nice list of references to prior cplx-valued net studies

\citep{hirose_complex-valued_2009}
\citep{hirose_generalization_2012}
[9] A. Hirose, “Complex-valued neural networks: The merits and their origins,” IJCNN, 2009.
* radar or sattelite imaging, sonar or radio signals, communications (references therein).
* geometry preserving transformations


\citep{danihelka_associative_2016}
Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. arXiv preprint arXiv:1511.06464, 2015.
Ivo Danihelka, Greg Wayne, Benigno Uria, Nal Kalchbrenner, and Alex Graves. Associative long short-term memory. arXiv preprint arXiv:1602.03032, 2016.
Scott Wisdom, Thomas Powers, John Hershey, Jonathan Le Roux, and Les Atlas. Full-capacity unitary recur- rent neural networks. In Advances in Neural Information Processing Systems, pages 4880–4888, 2016.
* all three use cplx in rnn
T. Minemoto, T. Isokawa, H. Nishimura, and N. Matsui, “Feed forward neural network with random quaternionic neurons,” Signal Processing, vol. 136, pp. 59–68, 2017.


\citep{bruna_mathematical_2015}
[16] M. Tygert, J. Bruna, S. Chintala, Y. LeCun, S. Piantino, and A. Szlam, “A mathematical motivation for complex-valued convolutional net- works,” Neural Computation, vol. 28, no. 5, pp. 815 – 825, May 2016.

[8] A. Hirose, and S. Yoshida, “Generalization characteristics of complex-valued feedforward neural networks in relation to signal coherence,” IEEE Trans Neural Netw Learn Syst, 2012, 23(4): 541-551.




Motivation
\begin{itemize}
    \item joint phase rotation and amplitude attenuation yields better generalization
        \citep{hirose_generalization_2012}
    \item richer representation and synchrony
        \citep{reichert_neuronal_2014}
    % \item motivation for $\cplx$-valued convolutions
    %     \citep{bruna_mathematical_2015}
\end{itemize}

Applications:
\begin{itemize}
    \item \citep{danihelka_associative_2016}  % $\cplx$ RNN with holographic associative memory
    sequence prediction, XML modelling, neural arithmetic, sequence replication
    \item \citep{wisdom_full-capacity_2016}  % unitary transition matrices
    pixel-MNIST, spectral speech modelling (one-step-ahead stft prediction),
    dynamical system identification, sequence replication
    \item \citep{popa_complex-valued_2017}  % $\cplx$-valued convolutional NN 
    MNIST CIFAR image classification, \citep{zhang_complex-valued_2017}
    terrain surface classification
\end{itemize}




$\cplx$-valued Batchnorm and Init
\citep{trabelsi_deep_2018}
    apply to speech spectrum prediction, music transcription

complex-valued transformer
\citet{yang_complex_2019}



Quternion networks
\citep{gaudet_deep_2018} image classification and segmentation
\citep{vecchi_compressing_2020} LASSO regularizer for quaternion nets

Pruning and quantization
\citep{wu_compressing_2019}

Our contribution
\citep{nazarov_bayesian_2020} Complex-valued Varaitional Dropout

% motivation for complex valued networks
%  * applications with naturally C-valued data
%  * list of references which consider complex-valued
%    networks and their application and motivation


