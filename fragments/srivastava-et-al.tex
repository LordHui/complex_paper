(dropout)
prevents overfitting and provides a way to combine (probabilistically) exponentially
many models with shared weights during training, and a single network at test time.
Stochastic Grad descent accretes brittle co-adaptations that overfit: work well for training
data but fail to generalize. During training sample a thinned network by dropping out
units within each element of the minibatch by zeroing intermediate features. Injecting
noise severs these coadaptations by making features unreliable. On of the drawbacks
is increased training time, though.

(on denoising AE)
noise/ dropout can be injected into intermediate states which effectively accomplishes
a form of model averaging.

In dropout each models (through the distribution on masks) is weighted equally, whereas
in bayesian networks the prior distribution and the likelihood in model-space does the
weighting.

(trick)
constraining weights to within a norm-ball of fixed radius (with projection). This makes
it possible to use huge learning rates without blowup.

max-norm regularization, large decaying learning rates, and high momentum

(Gaussian dropout)
noted that $z\sim \mathcal{N}(1,1)$ works better than Bernoulli; Gaussian has the highest
entropy and Bernoulli -- the lowest.

(on TIMIT)
a benchmark for clean speech recognition

structure 120 dim/frame x 21 frames
train 1.1M frames, test 58K frames

windows of 21-log filter bank frames to predict the label of a central frame


(regression)
$$
\mathbb{E}_{z\sim {Ber}} \|y - (z\odot X) W\|^2
    % = \mathbb{E}_{z\sim {Ber}} \|y - p X W\|^2
    %     + \langel y - p X W, p X W - (z\odot X) W \rangle
    %     + \|p X W - (z\odot X) W\|^2
    = \|y - p X W\|^2
        % + \mathbb{E}_{z\sim {Ber}} \| ((z - p) \odot X) W \|^2
        % + \mathbb{E}_{z\sim {Ber}} \tr D_{z - p}^2 X W W^\top X^\top
        + p(1 - p) \|X W \|^2
    \,. $$


