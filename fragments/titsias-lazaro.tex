Doubly Stochasticc Variational Bayes for no-conjugate Inference

continuous parameter space

(on bayes) probabilistic framework for inference that combines prior knowledge with observed
data in a principled manner. However apart from simple cases involving conjugate distributions
the Bayesian computations are intractable and approximations are used.

Such approximations are either Markov Chain Monte Carlo methods that sample from the unnormalized
posterior and need to converge or Bayesian variational inference (Jordan et al. 1999), that
cast the inference problem as optimization and finds the best approximation to the posterior
within some parametric family. MCMC samples well, but needs time to converge to the stationary
distribution and VI still has an expectation in the objective (ELBO).

Stochastic approximation (Robbins & Monroe, 1951)

a general (standard) parameter-free source noise distribution $\phi(\varesilon)$, which, coupled
with a parameterized diffeomorphism, induces the used variational posterior approximation.
Flexibility: from the diffeomorphism, from the structure of the standard parameter-free noise
(fully factorized with special univariate marginals for which exact simulation is possible).

Standard var bayes setting is to use KL-divergence (Jordan, 1999) between the var posterior
and the true posterior.

(DSVI) stochasticity from var. distribution and from dataset sub-sampling, internalized the
reparam. trick of (Kingma et al., 2013), and derived (expected wrt var distribution) gradients
for the parameters. Essentially a precursor to SGVB.

The goal is to keep the estimators of the gradients as unbiased as possible.

(lit-review) compare reinforcement trick (score function approach) and reparameterization trick
(path-wise differentiation). Reinf. has undesirable random walk behaviour due to the score
function and does not utilize info form the log-joint density. Can add control variates (depends
on var-param only) to stabilize the variance.

(tricks) TT/TR tensor format for compression (lossy), sampling (w. variance control), importance
weights (direct in expectations and indirect as in Metropolis Hastings), domain/range truncation
and monotone ct, dominated ct, martingale ct applications.

(ard) (Tippping, 2001) uses proper Gaussian prior allows efficiency since some part of the
expectation can be optimized analytically. The optimized kl-div encourages sparsity: shrinks
the var. mean to zero if the dimension is irrelevant.
