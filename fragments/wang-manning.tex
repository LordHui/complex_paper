(dropout)
dropout randomly zeroes hidden units during training. This prevents co-adaptiation and 
helps regularize by lowering the trust in a feature. It can be viewed as averaging many
netowrks with shared weights.

Propose fast dropout via Gaussian approximation and without sampling. The Gaussian
approximation is justified by the CLT. In addition they derive the gradient of the objective,
and approximate the integrals using logit-probit trick (MacKay; 1992). The shortcoming
of their method in more complicated gradient in the backprop.

(details)
Instead of sampling bernoulli masks for the MC estimate of the gradient, use gaussian
approximation of the outputs within the graident evaluation. They propose a special
gradient estimator that takes into account partial derivatives wrt $\mu$ and $\sigma^2$
of the approximation and uses finite difference approximations for changes in the latter
conditioned on the dropout rvs.

They note that $y_j = e_j^\top W D_z x$ is a weigheted sum of bernoulli rv's with a
unimodal or bounded distributed $x_i w_i$. Then they apply the CLT and argue that if
x-dim is large then $y_j \sim \matcal{N}(...)$  with mean  $e_j^\top W D_p x$ and
variance $\sum_i (W_{ji} x_i)^2 p_i(1 - p_i)$. 

(closed form sigmoid approximation)
the Gaussain expected value of a logistic function via scaled probit. See (MacKay; 1992)
$\sigma(x)$ is close to a gaussian integral, i.e. $
  \sigma(x) \approx \Phi(\sqrt{\tfrac\pi8} x)
$ for $
  \phi(x) = \int_{-\infty}^x \tfrac1{\sqrt{2\pi}} e^{-\tfrac{t^2}2} dt
$, and then by fubini and differentiation under the integral, that
$$
\int \Phi(\lambda x) \mathcal{N}(x, \mu, \sigma^2) dx
    = \Phi\bigl(
        \tfrac\mu{\sqrt{\lambda^{-2} + \sigma^4}}
    \bigr)
    \,. $$

(augmetnation as noise) Transformation can be locally approximated by its Lie derivative
(Simard et al.; 1996) as a weighted matrix operationd (plus identity) with gaussian weights.
The authors argue thet the techniques they offer can be used to speed up this.

(other)
in dropout framework uncertainty comes form the dropout mask, in Bayesian models -- form
weight uncertainty. 

Their analysis resembles derivation of the SGVB (DSVI): they differentialte the log-likeihood
loss.

Argue that using approximation makes it obvious that dropout approximates the ELBO for
approximate posteriors of the form $
  q(w) = \mathcal{N}(w\mid \mu, \alpha^\mu^2)
$.
