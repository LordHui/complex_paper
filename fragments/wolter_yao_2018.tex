(experiment)
For musicnet they use FFT features from a sliding window of size 2048 with stirde
512 and perform 128-multioutput classification (unlike Trabelsi who predicct 84).

They study various gating functions on neural addition problem.

(on complex networks)
They refrain the general sentiment that the key advantage of complex-valued networks
against paired (double-dim) real networks, is the intrinsic regularization induced by
complex arithemtic operations.

A complex number $z\in \mathbb{C}$ as represented as $z = u + j v$ for reals $u, v$
with $u = \Re z$ and $v = \Im z$. The complex conjugate $\overline{z} = u - j v$, and
polar representation of $z = \lvert z \rvert e^{j \theta}$ where the $\theta$ is the
phase (argument $\arg z$, angle $\angle z$) and $\lvert z \rvert$ is the modulus
(magnitude).

The nonlinearities proposed in the earlier literature are known as the split-complex
approach, whereby a function is appled to real and imaginary components, or phase and
magnitude of a complex number separately. \citet{hirose_complex_2009} proposed 
$$
\sigma_b(z)
    = \tanh{\Bigl(\frac{\lvert z \rvert}{b}\Bigr)}
        \frac{\overline{z}}{\lvert z \rvert}  % why do they use conjugate?
\,, $$
and \citet{arjovsky_unitary_2016} used a ReLU varinot of Hirose's activation: modRelU
$$
\sigma_b(z)
    = \max{\bigl\{0, \lvert z \rvert + b\bigr\}}
        \frac{\overline{z}}{\lvert z \rvert}
\,. $$
To transform outputs from $\mathbb{C}$ to $\mathbb{R}$, \citet{wolter_complex_2018}
propose to use
$$
\mathbb{C}^n \to \mathbb{R}^m
    \colon z \mapsto W_r \Re z + W_i \Im z + b
    \,, $$
with $W_r, W_i \in \mathbb{R}^{m \times n}$.


(on rnns)
Propose novel complex-valued gating mechanism to unitary-RNN of Wisdom et al. (2016).
Investigate the effects of different gating mechanisms and conduct experiments on human
motion prediction adn MusicNet. End up getting 53\% AP in the latter.

Study gating mechanisms since they are one of the common heuristics, employed to
stabilize RNN training (vanishing or exploding grads). Gates adaptively protect the
cell state from bad updates, tha may cause instability. Besides gating, there are
gradient clipping, norm-preserving state transition matrices (unitary / orthogonal).

The studied gates include 
$$
\sigma_b(z)
    = \sigma(\alpha \Re z + \beta \Im z)
    \,, \alpha,\beta \in [0, 1] 
\,. $$
% Vanishing gradients are disrtibuted over a sum, exploding are clipped.

(on norm-preservation)
they cite \citep{arjovsky_unitary_2016}, 
Observe that for a scalar-valued function $f \colon \mathbb{C}^{n\times k} \to \mathbb{R}$
with $J_z = \nabla_z f$ and $J_{\overline{z}} f =  \nabla_{\overline{z}} f$, and
$W \in \mathbb{C}^{d \times k}$ by $\mathbb{CR}$-calculus we have
$$
\partial f
    = \mathop{tr} J_z^H \bigl((\partial x) W + x \partial W\bigr)
    + \mathop{tr} J_{\overline{z}}^H \bigl(
        (\partial \overline{x}) \overline{W}
        + \overline{x} \partial \overline{W}
    \bigr)
    \,, $$
whence
$$ 
\tfrac{\partial f(x W)}{\partial x}
    = J_{\overline{z}} W^H \Big\vert_{z = xW}
    \,, \tfrac{\partial f(x W)}{\partial \overline{x}}
        = J_{z} W^\top \Big\vert_{z = xW}
    \,. $$

Thus to preserve backwards gradinets and ensure forward stability in the case of $d=k$
the operator norm of $W$ should be $1$.