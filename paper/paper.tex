\documentclass[a4paper,10pt,twocolumn]{article}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{natbib}
% \setcitestyle{authoryear,open={((},close={))}}

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{xcolor}
\usepackage{graphicx, url}
\usepackage{grffile}

\graphicspath{{../assets/}}

\usepackage{subcaption}

\usepackage{booktabs}

\title{Bayesian Sparsification of Deep $\cplx$-valued networks}
\author{Ivan Nazarov, and Evgeny Burnaev}

%% notation
\newcommand{\real}{\mathbb{R}}
\newcommand{\cplx}{\mathbb{C}}
\newcommand{\tr}[1]{\mathop{tr}{#1}}

\newcommand{\hop}{{\mkern-1.5mu\dagger}}
\newcommand{\conj}[1]{\overline{#1}}

% \renewcommand{\top}{{\mkern-1.5mu\intercal}}
\renewcommand{\vec}[1]{\overrightarrow{#1}}
\newcommand{\diag}[1]{\mathrm{diag}{#1}}

%% drafting macro
\newcommand{\important}[1]{\textbf{\!\colorbox{red}{#1}\!}}
\newcommand{\attn}[2]{\textbf{\color{red} #2~\textsuperscript{\textit{[#1]}}}}
\newcommand{\verify}[1]{\textit{\!\colorbox{red}{#1}\!}}
\newcommand{\rewrite}[1]{\attn{rewrite}{#1}}
\newcommand{\todo}[1]{{\color{blue} [TODO]} \important{#1}}

%% red-highlight missing citations
\usepackage{etoolbox}
\makeatletter
\patchcmd{\@citex}{\bfseries ?}{\colorbox{red}{\bfseries ?}}{}{}
\makeatother

\begin{document}
\maketitle

\begin{abstract}
With continual miniaturization ever more applications of deep learning can be found
in embedded systems, where it is common to encounter data with natural complex domain
representation. To this end we extend Variational Dropout to complex-valued neural
networks. We verify the proposed Bayesian technique and assess the performance-compression
trade-off by conducting a large-scale numerical study of $\cplx$-valued networks on
two tasks: image recognition on MNIST-like and CIFAR10 datasets and music transcription
on MusicNet. We reliably replicate the state-of-the-art result by \citet{trabelsi_deep_2017}
for $\cplx$-valued networks on MusicNet with $50-100\times$ compression rate.
\end{abstract}

\section{Introduction} % (fold)
\label{sec:introduction}

% general intro text with motivation
Deep neural networks are an integral part of machine learning and data science toolset
for practical data-driven problem solving. With continual miniaturization ever more
applications can be found in embedded systems. Common embedded applications include
on-device image recognition and signal processing. Despite recent advances in generalization
and optimization theory specific to deep networks, deploying in actual embedded hardware
remains a challenge due to storage, real-time throughput, and arithmetic complexity
restrictions \citep{he_amc:_2018}. Therefore, compression methods for achieving high
model sparsity and numerical efficiency without losing much in performance are especially
relevant.
% Solving the storage and artihmetic constraints encourages development of model compression
% and sparsification methods, that focus on favourable trade-off between performance and size.

Inherently complex-valued nature of data in signal processing, specifically, acoustic or radio
frequency signal analysis, has been the key driver behind the adoption of $\cplx$-networks.
%
\citet{hirose_complex-valued_2009} argues that the key merit of $\cplx$-valued neural
networks is simultaneous phase rotation and amplitude adjustment, intrinsic to multiplication
in $\cplx$ field. Despite inherently higher arithmetic complexity, which stems from up to four
floating point operations per single $\cplx$ multiplication
$\cplx$-arithmetic reduces ineffective degrees of freedom in comparison with an analogous
double-dimensional $\real$-valued network. \citet{hirose_complex-valued_2009} lists numerous
applications related to acoustic and radio signal processing and demonstrates superiority of
$\cplx$-networks in the task of shallow landmine detection using ground penetrating radar
imaging. More recently they have been applied to music transcription and speech recognition
\citep{trabelsi_deep_2017}. \citet{arjovsky_unitary_2016} propose $\cplx$-valued unitary-RNNs
to deal with exploding~/~vanishing gradients and apply the recurrent models to long-term
dependence prediction tasks. \citet{wisdom_full-capacity_2016} expand on their work by optimizing
recurrent weights on the $\cplx^{n \times n}$ Stiefel manifold. And \citet{wolter_complex_2018}
investigate different $\cplx$-valued gating mechanisms for unitary-RNNs and conduct experiments
on human motion prediction and music transcription. \citet{yang_complex_2019} develop complex
transformer, with $\cplx$-attention and complex encoder-decoder, and apply it to music
transcription and wireless signal classification.

% complex-valued distributions \citep{pav_moments_2015,taubock_complex-valued_2012},
% and \citep{karseras_caution:_2014}

% motivation and other methods
However, in spite of relative success of $\cplx$-valued networks, compression methods tailored
to $\cplx$-domain remain a niche field of research.
%
Computational efficiency improvement and model compression methods such as quantization
\citep{uhlich_differentiable_2019}, integer-based arithmetic \citep{lin_fixed_2016,chen_fxpnet_2017},
and knowledge distillation \citep{hinton_distilling_2015} appear to be directly applicable
to $\cplx$-valued networks.
%
Certain model sparsification methods, e.g. sparsity inducing regularizers, and magnitude
pruning \citep{zuo_compression_2019}, which reactivates zeroed parameters based on gradients,
can be adapted directly to $\cplx$-valued models. Other methods require additional considerations.
% the more explicitly zero parameters there is, the less computations is needed.

% segue into bayesian inference
Bayesian Inference is a principled framework of reasoning about uncertainty, which updates
prior beliefs in accordance with the likelihood of empirical observations into posterior
belief. Under suitable modelling and distributional assumptions, Bayesian methods can be
used towards model sparsification, e.g. Variational Dropout \citep{kingma_variational_2015,molchanov_variational_2017}.

% (sparsity tech large-scale models) gale-elsen-hooker.tex
\citet{gale_state_2019} compare Variational Dropout, magnitude pruning, and $\ell_0$
regularization \citep{louizos_learning_2017} on large-scale models. The latter is a
probabilistic regularization method, which penalizes the expected number of non-zero
entries in $[0, 1]$-valued stochastic parameter masks with learnable hard-concrete
distribution with atoms at $0$ and $1$ \citep{maddison_concrete_2016,jang_categorical_2017}.
Their results suggest that Variational Dropout may achieve good accuracy-sparsity balance
and outperform pruning and $\ell_0$ in deep architectures, although pruning is prefered for
simplicity, stability and speed. They also observe that dropout induces non-uniform sparsity
throughout the model, which \citet{he_amc:_2018} has shown to be essential for superior
compression.
% Magnitude pruning induces user-specified sparsity distributions.
%
% L0 and dropout work in the low-to-mid sparsity range, variational dropout consistently
% ranked behind L0 on Transformer, and was bested by magnitude pruning for sparsity
% levels of 80\% and up. Variational dropout consistently produces models on-par or better
% than magnitude pruning on ResNet-50, and l0 fails to produce sparse models at all.
%

% our contribution
Inspired by variational and stochastic sparsification techniques we extend Bayesian dropout
to $\cplx$-valued networks. We assess the performance-compression trade-off of the proposed
Bayesian technique by conducting a large-scale numerical study of $\cplx$-valued networks
on two tasks: image recognition on MNIST-like and CIFAR10 datasets and music transcription
on MusicNet \citep{thickstun_learning_2017}.

The paper is structured as follows. In sec.~\ref{sec:variational_dropout} we review Bayesian
dropout techniques, and in sec.~\ref{sec:c_valued_networks} we provide a brief summary of
the inner working of complex-valued networks as functions of complex argument. The main
contribution of this study is presented in sec.~\ref{sec:c_variational_dropout},
where we consider different variational approximations and priors, outline the tricks
and derive the penalty terms in the evidence lower bound. In sec.~\ref{sec:experiments}
we evaluate the sparsification rate, compare the resulting performance of various $\cplx$-networks
proposed in prior work, and discuss the outcomes.

% section introduction (end)


\section{Variational Dropout} % (fold)
\label{sec:variational_dropout}

In general, the core of Bayesian Inference can be summarized as follows: given a prior
distribution $\pi(m)$ on models (hypotheses) $\mathcal{M}$, utilize the empirical evidence
$D$ to update the assumptions by considering the likelihood of the observations under each
$m \in \mathcal{M}$. Models are represented by a parametric family indexed by parameters
$\omega \in \Omega$ and each $m_\omega(\cdot)$ specifies the conditional distribution of
the data $
  D = (z_i)_{i=1}^N
$. The posterior distribution $p(\omega \mid D)$, derived using the Bayes rule $
  p(m \mid D) = \tfrac{p(D \mid m) \pi(m)}{p(D)}
$ with $
  p(D) = \mathbb{E}_{\pi(m)} p(D \mid m)
$, provides useful information about yet unobserved data and model parameters, e.g
classification uncertainty, predictive statistics, and parameter relevance.

Save for the relatively simple cases, the posterior distribution is intractable, and
therefore exactness Bayesian Inference is traded for tractability and scalability of
Variational Bayesian Inference. Instead of deriving the posterior $p(\omega \mid D)$,
this approach postulates an optimization problem for finding an approximation $q$,
which is close to $p(\omega \mid D)$ in terms of a proximity score $\rho$
\begin{equation}  \label{eq:variational-progam}
  q_*
    \in \arg \min_{\theta} \rho\bigl(
      q_\theta(\omega), p(\omega \mid D)
    \bigr)
    \,,
\end{equation}
over a tractable parametric family of distributions on the parameter space $
  \mathcal{Q} = \{Q_\theta(d\omega) \colon \theta \in \Theta\}
$, $\theta$ -- generic variational parameter. The variational approximation class
$\mathcal{Q}$ is chosen so that its members have tractable densities $
  q_\theta(\omega) d\omega
$, can be sampled from and have either tractable $\log$-derivatives $
  \omega \mapsto \nabla_\theta \log q_\theta(\omega)
$ or can be represented as differentiable transformations of non-parametric noise
\citep{williams_simple_1992,kingma_auto-encoding_2014,figurnov_implicit_2019}.
% (reparameterization trick, pathwise gradient) $q_{\theta}(d\omega)$ is a push-forward
% of $p(d\varepsilon)$ by a differentiable map $g(\varepsilon; \theta)$.
% % http://stillbreeze.github.io/REINFORCE-vs-Reparameterization-trick/#fn:1
%
The most common choice for $\rho$ is Kullback-Leibler divergence of density $q$ from $p$,
\begin{equation}  \label{eq:kl-div-def}
  KL(q \| p)
    % = \int \frac{dQ}{dP} \log{\frac{dQ}{dP}} P(d\omega)
    % = \int \frac{q(\omega)}{p(\omega)} \log{\frac{q(\omega)}{p(\omega)}} p(\omega) d\omega
    % = \mathbb{E}_{\omega \sim q}
    %   \log\frac{q(\omega)}{p(\omega)}
    = \mathbb{E}_{\omega \sim q}
      \log{q(\omega)}
    - \mathbb{E}_{\omega \sim q}
      \log{p(\omega)}
    \,,
\end{equation}
% and, despite it not being a metric and requiring distributions with nested support,
the main reason being that it satisfies the following identity for well-behaved densities:
% $(q, p, \pi)$
\begin{multline}  \label{eq:kl-div-master}
  % \log p(D)
    % = \mathbb{E}_{q(\omega)} \log \frac{p(D, \omega)}{p(\omega \mid D)}
    % = \mathbb{E}_{q(\omega)} \log \frac{
    %   p(D \mid \omega) \pi(\omega)
    % }{p(\omega \mid D)}
    % = \mathbb{E}_{q(\omega)} \log \frac{
    %   p(D \mid \omega) \pi(\omega)
    % }{q(\omega)}
    % + \mathbb{E}_{q(\omega)} \log \frac{q(\omega)}{p(\omega \mid D)}
    % - \mathbb{E}_{q(\omega)} \log \frac{q(\omega)}{\pi(\omega)}
    % = \mathbb{E}_{q(\omega)} \log{p(D \mid \omega)}
    % + \mathbb{E}_{q(\omega)} \log \frac{q(\omega)}{p(\omega \mid D)}
  \log p(D)
    - KL(q \| p(\omega \mid D))
    \\ = \mathbb{E}_{q(\omega)} \log{p(D \mid \omega)}
    - KL(q \| \pi)
    \,.
\end{multline}
This leads to an equivalent problem for \eqref{eq:variational-progam} of maximizing
the \textit{evidence lower bound} (ELBO)
\begin{equation}  \label{eq:elbo_general}
  \mathcal{L}(\theta; \phi, \lambda)
    = - KL(q_{\theta} \| \pi_{\lambda})
      + \mathbb{E}_{\omega \sim q_{\theta}}
        \log p_{\phi}(D \mid \omega)
      % \mathbb{E}_{\omega \sim q_{\theta}}
        % \log\frac{q_\theta(\omega)}{\pi_{\lambda}(\omega)}
  % = \mathbb{E}_{\omega \sim q_{\theta}}
  %     \log{p_{\phi}(D \mid \omega) \pi_{\lambda}(\omega)}
  %   + \mathbb{H}(q_{\theta})
  \,,
\end{equation}
where $\lambda$, $\theta$ and $\phi$ are optimization parameters of the prior $\pi$,
approximation $q$ and the likelihood $p(D\mid \omega)$, respectively. Other objectives
are possible, besides \eqref{eq:elbo_general}, provided $p(\omega \mid D)$ is evaluated
only through $\log p(D \mid \omega)$ and $\log \pi(\omega)$, \citep{ranganath_operator_2018}.

\textit{Stochastic Gradient Variational Bayes} (SGVB), proposed by \citet{kingma_auto-encoding_2014},
replaces the Variational objective \eqref{eq:elbo_general} by its unbiased differentiable
Monte-Carlo estimator, which makes it possible to employ stochastic gradient optimization
methods to solve \eqref{eq:variational-progam}. If $
  \omega \sim q_{\theta}
$ is equivalent in distribution to $
  \omega = g(\varepsilon; \theta)
$ for some differentiable $
  g(\varepsilon; \theta)
$ and non-parametric random variable $
    \varepsilon \sim p_\varepsilon
$, then the SGVB objective is
\begin{equation}  \label{eq:sgvb_objective}
  \widetilde{\mathcal{L}} %(\theta; \phi, \lambda)
    = - KL(q_{\theta} \| \pi_{\lambda})
      % + \frac{N}{\lvert B \rvert} \sum_{i\in B}
        % \log p(z_i \mid \omega_i)b
      + \frac1{L} \sum_{l=1}^L
        \log p_{\phi}(D \mid g(\varepsilon_{l}; \theta))
        % \log p_{\phi}(D \mid \omega_{l})
        %   \Big\vert_{\omega_{l} = g(\varepsilon_{l}; \theta)}
      % =
      % - KL(q_{\theta} \| \pi)
      % + N \mathbb{E}_{B \sim \{D\}_M \otimes q_\theta^M}
        % \hat{\mathbb{E}}_{z,\omega \sim B}
          % \log p(z \mid \omega)
    \,, 
\end{equation}
where $
  (\varepsilon_{l})_{l=1}^L
$ is an iid sample from $p_\varepsilon$, the KL-divergence term replaced by unbiased
differentiable finite sample estimator if it is intractable.
%
For the purposes of this study, we assume that the likelihood satisfies $
  \log p_\phi(D\mid \omega)
    = \sum_i \log p_\phi(z_i \mid \omega)
$, i.e. the observed data $
  D = (z_i)_{i=1}^N
$ is conditionally independent, and consider \eqref{eq:sgvb_objective} with $L=1$
and random mini-batches from $D$:
\begin{equation}  \label{eq:elbo}
  \widetilde{\mathcal{L}} %(\theta; \phi, \lambda)
    = - KL(q_{\theta} \| \pi_{\lambda})
      + \frac{N}{M} \sum_{k=1}^M
        \log p_{\phi}(z_{i_k} \mid g(\varepsilon_k; \theta))
    \,,
\end{equation}
for a random subsample $(z_{i_k})_{k=1}^M$ and iid $\varepsilon_k \sim p_\varepsilon$.
%
% (DSVI) titsias-lazaro.tex \todo{DSVI} \citep{titsias_doubly_2014}

With a special family of posterior approximation, Variational Inference can be used as
a regularization method and as a model sparsification technique.

% on dropout
Dropout \citep{hinton_improving_2012} prevents overfitting by injecting multiplicative
binary noise into layer's weights, which breaks up co-adaptations that could occur
during training. \citet{wang_fast_2013} argue that the overall effect of binary dropout
on the intermediate outputs via the Central Limit Theorem can be approximated by a Gaussian
with weight-input dependent mean and variance. \citet{srivastava_dropout_2014} propose
using independent $\mathcal{N}(1,1)$ noise, arguing that higher entropy of a Gaussian has
better regularizing effect. In a recent study concerning multitask learning
\citet{cheung_superposition_2019} have shown the possibility of storing task-specific
parameters in non-destructive superposition within a single network. In particular, their
argument implies that if the single task setting is viewed as multitask learning with
replicated task, then by sampling uncorrelated binary masks Dropout acts as a superposition
method, utilizing the learning capacity of the network better.
% many identical copies of the same task; \cite[Appendix A.1]{cheung_superposition_2019}
% (fast dropout gauss-approx) wang-manning.tex
% (gauss dropout) srivastava-et-al.tex

Dropout can be viewed as a Bayesian model averaging method, where models share parameters
and are weighted equally. Thus, \citet{kingma_variational_2015} consider Dropout, DropConnect
\citep{wan_regularization_2013}, and Gaussian dropout \citep{wang_fast_2013} through the
lens of Bayesian Inference methods and propose \textit{Variational Dropout}. They argue that
the multiplicative noise introduced by these methods induces a distribution equivalent to
a fully factorized variational posterior of the form $
  q_\theta(\omega) = \prod_j q_{\theta}(\omega_j)
$, where $q_{\theta}(\omega_j)$ is $\omega_j = \mu_j \xi_j$ with $
  \xi_j \sim p_\theta(\xi_j)
$ iid from some $p_\theta(\xi)$.

% Binary dropout with rate $p \in (0, 1)$ uses $
%   p_\xi
%     = \mathcal{B}\bigl(
%       \{0, \tfrac1{1-p}\}, 1-p
%     \bigr)
% $, whereas in Gaussian dropout $
%   \xi \sim \mathcal{N}(1, \alpha)
% $ for $\alpha = \tfrac{p}{1-p}$.
% This suggests making $\alpha$ a variational parameter
% and optimizing it in \eqref{eq:elbo_general} with an appropriate penalty term. Thus,
%
Variational Dropout assumes fully factorized Gaussian approximate posterior $
  q_\theta(\omega)
    = \prod_j \mathcal{N}(\omega_j \vert\, \mu_j, \alpha_j \mu_j^2)
$ and factorized log-uniform prior $\pi(\omega)$ with $
  \pi(\omega_j) \propto \lvert \omega_j \rvert^{-1}
$. The divergence term in \eqref{eq:elbo} unravels into the following sum:
\begin{equation}  \label{eq:improper-kl-div-real}
  KL(q_\theta \| \pi)
    \propto
      \frac12 \sum_j \mathbb{E}_{\varepsilon \sim \mathcal{N}(0, 1)}
        \log{\Bigl\lvert \tfrac1{\sqrt{\alpha_j}} + \varepsilon \Bigr\rvert^2}
      % \frac12 \mathbb{E}_{\varepsilon \sim \mathcal{N}(0, 1)}
      %   \log{\bigl\lvert 1 + \sqrt{\alpha} \varepsilon \bigr\rvert^2}
      % - \frac12 \log{\alpha}
  \,.
\end{equation}
\citet{kingma_variational_2015} approximate this expression by a non-linear polynomial
regression over $\alpha \in (0, 1)$. \citet{molchanov_variational_2017} refine the
approximation by using a nonlinear regression based on sigmoid and soft-plus.
% see p.6 sec 3.4 of kingma_variational_2015
In appendix~\ref{sec:real-chisq-grad} we verify the value and the derivative of their
approximation against the Monte-Carlo estimate of \eqref{eq:improper-kl-div-real} for $\alpha$
varying over a fine logarithmic grid and the gradient of \eqref{eq:improper-kl-div-real},
for which we obtain an exact expression.
% (!) good for SGVB: gradient-based methods require unbiased gradient estimates
% (in) it is in poor taste to brag about this beign the first time anyone has derived a
% derivative of an intractable function.

The key role in the factorization is played by $\alpha_j$, which reflects the relevance
of the parameter $\omega_j$ it is associated to, by being the ratio of its squared mean
to its variance. Thus, \citet{molchanov_variational_2017} focus on the model sparsification
capabilities of Variational Dropout, optimizing $\alpha_j$ for each individual parameter,
or across groups of parameters for structured sparsity. \citet{kharitonov_variational_2018}
address theoretical issues with improper priors and propose a method based on \textit{
Automatic Relevance Determination} procedure, which falls under \textit{Empirical Bayes}
approach that fits a prior distribution while performing Variational Inference. In this
method the log-uniform factor $\pi(\omega_j)$ is replaced by a proper Gaussian ARD
prior $
  \mathcal{N}(\omega_j \vert\, 0, \tau^{-1}_j)
$ with learnable precision $\tau_j > 0$, \citep{neal_bayesian_1996}. Maximizing
\eqref{eq:elbo} over $\tau$, holding other parameters fixed, yields $
  \tau^*_j = {(\mu_j^2 + \sigma^2_j)}^{-1}
$, whence
\begin{equation}  \label{eq:ard-kl-div-real}
  KL(q_\theta \| \pi)
    = \frac12 \sum_j \log{\bigl(1 + \tfrac1{\alpha_j} \bigr)}
    \,.
\end{equation}

Another contribution of \citep{kingma_variational_2015} is the analysis of the effects of the
\textit{local reparameterization trick} on the variance of the gradient of \eqref{eq:elbo}.
Proposed by \citet{wang_fast_2013} to speed up Gaussian dropout, this trick translates
uncertainty from parameters of the model to intermediate outputs within the network:
the stochasticity of $
  W \in \mathbb{R}^{n\times m}
$ with $
  q(W) = \prod_{ij}
    \mathcal{N}(w_{ij}\vert\, \mu_{ij}, \alpha_{ij} \mu_{ij}^2)
$ in a linear layer is pushed to its output $
  y = b + W^\top x
$,
\begin{equation}  \label{eq:r-gauss-trick}
  q(y) = \prod_i \mathcal{N}\Bigl(
        y_i \big\vert\,
        b_i + \sum_j \mu_{ij} x_j,
        \sum_j \alpha_{ij} \mu_{ij}^2 x_j^2
    \Bigr)
  \,.
\end{equation}
%
\citet{kingma_variational_2015} show, that for the case of fully factorized Gaussian
posterior approximations, using \eqref{eq:r-gauss-trick} decorrelates the estimators
\eqref{eq:elbo} within the mini-batch and makes the gradient estimator $
  \nabla_\theta \tilde{\mathcal{L}}
$ more statistically efficient. Specifically for the problem of selecting relevant parameters
\citet{molchanov_variational_2017} further improve the efficiency of the estimator of the $
  \nabla_\mu \tilde{\mathcal{L}}
$ by introducing \textit{additive noise parameterization}. They revert the $(\mu, \alpha)$
parameterization back to the equivalent $(\mu, \sigma^2)$, since it stabilizes gradients
of SGVB \eqref{eq:elbo}, by rendering the gradient with respect to $\mu$ independent from
the local output noise, injected by \eqref{eq:r-gauss-trick}. The relevance $\alpha$ is
calculated using the ratio $
  \tfrac{\sigma^2}{\mu^2}
$ when needed in the divergence term.

% section bayesian_dropout (end)


\section{$\cplx$-valued networks} % (fold)
\label{sec:c_valued_networks}

Complex networks are, in general, non-holomorphic, i.e. not $\cplx$-differentiable, which is
exacerbated by the loss being real-valued. Indeed a $\cplx$-differentiable real-valued function is
necessarily trivial by Cauchy-Riemann conditions. This issue is dealt with by employing Wirtinger,
or $\cplx\real$ calculus, which generalizes holomorphic calculus to non-holomorphic functions
of $\cplx$-argument \citep{adali_complex-valued_2011,boeddeker_computation_2019}. It satisfies
the product and chain rules, respects complex conjugation and linearity, but most importantly the
$\cplx\real$-differential of a function on $\cplx$ coincides with its classical differential as
a function on $\real^2$. Essentially, it allows straightforward retrofiting of $\cplx$-valued
networks into existing $\real$ deep learning auto-differentiation frameworks. It was considered
as a basis for $\cplx$ version of back-propagation by \citet{benvenuto_complex_1992}, (see
appendix~\ref{sub:wirtinger_calculus}).
% \todo{read on holomorphic nets}

More recently general development of $\cplx$-valued networks has been continued by
\citet{trabelsi_deep_2017}, who outline the essential building blocks for deep $\cplx$-valued
networks and describe suitable representation and operations, including convolutional and
dense layers, $\cplx$-valued activations, complex batch-normalization and weight initialization.
Their approach coupled with $\cplx\real$ calculus for backprop transforms a $\cplx$-valued
network into an intricately connected $\real$-valued computational graph, constructured
to respects $\cplx$-arithmetic.
%
In particular, natural identification of $\cplx$ and $\real^2$ implies that dense linear
layers $
  L \colon \cplx^m \to \cplx^n
$ act upon their inputs as follows:
\begin{equation}  \label{eq:cplx-lin-op}
  {(\real^n)}^{2}
    \to {(\real^m)}^{2}
    \colon (u, v)
      \mapsto \Bigl(
        P u - Q v,
        P v + Q u
      \Bigr)
    \,,
\end{equation}
where $
  P, Q \colon \real^{n} \to \real^{m}
$ are unique operators such that $L = P + j Q$. A $\cplx$-convolutional layer with kernel
$W$ can be implemented as two $\real$-convolutions with kernels $\Re{W}$ and $\Im{W}$ using
\eqref{eq:cplx-lin-op}. Activations include trigonometric functions and representation-dependent
non-linearities \citep{hirose_complex-valued_2009}: polar (phase-amplitude) $
  z \mapsto \sigma(\lvert z \rvert) e^{j \arg z}
$ or planar (real-imaginary) $
  z \mapsto \sigma(\Re z) + j \sigma(\Im z)
$ for some $\real$-valued non-linearity $\sigma$, possibly with learnable parameters
\citep{trabelsi_deep_2017,wolter_complex_2018}.

% section c_valued_networks (end)


\section{$\cplx$-Variational Dropout} % (fold)
\label{sec:c_variational_dropout}

We propose to use fully factorized complex Gaussian posterior approximation and $\cplx$
variant of the local reparameterization trick to extend Bayesian Dropout techniques to
$\cplx$-valued networks. In this section we describe the approximation and derive the
divergence penalties in \eqref{eq:elbo}.

\subsection{$\cplx$-Gaussian Distribution} % (fold)
\label{sub:c_gauss_and_local_rep}

A vector $z\in \cplx^m$ has complex Gaussian distribution, $
  q(z) = \mathcal{N}^{\cplx}_m(\mu, \Gamma, C)
$ with mean $\mu \in \cplx^m$ and $\cplx^{m\times m}$ covariance and relation matrices
$\Gamma$ and $C$, respectively, if
\begin{equation}  \label{eq:cn-paired-real-density}
  \begin{pmatrix}
    \Re z \\ \Im z
  \end{pmatrix}
    \sim \mathcal{N}_{2 m}\biggl(
      \Bigl(
        \begin{smallmatrix}
          \Re \mu \\ \Im \mu
        \end{smallmatrix}
      \Bigr),
      \tfrac12 \Bigl(
        \begin{smallmatrix}
          \Re{(\Gamma + C)} & \Im{(C - \Gamma)} \\
          \Im{(\Gamma + C)} & \Re{(\Gamma - C)}
        \end{smallmatrix}
      \Bigr)
    \biggr)
    \,,
\end{equation}
provided $\Gamma \succeq 0$, $\Gamma^{\hop} = \Gamma$, $C^\top = C$, and $
  \conj{\Gamma} \succeq C^{\hop} \Gamma^{-1} C
$.
% notation
Here, $M^{\top}$ is the matrix transpose, $\conj{M}$ is elementwise complex
conjugation, and $M^{\hop} = (\conj{M})^\top$ denotes Hermitian conjugate.
%
Matrices $\Gamma$ and $C$ are given by $
  \mathbb{E} (z - \mu)(z - \mu)^{\hop}
$ and $
  \mathbb{E} (z - \mu)(z - \mu)^{\top}
$, respectively, and $z$ is a \textit{proper} $\cplx$-Gaussian vector if $z$ and
$\conj{z}$ are uncorrelated, i.e. $C = 0$.
%
The entropy of $z$ terms of $\Gamma$ and $C$ is
\begin{align}
  \mathbb{H}(q)
    & = - \mathbb{E}_{z \sim q} \log{q(z)}
    \notag \\
    % &
    % entropy of \mathcal{N}_n(\mu, \Sigma)
    %   % = \tfrac12 \log \det{(2 \pi \Sigma)}
    %   % + \tfrac12 \mathop{tr}{(
    %   %     \Sigma^{-1}
    %   %     \mathbb{E}_{x\sim q(x)}
    %   %       (x - \mu) (x - \mu)^\top
    %   %   )}
    %   = \tfrac12 \log \det{(2 \pi e \Sigma)}
    % = \tfrac12 \log \det{\biggl(
    %   2 \pi e \tfrac12
    %   \begin{pmatrix}
    %     \Re{(\Gamma + C)} & - \Im{(\Gamma - C)} \\
    %     \Im{(\Gamma + C)} &   \Re{(\Gamma - C)}
    %   \end{pmatrix}
    % \biggr)}
    % \notag \\
    % &
    % = \begin{bmatrix}
    %   r_1 + j r_2 \to r_1 \\  % sufficient for C=0 case
    %   j r_1 + r_2 \to r_2     % j conj of transformation for r1, det x 2
    % \end{bmatrix}
    % = \tfrac12 \log \det{\biggl(
    %   % \det{[I & j I \\ j I & I]} = \det{I} \det{I - jI I^{-1} jI} = \det{2 I}
    %   % left apply : thus dividing by \sqrt{2}
    %   \tfrac1{\sqrt{2}} 2 \pi e \tfrac12
    %   \begin{pmatrix}
    %     \Gamma + C & j (\Gamma - C) \\
    %     j (\conj{\Gamma} + \conj{C}) & \conj{\Gamma} - \conj{C}
    %   \end{pmatrix}
    % \biggr)}
      % % Implicitly using the fact that for a non-singular $\Omega$ we have $
      % %   \det{A}
      % %     = \tfrac{\det{(\Omega A)}}{\det{\Omega}}
      % %     = \tfrac{\det{(A \Omega)}}{\det{\Omega}}
      % % $.
    % \notag \\
    % &
    % = \begin{bmatrix}
    %   c_1 - j c_2 \to c_1 \\  % -j conj of transformation for c2
    %   c_2 - j c_1 \to c_2     % sufficient for C=0 case
    % \end{bmatrix}
    % = \tfrac12 \log \det{\biggl(
    %   % \det{[I & - j I \\ - j I & I]} = \det{I} \det{I - (-j)I I^{-1} (-j)I} = \det{2 I}
    %   % right apply : thus dividing by \sqrt{2}
    %   \tfrac12 2 \pi e \tfrac12
    %   \begin{pmatrix}
    %     2 \Gamma     & - 2 j C \\
    %     2 j \conj{C} & 2 \conj{\Gamma}
    %   \end{pmatrix}
    % \biggr)}
    % \notag \\
    % &
    % = \begin{bmatrix}
    %   r_2 - j \conj{C} \Gamma^{-1} r_1 \to r_2
    % \end{bmatrix}
    % = \tfrac12 \log \det{\biggl(
    %   % \det{[I & 0 \\ - j \conj{C} \Gamma^{-1} & I]} = \det{I} = 1
    %   \pi e
    %   \begin{pmatrix}
    %     \Gamma & - j C \\
    %     0 & \conj{\Gamma} - \conj{C} \Gamma^{-1} C
    %   \end{pmatrix}
    % \biggr)}
    % \notag \\
  % \label{eq:c-gauss-entropy-derivation}
    &
    = \tfrac12 \log \det{(\pi e \Gamma)}
      \det{(\pi e (\conj{\Gamma} - \conj{C} \Gamma^{-1} C))}
    \notag \\
  \label{eq:cn-proper-entropy}
    &
    % determinant commutes with complex conjugation, since it is multilinear
    % = \tfrac12 \log \det{(\pi e \Gamma)} \det{(\pi e (\conj{\Gamma} - \conj{C} \Gamma^{-1} C))}
    % = \tfrac12 \log \det{(\pi e \Gamma)} \det{(\pi e \conj{\Gamma})}
    % = \tfrac12 \log \det{(\pi e \Gamma)} \conj{\det{(\pi e \Gamma)}}
    % = \tfrac12 \log \bigl\lvert \det{(\pi e \Gamma)} \bigr\rvert^2
    \Bigl[
    = \log \bigl\lvert \det{(\pi e \Gamma)} \bigr\rvert
    \quad \text{for } C = 0
    \Bigr]
    \,.
\end{align}
%
% The roles of $\Gamma$ and $C$ are more evident in univariate case: here $
%   \Gamma = \sigma^2 \geq 0
% $, and $C = \sigma^2 \rho \in \cplx$ with $
%   \lvert \rho \rvert \leq 1
% $. $\sigma^2$ determines the ``dispersion'', whereas $\rho$ is determines the
% ``strength'' and ``direction'' of the linear relation between real and imaginary
% parts \citep{lapidoth_capacity_2003}.
% $$
% \frac{\sigma^2}2
%   \begin{pmatrix}
%     1 + \Re{\rho} & \Im{\rho} \\
%     \Im{\rho} & 1 - \Re{\rho}
%   \end{pmatrix}
%   = \frac{\sigma^2}2 
%       \begin{pmatrix}
%         1 + \lvert \rho \rvert \cos\theta
%           & \lvert \rho \rvert \sin\theta \\
%         \lvert \rho \rvert \sin\theta
%           & 1 - \lvert \rho \rvert \cos\theta
%       \end{pmatrix}
%   \,. $$

The $\cplx$-variant of the local reparameterization trick utilizes the fact that Gaussian
vectors are closed under affine transformations:
% $\cplx$-Gaussians are closed under affine transformations
\begin{equation}  \label{eq:cn-affine}
  b + A z \sim \mathcal{N}_n^{\cplx}\bigl(
      b + A\mu, A \Gamma A^{\hop}, A C A^\top
    \bigr)
  \,,
\end{equation}
for $A \in \cplx^{n \times m}$ and $b \in \cplx^{n}$. Therefore, if $W \in \cplx^{n\times m}$
is drawn from a fully factorized approximation
\begin{equation}  \label{eq:c-gauss-vi-general}
  W \sim q(W)
    = \prod_{ij} \mathcal{N}^{\cplx} \bigl(
      w_{ij} \vert\, \mu_{ij}, \Sigma_{ij}, C_{ij}
    \bigr)
  \,,
\end{equation}
with $
  \mu, C \in \cplx^{n\times m}
$, $\Sigma \in \real^{n\times m}$, and $
  \lvert C_{ij} \rvert^2 \leq \Sigma_{ij}
$, then for $x \in \cplx^m$ and $b \in \cplx^n$ the vector $y = b + W x$ has independent
components with
\begin{equation}  \label{eq:cplx-gauss-trick}
  y_i
    \sim \mathcal{N}^{\cplx}
      \Bigl(
        % e_i^\top \mu x + b_i,
        b_i + \sum_{j=1}^m \mu_{ij} x_j,
        \, \sum_{j=1}^m \Sigma_{ij} \lvert x_j \rvert^2,
        \, \sum_{j=1}^m C_{ij} x_j^2
      \Bigr)
    \,.
\end{equation}
The trick requires three operations, i.e. $b + \mu x$, $\Sigma \lvert x \rvert^2$ and
$C x^2$ with the complex modulus and square applied elementwise, and can be applied to
any layer, the output of which depends linearly on its parameters, such as convolutional,
affine, and bilinear transformations ($
  (x, z) \mapsto x^\top W^{(j)} z + b_j
$). Similar to $\real$ case, Variational Dropout for $\cplx$ convolutions draws independent
realizations of $W$ for each spatial patch in the input \citep{molchanov_variational_2017}.
This provides faster computations and better statistical efficiency of the SGVB gradient
estimator by eliminating correlation from overlapping patches \citep{kingma_variational_2015}
and allowing \eqref{eq:cplx-gauss-trick} to leverage $\cplx$ convolutions of the relation
and variance kernels with elementwise complex squares $x^2$ and amplitudes $\lvert x \rvert^2$.
% Weight sharing is essentially pushed to shared variational parameters $\theta$ in the approximation $q_\theta$.
% A convolution is a matrix product of $W$, unrolled into a toeplitz matrix, and im-to-col
% flattened intput $x$

For $\cplx$-Bayesian Dropout we propose to use fully factorized proper $\cplx$-Gaussian
approximation \eqref{eq:c-gauss-vi-general} for weights and point estimates of biases in
dense linear, convolutional and other, effectively, affine layers, i.e. $
  \Sigma_{ij} = \alpha_{ij} \lvert \mu_{ij} \rvert^2
$ and $C_{ij} = 0$. This forces the relation coefficients in \eqref{eq:cplx-gauss-trick}
to be zero.

% subsection c_gauss_and_local_rep (end)

\subsection{The priors} % (fold)
\label{sub:the_priors}

For a fully factorized approximation $q(\omega)$ and factorized prior belief $
  \pi(\omega) = \prod_{ij} \pi(\omega_{ij})
$, the divergence term in ELBO \eqref{eq:elbo} is
\begin{equation}  \label{eq:elbo-general-kl-div}
  KL(q \| \pi)
    % = \mathbb{E}_{\oemga \sim q(\oemga)} \log \tfrac{q(\oemga)}{\pi(\oemga)}
    % = \sum_{ij} \mathbb{E}_{\omega_{ij} \sim q(\omega_{ij})}
    %   \log \tfrac{q(\omega_{ij})}{\pi(\omega_{ij})}
    % = \sum_{ij} KL\bigl( q(\omega_{ij}) \| \pi(\omega_{ij}) \bigr)
    = - \sum_{ij}
        \mathbb{H}(q(\omega_{ij}))
        + \mathbb{E}_{q(\omega_{ij})} \log{\pi(\omega_{ij})}
    \,.
\end{equation}
We consider two fully factorized priors: an improper prior for Variational Dropout and
$\cplx$-Gaussian ARD prior. We omit subscripts ${ij}$ for brevity in the next sections.

\subsubsection{VD prior} % (fold)
\label{ssub:vd_prior}

From \eqref{eq:cn-proper-entropy} the KL-divergence for an improper prior $
  \pi(\omega) \propto {\lvert \omega \rvert}^{-\beta}
$ with $\beta \geq 1$ is
\begin{equation}  \label{eq:c-vd-kl-div-raw}
  KL(q\| \pi)
    \propto
    %   \mathbb{E}_{\omega \sim q(\omega)} \log q(\omega)
    %   + \tfrac{\beta}2 \mathbb{E}_{\omega \sim q(\omega)} \log \lvert \omega \rvert^2
    % =
    %   - \log \lvert \det{(\pi e \sigma^2)} \rvert
    %   + \tfrac{\beta}2 \mathbb{E}_{\omega \sim q(\omega)} \log \lvert \omega \rvert^2
    % =
      - \log{\sigma^2}
      + \tfrac{\beta}2 \Bigl(
        \mathbb{E}_{\omega \sim q(\omega)} \log \lvert \omega \rvert^2
      \Bigr)
    \,.
\end{equation}
Property \eqref{eq:cn-affine} implies that $
  \mathcal{N}^{\cplx}(\mu, \sigma^2, 0)
  \sim \mu \cdot \mathcal{N}^{\cplx}(1, \alpha, 0)
$ for $\mu \neq 0$ and $
  \sigma^2 = \alpha \lvert \mu \rvert^2
$, whence the expectation in brackets is given by
\begin{equation}  \label{eq:expect-improper-term-cplx}
  % \mathbb{E}_{\omega \sim q(\omega)} \log \lvert \omega \rvert^2
  %   % = \mathbb{E}_{\xi \sim \mathcal{N}^{\cplx}(\xi \vert 1, \alpha, 0)}
  %   %   \log \lvert \mu \xi \rvert^2
  %   % = \log \lvert \mu \rvert^2
  %   %   + \mathbb{E}_{\xi \sim \mathcal{N}^{\cplx}(\xi \vert 1, \alpha, 0)}
  %   %       \log \lvert \xi \rvert^2
  %   =
    \log \alpha \lvert \mu \rvert^2
      + \mathbb{E}_{\xi \sim \mathcal{N}^{\cplx}(\xi \vert 0, 1, 0)}
          \log{\bigl\lvert \tfrac1{\sqrt{\alpha}} + \xi \bigr\rvert^2}
    \,.
\end{equation}
If $
  (z_i)_{i=1}^m
    \sim \mathcal{N}^{\cplx}(0, 1, 0)
$ iid and $\theta \in \cplx^m$, then $
  \sum_i \lvert \theta_i + z_i \rvert^2
    \sim \chi^2_{2m}(s^2)
$ with $
  s^2 = \sum_i \lvert \theta_i \rvert^2
$, i.e. a non-central $\chi^2_{2m}$ with parameter $s^2$. Its log-moments for general
integer $m \geq1$ have been derived by \citet[p.~2466]{lapidoth_capacity_2003}.
% in fact Appendix X, Lemma 10.1
In particular, for $m=1$ and $\theta\in \cplx$ we have
\begin{equation}  \label{eq:log-moment-for-chi-2}
  \mathbb{E}_{z \sim \mathcal{N}^{\cplx}(0, 1, 0)}
    \log \lvert \theta + z \rvert^2
    = \log \lvert \theta \rvert^2 - \mathop{Ei}( - \lvert \theta \rvert^2)
    \,,
\end{equation}
where $
  \mathop{Ei}(x) = \int^x_{-\infty} t^{-1} e^t dt
$ for $x < 0$ is the Exponential Integral, which satisfies $
  \mathop{Ei}(x) \leq \log{(-x)}
$, $
  % as x to 0 from below
  \mathop{Ei}(x) \approx \log{(-x)} - \gamma
$ as $x\to 0$ ($\gamma$ is Euler's constant) and $
  \mathop{Ei}(x) \geq -e^x
$ for $x \leq -1$.
% 1. by Leibniz rule $Ei(x)$ has -ve derivative on $x < 0$
% 2. $Ei(x) \leq 0$ on $x < 0$, since $t^{-1} e^t \leq t^{-1} < 0$ on $t < 0$
% 3. $Ei(x) \leq \log{(-x)}$ on $x < 0$, since on $x \in [-1, 0)$
% $$
%   \mathop{Ei}(x)
%     = \mathop{Ei}(-1) + \int_{-1}^x t^{-1} e^t dt
%     \leq \mathop{Ei}(-1) + \int_{-1}^x t^{-1} dt
%     = \mathop{Ei}(-1) + \log{(-x)}
%     \leq \log{(-x)}
%   \,. $$
% 3. by l'H{\^o}pital rule $Ei(x)$ is asymptotically $\log{-x}$ as $x \to 0-$
% $$
% % t = -e^{-x} ,  -\log{(-t)} = x
%   \lim_{x\to 0-} \frac{\mathop{Ei}(x)}{\log{(-x)}}
%     = \lim_{x\to 0-} \frac{x^{-1} e^x}{x^{-1}}
%     = 1
%   \,. $$
% 4. \citep{lapidoth_capacity_2003} yields $\lim_{x\to 0-} \log{(-x)} - \mathop{Ei}(x) = \gamma$
% 5. $Ei(x) \geq -e^x$ for any $x < -1$, since $t^{-1} e^t \geq - e^t$ for $t < -1$
% $$
% \mathop{Ei}(x)
%   = \int_{-\infty}^x t^{-1} e^t dt
%   \geq \int_{-\infty}^x -e^t dt = -e^x
%   \,. $$
% 6. $Ei(x) \leq \log{(-x)} - \gamma$ on $x < 0$ and $-e^x \leq Ei(x) \leq 0$ on $x < -1$.
%
Although $\mathop{Ei}$ is an intractable integral, requiring numerical approximations to
compute, its derivative is exact: $
  \tfrac{d}{dx} \mathop{Ei}(x) = \tfrac{e^x}{x}
$ at $x < 0$.

From \eqref{eq:expect-improper-term-cplx} and \eqref{eq:log-moment-for-chi-2}, the terms of
the divergence that depend on the parameters are given by
\begin{equation}  \label{eq:c-vd-kl-div}
  KL(q\| \pi)
    \propto
    %   - \log \lvert \det{(\pi e \sigma^2)} \rvert
    %   + \tfrac{\beta}2 \log \alpha \lvert \mu \rvert^2
    %   + \tfrac{\beta}2 \mathbb{E}_{\xi \sim \mathcal{N}^{\cplx}(\xi \vert 0, 1, 0)}
    %       \log{\bigl\lvert \tfrac1{\sqrt{\alpha}} + \xi \bigr\rvert^2}
    % =
    %   - \log{\sigma^2}  % - \log{\pi e}
    %   + \tfrac{\beta}2 \log \alpha \lvert \mu \rvert^2
    %   + \tfrac{\beta}2 \bigl(
    %     - \log \alpha - \mathop{Ei}( -\tfrac1{\alpha})
    %   \bigr)
    % =
    %   - \log{\sigma^2}
    %   + \tfrac{\beta}2 \log \alpha \lvert \mu \rvert^2
    %   - \tfrac{\beta}2 \log \alpha
    %   - \tfrac{\beta}2 \mathop{Ei}( -\tfrac1{\alpha})
    % =
      % \log \tfrac{\lvert \mu \rvert^\beta}{\sigma^2}
      % \log \tfrac{\lvert \mu \rvert^\beta}{\alpha \lvert \mu \rvert^2}
      \tfrac{\beta-2}2 \log{\lvert \mu \rvert^2}
      + \log{\tfrac1{\alpha}}
      - \tfrac{\beta}2 \mathop{Ei}(- \tfrac1{\alpha})
      \,.
      \tag{\ref{eq:c-vd-kl-div-raw}'}
\end{equation}
We set $\beta = 2$ to make the divergence term depend only on $\alpha$ and add $\gamma$
so that the right-hand side is nonnegative \citep[eq.(84)]{lapidoth_capacity_2003}.
%
Since $\mathop{Ei}(x)$ has simple analytic derivative and \eqref{eq:elbo} depends additively
on \eqref{eq:c-vd-kl-div}, it is possible to back-propagate through the divergence without
forward evaluation, which speeds up gradient updates.
% Gradient-based optimiziation of \eqref{eq:elbo} usually does not use the value of the objective. 
% In stochastic gradient methods are concerned more with the gradient, induced by the objective
% function, rather than its value.

% subsubsection variational_dropout (end)

\subsubsection{ARD prior} % (fold)
\label{ssub:ard_prior}

We consider the fully factorized proper $\cplx$-Gaussian ARD prior with $
  \pi_\tau(\omega)
    \propto \mathcal{N}^{\cplx}\bigl(
      \omega \vert 0, \tau^{-1}, 0
    \bigr)
$. The per element divergence term in \eqref{eq:elbo-general-kl-div} is
\begin{equation}  \label{eq:emp-bayes-kl-div}
  KL(q \| \pi_\tau)
    = - 1 - \log{(\tau \sigma^2)}
      + \tau \bigl(
        \sigma^2 + \lvert \mu \rvert^2
      \bigr)
    \,.
\end{equation}
%
% This follows from the KL-divergence expression for two multivariate Gaussians
% $$
% % \mathbb{E}_{\omega\sim q_1(\omega)} \log \tfrac{q_1(\omega)}{q_2(\omega)}
% %   =
% KL(q_1\| q_2)
%   =
%   % - \tfrac12 \log\det{(2\pi e \Sigma_1)} + \tfrac12 \log\det{(2\pi \Sigma_2)}
%   - \tfrac12 \log \det{e I}
%   + \tfrac12 \log \frac{\det{\Sigma_2}}{\det{\Sigma_1}}
%   + \tfrac12 \tr{\bigl( \Sigma_2^{-1} \Sigma_1 \bigr)}
%   + \tfrac12 (\mu_1 - \mu_2)^\top \Sigma_2^{-1} (\mu_1 - \mu_2)
%   =
%   - \log\lvert\det{\pi e \Gamma}\rvert
%   + \log\lvert\det{\pi \tau^{-1} I} \rvert
%   + \tau \Re \tr{\Gamma} + \tau \mu^\hop \mu
%   =
%   - n - \log \lvert\det{\tau \Gamma}\rvert
%   + \tau \Re \tr{\Gamma} + \tau \mu^\hop \mu
%   \,. $$
%
In Empirical Bayes the prior adapts to the observed data, i.e. \eqref{eq:elbo} is optimized
w.r.t. $\tau$ of each weight's prior. Since divergence sits in \eqref{eq:elbo} with negative
sign, optimal $
  \tau^\ast = (\sigma^2 + \lvert \mu \rvert^2)^{-1}
$ is found by minimizing \eqref{eq:emp-bayes-kl-div}, which gives
\begin{equation}  \label{eq:emp-bayes-opt-kl}
  KL(q \| \pi_{\tau^\ast})
    % = - 1 - \log{({\tau^\ast} \sigma^2)}
    %   + {\tau^\ast} \sigma^2 + {\tau^\ast} \lvert \mu \rvert^2
    % = \log{((\sigma^2 + \lvert \mu \rvert^2) \tfrac1{\sigma^2})}
    = \log{\bigl(1 + \tfrac{\lvert \mu \rvert^2}{\sigma^2}\bigr)}
    = \log{\bigl(1 + \tfrac1\alpha \bigr)}
    \,.
    \tag{\ref{eq:emp-bayes-kl-div}'}
\end{equation}

% subsubsection empirical_bayes (end)

\subsubsection{$\cplx$-Bayesian Dropout via $\real$-scaling} % (fold)
\label{ssub:real_scaling_dropout}

Consider the following parameterization of $W$: $
  W_{ij} = \mu_{ij} \xi_{ij}
$, $\xi_{ij} \in \real$ with $
  \xi_{ij} \sim \mathcal{N}(1, \alpha_{ij})
$, yet $\mu \in \cplx^{n \times m}$. This case corresponds to inference regarding
relevance multipliers $\xi$ rather than the parameters themselves. Under this parameterization
the weight distribution is fully factorized degenerate $\cplx$-Gaussian approximation
\eqref{eq:c-gauss-vi-general} with $
  \Sigma_{ij} = \alpha_{ij} \lvert \mu_{ij} \rvert^2
$ and $
  C_{ij} = \alpha_{ij} \mu_{ij}^2
$. Unlike proper $\cplx$-Gaussian approximation, the complex relation parameter in
\eqref{eq:cplx-gauss-trick} is nonzero in this case and equals $
  \sum_j \alpha_{ij} (x_{ij} \mu_{ij})^2
$.
%
The KL-divergence term for this approximation coincides with \eqref{eq:improper-kl-div-real},
however the major drawback of this approximation is that the additive noise reparameterization,
which disentangles the gradient with respect to $\mu$ from the local output noise, cannot be
applied.

% subsubsection real_scaling_dropout (end)

% subsection priors_and_kullback_leibler_divergence (end)

% section c_variational_dropout (end)


\section{Experiments} % (fold)
\label{sec:experiments}

We study the compression-performance trade-off of the proposed Bayesian sparsification
methods for $\cplx$-valued networks presented above on the datasets, that have been
studied in prior research on complex-valued networks.

% setup and common experiment settings
For each dataset we conduct a set of experiments in which we try out all combinations of
model kinds ($\real$ or $\cplx$), sparsification methods (VD or ARD) and input features.
Where reasonable, we also study the effects of halving ($\cplx$) or doubling ($\real$)
of the size of intermediate layers in each network \citep{monning_evaluation_2018}.
%
Each network within every experiment passes through three successive stages of training:
pre-training, sparsification, and fine-tuning (sec.~\ref{sub:fitting_and_sparsification}).
%
Each experiment is replicated \textit{five} times to take into account random initialization,
random order of the mini-batch during SGD, stochastic outputs of the intermediate layers,
and non-determinism of single-precision computations on GPUs.

Unless specified otherwise, every experiment uses global gradient clipping within an
$\ell_2$-norm ball of radius $0.5$, employs SGD with ADAM optimizer with the base learning
rate of $10^{-3}$. We note that since our experiments are on classification tasks,
$\cplx$-valued networks ultimately have to output a real-valued logit score. Following
\citet{wolter_complex_2018} and \citet{trabelsi_deep_2017}, the class scores are taken
as real parts of the $\cplx$ output.

% write a note on the incompressible parameters (bias, batch norm etc.)
The compression rate is computed based on the number of IEEE754 floating point values
needed to store the model and equals $
  \tfrac{n_\mathtt{par}}{n_\mathtt{par} - n_\mathtt{zer}}
$, where $n_\mathtt{zer}$ is the number of explicit zeros, $n_\mathtt{par}$ is the total
number of floats. In particular, each parameter is counted once in a $\real$-valued network,
but twice in a $\cplx$-network. Each model has a compression limit, determined by its
layers' biases, shift and scaling from $\real$- and $\cplx$-valued batch normalization.

\subsection{Stagewise training} % (fold)
\label{sub:fitting_and_sparsification}

% The training consists of three successive stages: ``dense'' $\to$ ``sparsify'' $\to$ ``fine-tune''.

At the \textit{``dense''} stage every network is fit ``as-is'', retaining its original
non-Bayesian architecture, and using only the likelihood term from \eqref{eq:elbo} without
variational dropout.

During the \textit{``sparsify''} stage we make every layer Bayesian and apply $\cplx$-variational
dropout sec.~\ref{ssub:vd_prior} (or sec.~\ref{ssub:ard_prior}) or its $\real$ counterpart.
We inject a coefficient $
  C \in (0, 1]
$ at the KL divergence term in \eqref{eq:elbo} to study the attainable sparsity levels:
\begin{equation}  \label{eq:elbo_with_coef}
  - \frac{C}N KL(q_{\theta} \| \pi_{\lambda})
  + \frac1{M} \sum_{k=1}^M
      \log p_{\phi}(z_{i_k} \mid g(\varepsilon_k; \theta))
  \,.
  \tag{\ref{eq:elbo}'}
\end{equation}
By varying $C$ we adjust the importance of the sparsifying prior in learning $q_\theta(W)$,
meaning that higher value yields more irrelevant parameters during in inference and makes
layers sparser upon termination of this stage.

% on compresssion
Between \textit{``sparsify''} and \textit{``fine-tune''} stages we compute masks of non-zero
weights in each layer based on the relevance scores $\alpha$ (sec.~\ref{sec:variational_dropout}).
Since the $q_\theta$ factorizes into univariate distributions, a $\cplx$ or $\real$ parameter
is considered non-zero if $\log \alpha \leq \tau$ for $
  \alpha = \tfrac{\sigma^2}{\lvert\mu\rvert^2}
$, or zero otherwise.

The \textit{``fine-tune''} stage reverts the network back to non-Bayesian layers, applies
sparsity masks to its parameters and proceeds with training the kept parameters as during
the ``dense'' stage. Unless specified otherwise the network is initialized to the mode
of the learnt $q_\theta(W)$.

% on chosing the threshold
The threshold $\tau$ is picked so, that the remaining non-zero parameters are within
$\delta$ relative tolerance of their mode with high probability under the approximate
posterior. Since $q_\theta$ is fully factorized $\real$- or a proper $\cplx$-Gaussian,
the random variable $
  \tfrac{k \lvert w - \mu \rvert^2}
        {\alpha \lvert \mu \rvert^2}
$ is $\chi^2_k$ distributed with $k=1$ ($\real$) or $2$ ($\cplx$).
%
For a generous tolerance $\delta = 50\%$ all values of $\log \alpha$ below $-2.5$ yield
at least $90\%$ chance of a non-zero $\real$/$\cplx$ parameter. We pick $\tau = -\tfrac12$
to retain parameters sufficiently concentrated around their mode, and encourage higher
sparsity, at the same time being aware that $q_\theta$ is merely an approximation.
In comparison, $\tau = 3$ is commonly used as the threshold
\citep{molchanov_variational_2017,kingma_variational_2015}.

% subsection fitting_and_sparsification (end)


\subsection{Metrics and compression} % (fold)
\label{sub:metrics_and_compression}

% conlcusions
The impact of the ``fine-tune'' stage appears to be strongly influenced by the compression
rate. We can observe in this and, more prominently, in MusicNet experiment (sec.~\ref{sub:musicnet})
that higher compression rate render the model more regularized, leaving less capacity for
overfitting.
% more ``holes'' Bayesian dropout ``poked'' makes the ``fine-tune'' serve as extended training.

% effects of extra training epochs and the impact of tau
Figure~\ref{fig:hist__and__threshold__tradeoff} is constructed
for $C \in \{\tfrac1{20}, \frac1{200}\}$ before running the ``fine-tune'' stage. From
\eqref{eq:elbo_with_coef}, the relative positions of the compression curves on this
plot, and the results of experiments below it can be concluded that $C$ has much more
substantial impact on the sparsity and performance, than the choice of $\tau$. Furthermore,
for $\tau > 0$ the performance quickly saturates while compression creeps downwards.

\begin{figure}[!t]
  \centering
  \begin{subfigure}[b]{1.\columnwidth}  % imcl2019-style swears at this
    \centering
    \includegraphics[width=\columnwidth]{figure__musicnet__threshold.pdf}
  \end{subfigure}
  \caption{%
    Test split performance and compression rate for a $\cplx$-valued net on MusicNet.
  }
  \label{fig:hist__and__threshold__tradeoff}
\end{figure}

% perfromance metrics
We use accuracy as the performance metric for multi-class image classification on MNIST-like
and CIFAR10 datasets. Similarly to \citet{trabelsi_deep_2017} performance in experiments with
MusicNet is measured with (pooled) average precision. The evaluation is done after ``dense''
stage, before the beginning and upon termination of the ``fine-tune'' stage.

% subsection metrics_and_compression (end)


\subsection{MNIST-like datasets} % (fold)
\label{sub:mnist_like_datasets}

We conduct a moderately sized experiment on MNIST and similar $28\times 28$ greyscale image
datasets to study the compression-performance trade-off of the proposed $\cplx$-variational
dropout.
% dataset size
We deliberately use a {fixed} random subset of $10k$ images from the train split of each
dataset to fit the networks and gauge the final performance on the usual test split.
Limiting the size was done primarily to test the regularization strength of compression.

% model architectures
% discussion of TwoLayerDenseModel (TLDM) and SimpleConvModel (SCM)
The $\real$/$\cplx$ networks have been chosen for the purpose of illustrating the compression
and understanding the effects of experiment parameters. \textit{SimpleConvModel} (SCM) is a $2d$
ReLU convolutional network with average pooling ($2\times 2$) after non-linearities with
two convolutions (kernel $5\times 5$, stride $1$) with filters $n_1 \to n_2$ followed by
dense layers $n_3 \to n_\mathtt{out}$. The second network, \textit{TwoLayerDenseModel} (TLDM),
is a wide dense network $n_1 \to n_\mathtt{out}$. For SCM model $n_1, n_2, n_3$ are $20, 50, 500$,
respectively, and for TLDM model $n_1 = 4096$ regardless of their type ($\real$ or $\cplx$),
although \citet{monning_evaluation_2018} argue that this renders models incomparable.

% image features
Since image data is not naturally $\cplx$-valued, we preprocess images by either embedding
$\real$-data into $\cplx$ assuming $\Re z = 0$ (\texttt{raw}), or applying the two-dimensional
Fourier Transform (\texttt{fft}), shifting lower frequencies to the centre of the image. We
do not investigate the option, proposed by \citet{trabelsi_deep_2017}, to train an auxiliary
model to synthesize the imaginary component from the real data.

% first experiment: 40-75-40 with fast scheduler
In this experiment stages last for $40$, $75$ and $40$ epochs, respectively, and the base
learning rate of ${10}^{-3}$ is reduced after the $10$-th epoch to ${10}^{-4}$.
%
Plots~\ref{fig:mnist-like__trade-off} depict samples from the performance-compression curve
on KMNIST and EMNIST-Letters datasets for the studied models and VD compression method
(sec.~\ref{ssub:vd_prior}). Besides that we try \texttt{raw} or \texttt{fft}
input features, vary $
  C \in \{\tfrac3{2^{k+1}}\colon k=1..16\}
$, and repeat the experiment $5$ times.

Shaded horizontal bands reflect the spread of the performance of the uncompressed network
after ``dense'' stage. Each point represents a precision-compression value attained by the
network after ``fine-tune'' stage. The end of each point's tail represents the performance
gain or loss from running the ``fine-tune'' stage.
%
We make a couple of observations based on the results of these plots. First, \texttt{fft}
features perform on par with raw colour channel data in the convolutional networks across most
compression rates, but require high compression rate to make wide shallow dense network work
as well. The upward trend manifests itself in MNIST and Fashion MNIST experiments as well
(see appendix~\ref{sec:mnist_like_experiments}). Finally, overall $\cplx$ variational dropout
methods seem to compress networks adequately well without much loss in performance.

% a couple of plots from MNIST and Fashion-MNIST
\begin{figure}[!t]
  \centering
  \begin{subfigure}[b]{1.\columnwidth}  % imcl2019-style swears at this
    \centering
    % \includegraphics[width=\columnwidth]{figure__mnist-like__trade-off/legacy__VD__fashionmnist__fft__-0.5.pdf}
    \includegraphics[width=0.8\columnwidth]{figure__mnist-like__trade-off/legacy__VD__kmnist__fft__-0.5.pdf}
    % \includegraphics[width=\columnwidth]{figure__mnist-like__trade-off/VD__kmnist__fft__-0.5.pdf}
  \end{subfigure} \\%
  % \hfill
  \begin{subfigure}[b]{1.\columnwidth}  % imcl2019-style swears at this
    \centering
    % \includegraphics[width=\columnwidth]{figure__mnist-like__trade-off/legacy__VD__fashionmnist__raw__-0.5.pdf}
    \includegraphics[width=0.8\columnwidth]{figure__mnist-like__trade-off/legacy__VD__kmnist__raw__-0.5.pdf}
    % \includegraphics[width=\columnwidth]{figure__mnist-like__trade-off/VD__kmnist__raw__-0.5.pdf}
  \end{subfigure} \\%
  \begin{subfigure}[b]{1.\columnwidth}  % imcl2019-style swears at this
    \centering
    % \includegraphics[width=\columnwidth]{figure__mnist-like__trade-off/legacy__VD__mnist__fft__-0.5.pdf}
    \includegraphics[width=0.8\columnwidth]{figure__mnist-like__trade-off/legacy__VD__emnist_letters__fft__-0.5.pdf}
    % \includegraphics[width=\columnwidth]{figure__mnist-like__trade-off/ARD__kmnist__fft__-0.5.pdf}
  \end{subfigure} \\%
  % \hfill
  \begin{subfigure}[b]{1.\columnwidth}  % imcl2019-style swears at this
    \centering
    % \includegraphics[width=\columnwidth]{figure__mnist-like__trade-off/legacy__VD__mnist__raw__-0.5.pdf}
    \includegraphics[width=0.8\columnwidth]{figure__mnist-like__trade-off/legacy__VD__emnist_letters__raw__-0.5.pdf}
    % \includegraphics[width=\columnwidth]{figure__mnist-like__trade-off/VD__emnist_letters__raw__-0.5.pdf}
  \end{subfigure}
  \caption{%
    The compression-accuracy trade-off on KMNIST and the letters of the Extended MNIST datasets.
    (\textit{top}) \texttt{fft} features, (\textit{bottom}) \texttt{raw} features.
  }
  \label{fig:mnist-like__trade-off}
\end{figure}

% subsection mnist_like_datasets (end)


\subsection{CIFAR10} % (fold)
\label{sub:cifar10}

% mention the same experimental setup as in MNIST
Experiments on CIFAR10 dataset are set up similarly to MNIST-like experiment, except
that we study VGG16 model \citep{simonyan_very_2015} and its $\cplx$-counterpart and
replicate each experiment only twice.
%
% describe the VGG16 model and its cplx-valued modification
The $\cplx$-VGG16 model is architecturally identical to VGG16, except for all layers being
replaced by their $\cplx$-valued versions. We halve the sizes of intermediate layers in
$\cplx$ model to assess comparability \citep{monning_evaluation_2018}.
%
Each network is trained stagewise for $20$, $40$,
and $20$ epochs on the full training split of $50k$ colour $32\times32$ images. We use
only raw colour features (\texttt{raw}). We apply random crop and horizontal flips as
data augmentation and use a learning rate scheduler in SGD, which scales the base learning
rate after $5$, $10$, and $20$-th epoch by $\tfrac1{10}$, $\tfrac1{20}$ and $\tfrac1{100}$,
respectively.

% draw some conclusions
Figure~\ref{fig:figure__cifar10__trade-off} shows that although the prior used in ARD method
offers slightly less compression, it makes up for it by equally slightly better accuracy.
Furthermore, the results imply that that the halved $\cplx$ network visibly underperforms
in comparison with its $\real$ counterpart, although in MNIST-like experiment $\cplx$-valued
network with the same inner feature dimensions demonstrates trade-off very similar to
$\real$-network, which has half the number of real parameters than the former.

% compare compressed cplx-vgg against real-vgg
\begin{figure}[!t]
  \centering
  \begin{subfigure}[b]{1.\columnwidth}  % imcl2019-style swears at this
    \centering
    \includegraphics[width=\columnwidth]{figure__cifar__trade-off/legacy__augmentedcifar10__raw__-0.5.pdf}
  \end{subfigure}
  \caption{%
    Compression-accuracy frontier for $\real$ and $\tfrac12 \cplx$ versions of VGG16 on CIFAR10.
  }
  \label{fig:figure__cifar10__trade-off}
\end{figure}

% subsection cifar10_100 (end)


\subsection{musicNet} % (fold)
\label{sub:musicnet}

% describe musicnet as in Trabelsi et al. (2017) also get some hints from thickstun_learning_2017
MusicNet \citep{thickstun_learning_2017} is an annotated audio dataset, consisting of $330$
musical compositions. Following \citet{trabelsi_deep_2017}, we use it in automatic music
transcription task and study the $1d$ VGG-like $\cplx$-convent proposed therein. Similarly to
the study, we downsample the audio from 44.1kHz to 11kHz, retain only $84$ out of $128$ labels,
and hold-out the same validation and test splits. During each training epoch we sample $1000$
mini-batches of the remaining $321$ compositions, with each mini-batch comprised of a random
full window of $4096$ samples picked from each waveform and transformed from $\real$ to
$\cplx$ with $1d$ FFT. Annotation labels are taken at the mid-point of the window.

Preliminary experiments with the model without sparsification have shown that early stopping
almost always terminates the training process within the first $10-20$ epochs with the validation
performance peaking at $10-15$ epochs, much sooner than $200$ epochs used by \citet{trabelsi_deep_2017}.
Thus we adopt shorter lasting stages: $12$, $32$ and $50$ epochs, which essentially turn the
``dense'' stage into pretraining. Early stopping is used only during the ``fine-tune'' stage.
To keep the learning rate schedule consistent, we allow the base rate of $10^{-3}$ to be scaled
after $5$, $10$, and $20$-th epoch by $\tfrac1{10}$, $\tfrac1{20}$ and $\tfrac1{100}$, respectively.
Other key deviations from the set-up used by \citet{trabelsi_deep_2017} include: shifting lower
frequencies to the centre of the window to maintain locality after applying the FFT
(sec.~\ref{sub:mnist_like_datasets}), and clipping $\ell_2$ norm of the gradients to $0.05$.

\begin{figure}[!t]
  \centering
  \begin{subfigure}[b]{1.\columnwidth}  % imcl2019-style swears at this
    \centering
    \includegraphics[width=\columnwidth]{figure__musicnet__trade-off/paper__musicnetram__fft__-0.5.pdf}
  \end{subfigure}
  \caption{%
    Performance-compression trade-off using \textbf{\color{blue} VD} and \textbf{\color{orange} ARD}.
    The \textbf{\color{pink} version} of the network with kernel $3$ in the first convolution is
    compressed with VD only.
  }
  \label{fig:musicnet__trade-off}
\end{figure}

In each experiment we vary the method and the value $C$, while keeping $\tau$ at $-\tfrac12$.

Figure~\ref{fig:musicnet__trade-off} shows the resulting performance-compression frontier
for the deep network of \citet{trabelsi_deep_2017} and its version, in which we purposefully
halve the spatial size of the first $1d$ convolution kernel from $6$ to $3$ (denoted by
suffix \textit{k}$3$). The motivation is to test if the handicap introduced by the forced
compression of the most upstream layer can be alleviated through non-uniform compression,
induced by variational dropout. We test only VD (sec.~\ref{ssub:vd_prior}) this
sub-experiment, since prior results have not demonstrated significant superiority of one method
over another (sec.~\ref{ssub:ard_prior}).

For compression levels lower than $50$, the final stage overfits the network, since it still
remains overparameterized. For the rates higher than $\approx 100$ the ``fine-tune'' stage
serves its purpose, essentially, due regularizing effects of high sparsity. There is evidence
in favour of achievability of the SOTA average precision of \citet{trabelsi_deep_2017} within
the $50$-$100$ compression region with $\cplx$-valued variational dropout with ARD prior
(sec.~\ref{ssub:ard_prior}).

% subsection musicnet (end)

% section experiments (end)


\section{Conclusion} % (fold)
\label{sec:conclusion}

Despite having higher arithmetic complexity, complex-valued models provide built-in
amplitude-phase transformations and regularization \citep{hirose_complex-valued_2009}
and enjoy superior performance on naturally $\cplx$-valued datasets \citep{tarver_design_2019}.
In this study we address the issue of excessive arithmetic complexity of complex-valued
networks by extending the existing Bayesian compression techniques by \citet{kingma_variational_2015},
\citet{molchanov_variational_2017} and \citet{kharitonov_variational_2018} to $\cplx$-valued
parameters. To this end we have conducted a large numerical study of such networks to assess
the performance-compression rate trade-off and have achieved performance comparable to
the state-of-the-art in $\cplx$-valued networks on the MusicNet dataset.

Much like $\real$-valued variational dropout, the proposed $\cplx$-valued extension can be
straightforwardly enhanced to provide structured sparsity. The results of this study have
direct implications for real-time signal processing with embedded deep learning applications
both in terms of lower storage requirements and higher throughput stemming from fewer
floating-point multiplications.

% section conclusion (end)

% \clearpage

\bibliographystyle{abbrvnat}
\bibliography{references}
% \nocite{*}

% \clearpage

\appendix
\onecolumn

\section{Complex-valued local reparameterization} % (fold)
\label{sec:complex_valued_local_reparameterization}

%notation
By $e_i$ we denote the $i$-th real unit vector of dimensionality \textit{conforming} to the
matrix-vector expression it is used in. Let $\vec{M}$ denote \textit{row-major} flattening
of $M$ into a vector, i.e. in lexicographic order of its indices. $\diag{(\cdot)}$ embeds
$\cplx^n$ into $n\times n$ diagonal matrices, and $\otimes$ denotes the Kronecker product,
for which we note the following identities $
  \vec{P Q R} = (P \otimes R^\top) \vec{Q}
$, and $
  (P \otimes R) (C \otimes S) = P Q \otimes R S
$ \citep{petersen_matrix_2012}.
% It follows from the definition of the Kronecker product
% of $P$ and $R^\top$ and the row-major order vectorization.

If we assume a mean field $\cplx$-Gaussian approximate posterior for $
  W \in \cplx^{n\times m}
$, then
\begin{equation}  \label{eq:c-gauss-vi-general-vec}
  \vec{W}
    \sim \mathcal{N}^{\cplx}_{nm} \bigl(
      \vec{\mu}, \diag{\vec{\Sigma}}, \diag{\vec{C}}
    \bigr)
  \,,
\end{equation}
where mean, \textit{variance} and relation are given by matrices $
  \mu, \Sigma, C \in \cplx^{n\times m}
$ with $\Sigma_{ij} \geq 0$, and $
  \lvert C_{ij} \rvert^2 \leq \Sigma_{ij}
$. For any $x \in \cplx^m$ and $b\in \cplx^n$ $
  y = W x + b
    = (I_n \otimes x^\top) \vec{W} + b
$, whence the covariance and relation matrices of $y$ are
\begin{align}
  (I \otimes x^\top) \diag{\vec{\Sigma}} (I \otimes x^\top)^{\hop}
    % = \sum_{ij} (I \otimes x^\top) e_i \otimes e_j
    %   \Sigma_{ij} (e_i \otimes e_j)^\top (I \otimes x^\top)^{\hop}
    % = \sum_{ij} (e_i \otimes x^\top e_j)
    %   \Sigma_{ij} (e_i \otimes x^\top e_j)^{\hop}
    % = \sum_{ij} (e_i \otimes x_j) \Sigma_{ij} (e_i^\top \otimes \conj{x}_j)
    & = \sum_{i=1}^n (e_i e_i^\top) \sum_{j=1}^m \Sigma_{ij} x_j \conj{x}_j
    \,,  \label{eq:cn-gauss-lrt-cov} \\
  (I \otimes x^\top) \diag{\vec{C}} (I \otimes x^\top)^\top
    % = \sum_{ij} (e_i \otimes x_j) C_{ij} (e_i \otimes x_j)^\top
    & = \sum_{i=1}^n (e_i e_i^\top) \sum_{j=1}^m C_{ij} x_j^2
    \,.  \label{eq:cn-gauss-lrt-rel}
\end{align}
Since \eqref{eq:cn-gauss-lrt-cov} and \eqref{eq:cn-gauss-lrt-rel} are diagonal, $(y_i)_{i=1}^n$
are independent whence \eqref{eq:cn-affine} implies
\begin{equation}  \label{eq:cplx-gauss-trick-appendix}
  y_i
    \sim \mathcal{N}^{\cplx}
      \bigl(
        e_i^\top \mu x + b_i,
        \, \sum_{j=1}^m \Sigma_{ij} \lvert x_j \rvert^2,
        \, \sum_{j=1}^m C_{ij} x_j^2
      \bigr)
    \,.
\end{equation}
Hence \eqref{eq:cplx-gauss-trick} follows.

% section complex_valued_local_reparameterization (end)

\section{Gradient of the KL-divergence in $\real$ case} % (fold)
\label{sec:real-chisq-grad}  % gradient_of_the_kl_divergence_in_R_case

In this section we derive the exact gradient of \eqref{eq:improper-kl-div-real}. Even
though, ultimately, the divergence involves a hypergeometric function, and its gradient
requires evaluating the Gauss error function, the analysis provides independent verification
of the approximation, proposed by \citet{molchanov_variational_2017}. The logic of the
derivation follows \citet{lapidoth_capacity_2003}.
% \citet{pav_moments_2015} correctly notice that \cite[p. 2466]{lapidoth_capacity_2003} has a missing $\log{2}$ term.

For $(z_i)_{i=1}^m \sim \mathcal{N}(0, 1)$ iid and $
  (\mu_i)_{i=1}^m \in \mathbb{R}
$, the random variable $W = \sum_i (\mu_i + z_i)^2$ is noncentral $\chi^2$ with shape $m$
and noncentrality parameter $\lambda = \sum_i \mu_i^2$, i.e. $W\sim \chi^2_m(\lambda)$.
Its density is
\begin{equation}  \label{eq:nonchisq-density}
  f_W(x)
    % = e^{- \tfrac\lambda2} \sum_{n \geq 0} \tfrac{\bigl(\tfrac\lambda2\bigr)^n}{n!}
    %   \tfrac{
    %     x^{n + \tfrac{m}2 - 1} e^{-\tfrac{x}2}
    %   }{
    %     2^{n + \tfrac{m}2} \Gamma(n + \tfrac{m}2)
    %   }
    % = e^{- \tfrac\lambda2} \sum_{n \geq 0} \tfrac{\bigl(\tfrac\lambda2\bigr)^n}{n!}
    %     \tfrac{
    %         \bigl(\tfrac{x}2\bigr)^{n + \tfrac{m}2 - 1} e^{-\tfrac{x}2}
    %     }{
    %         2 \Gamma(n + \tfrac{m}2)
    %     }
    = \frac12 e^{- \tfrac{x + \lambda}2} \bigl(\tfrac{x}2\bigr)^{\tfrac{m}2 - 1}
      \sum_{n \geq 0} \frac{
        \bigl(\tfrac{x \lambda}4\bigr)^n
      }{
        n! \Gamma(n + \tfrac{m}2)
      }
    % = \frac12 e^{- \tfrac{x + \lambda}2}
    % \bigl(\tfrac{x}\lambda\bigr)^{\tfrac{m}4 - \tfrac12}
    % \bigl(\tfrac{x \lambda}4\bigr)^{\tfrac{m}4 - \tfrac12}
    % \sum_{n \geq 0} \tfrac{
    %         \bigl(\tfrac{x \lambda}4\bigr)^n
    %     }{
    %         n! \Gamma(n + \tfrac{m}2)
    %     }
    % = \frac12 e^{- \tfrac{x + \lambda}2}
    % \bigl(\tfrac{x}\lambda\bigr)^{\tfrac{m}4 - \tfrac12}
    % \bigl(\tfrac{\sqrt{x \lambda}}2\bigr)^{\tfrac{m}2 - 1}
    % \sum_{n \geq 0} \tfrac{
    %         \bigl(\tfrac{x \lambda}4\bigr)^n
    %     }{
    %         n! \Gamma(n + \tfrac{m}2)
    %     }
    % = \frac12 e^{- \tfrac{x + \lambda}2}
    % \bigl(\tfrac{x}\lambda\bigr)^{\tfrac{m - 2}4}
    % I_{\bigl(\tfrac{m}2 - 1\bigr)}(\sqrt{x \lambda})
    \,.
\end{equation}
% where $I_k$ is the modified Bessel function of the first kind
% $$
% I_k(s)
%   = \Bigl(\frac{s}2\Bigr)^k
%       \sum_{n \geq 0} \tfrac{
%           \bigl( \tfrac{s}2 \bigr)^{2n}
%       }{n! \Gamma(n + k + 1)}
%   \,. $$
By integrating the power series \eqref{eq:nonchisq-density} with substitution $x \to 2u$,
the expectation $
  \mathbb{E}_{W\sim \chi^2_m(\lambda)} \log W
$, needed in \eqref{eq:improper-kl-div-real}, equals
\begin{multline}  \label{eq:exp-log-chisq-proof}
    % &= \int_0^\infty f_W(x) \log x dx
    % \notag \\
    % &\frac12 e^{- \tfrac\lambda2}
    % \int_0^\infty \sum_{n \geq 0} \biggl(
    %     \frac{
    %       e^{- \tfrac{x}2} \bigl(\tfrac\lambda2\bigr)^n
    %     }{
    %       n! \Gamma(n + \tfrac{m}2)
    %     }
    %     \bigl(\tfrac{x}2\bigr)^{n + \tfrac{m}2 - 1}
    % \biggr) \log{(x)} dx
    % = [\text{ absolute summability and Fubini, or any other conv. thm for integrals of nonnegative integrals}]
    % = e^{- \tfrac\lambda2} \sum_{n \geq 0}
    %     \tfrac{\bigl(\tfrac\lambda2\bigr)^n}{n!}
    %     \tfrac1{\Gamma(n + \tfrac{m}2)}
    % \int_0^\infty
    %     e^{- \tfrac{x}2} \bigl(\tfrac{x}2\bigr)^{n + \tfrac{m}2 - 1}
    %     (\log2 + \log \tfrac{x}2) \tfrac{dx}2
    % = [u = \tfrac{x}2]
    % \notag \\
    e^{- \tfrac\lambda2} \sum_{n \geq 0}
        \frac{\bigl(\tfrac\lambda2\bigr)^n}{
          n! \Gamma(n + \tfrac{m}2)
        }
    \int_0^\infty
        e^{- u} u^{n + \tfrac{m}2 - 1}
        \log{(2 u)} du
    % = [\text{definitions:} \Gamma, \psi]
    % = e^{- \tfrac\lambda2} \sum_{n \geq 0}
    %     \tfrac{\bigl(\tfrac\lambda2\bigr)^n}{n!}
    % (\psi(n + \tfrac{m}2) + \log2)
    % \notag
    \\
    = \log{2}
    + e^{- \tfrac\lambda2} \sum_{n \geq 0}
        \frac{\bigl(\tfrac\lambda2\bigr)^n}{n!}
        \psi(n + \tfrac{m}2)
    \,,
\end{multline}
where $\psi(x)$ is the digamma function, i.e.
\begin{equation}  \label{eq:digamma}
  \psi(x)
    % = \frac{d}{dx} \log \Gamma(x)
    = \frac1{\Gamma(x)}
      \int_0^\infty
        u^{x-1} e^{-u} \log{u}
      \, du
    \,,
\end{equation}
which satisfies $
  \psi(z+1) = \psi(z) + \tfrac1z
$, $
  \psi(x)\leq \log x - \tfrac1{2x}
$, and $
  \psi(\tfrac12) = -\gamma - 2\log 2
$ ($\gamma$ is Euler's constant). If we put
\begin{equation}  \label{eq:g-m-series}
  g_m(x)
    % alpha: e^{-x} \sum_{n=0}^{\infty} \frac{x^n}{n!} \psi(n + m / 2)
    % = e^{-x} \sum_{n \geq 0} \frac{x^n}{n! \Gamma(n + \tfrac{m}2)}
    %     \int_0^\infty e^{-t} t^{n + \tfrac{m}2-1} \log{t} dt
    = e^{-x} \sum_{n \geq 0} \frac{x^n}{n!} \psi(n + \tfrac{m}2)
    \,,
\end{equation}
then the desired expectation in \eqref{eq:improper-kl-div-real} equals $
  \log{2} + g_1(\tfrac1{2\alpha})
$.
%
% We can differentiate the series in $g_m(x)$, since the sum converges on $\mathbb{R}$.
% Indeed, it is a power series featuring nonnegative coefficients, which is dominated by $
%   \sum_{n \geq 1} \tfrac{x^n}{n!} \log{(n+\tfrac{m}2)}
% $. By the ratio test, the dominating
% series has infinite radius:
% $$
% \lim_{n\to\infty}
%   \biggl\lvert
%     \frac{
%       n! x^{n+1} \log{(n + 1 + \tfrac{m}2)}
%     }{
%       x^n \log{(n + \tfrac{m}2)} (n+1)!
%     }
%   \biggr\rvert
%   = \lim_{n\to\infty}
%     \lvert x \rvert 
%     \biggl\lvert
%       \frac{
%         \log{(n + 1 + \tfrac{m}2)}
%       }{
%         \log{(n + \tfrac{m}2)} (n+1)
%       }
%     \biggr\rvert
%   % = \lim_{n\to\infty}
%   %   \lvert x \rvert 
%   %   \biggl\lvert
%   %     \frac{
%   %       n + \tfrac{m}2
%   %     }{
%   %       (n + 1 + \tfrac{m}2)((n+1) + (n + \tfrac{m}2) \log{(n + \tfrac{m}2)})
%   %     }
%   %   \biggr\rvert
%   = 0 < 1
%   \,, $$
% since
% $$
% \frac{\log{(x + a + 1)}}{x \log{(x + a)}}
%   \sim \frac{\log{(x+1)}}{(x-a) \log{x}}
%   \sim \frac{\log{(x+1)}}{x \log{x}}
%   \sim \frac{\tfrac1{(x+1)}}{1 + \log{x}}
%   \to 0
%   \,. $$
% A theorem from calculus states, that the formal series derivative (integral)
% coincides with the derivative (integral) of the function, corresponding to
% the power series (everywhere on the convergence region). And the convergence
% regions of derivative (integral) coincide with the region of the original
% power series.
%
Formally differentiating the convergent power series within $g_m$ yields
\begin{equation}  \label{eq:g-m-deriv}
  \frac{d}{d x} g_m(x)
    % = e^{-x} \sum_{n\geq 1} \frac{x^{n-1}}{(n-1)!} \psi(n + \tfrac{m}2) - g_m(x)
    % = e^{-x} \sum_{n\geq 0} \frac{x^n}{n!} \psi(n + \tfrac{m}2 + 1) - g_m(x)
    % = e^{-x} \sum_{n\geq 0} \frac{x^n}{n!} (
    %     \psi(n + \tfrac{m}2 + 1) - \psi(n + \tfrac{m}2)
    % )
    % = e^{-x} \sum_{n\geq 0} \frac{x^n}{n!} \tfrac1{n + \tfrac{m}2}
    % = e^{-x} \sum_{n\geq 0} \frac{x^n}{n!} (n + \tfrac{m}2)^{-1}
    = x^{-\tfrac{m}2} e^{-x} \sum_{n\geq 0}
        \frac{x^{n + \tfrac{m}2}}{n!} (n + \tfrac{m}2)^{-1}
    \,.
\end{equation}
From $
    e^t  = \sum_{n\geq 0} \tfrac{t^n}{n!}
$ on $\mathbb{R}$ and $
  \tfrac{x^\alpha}{\alpha}
    = \int_0^x t^{\alpha-1} dt
$ for $\alpha\neq 0$ we get
% by the monotone convergence theorem on ([0, x], \mathcal{B}([0, x]), dx)
\begin{align}  \label{eq:g-m-deriv-int-2}
  \eqref{eq:g-m-deriv}
    % = x^{-\tfrac{m}2} e^{-x} \sum_{n\geq 0}
    %     \frac{x^{n + \tfrac{m}2}}{n!} (n + \tfrac{m}2)^{-1}
    &= x^{-\tfrac{m}2} e^{-x} \sum_{n\geq 0}
        \frac1{n!} \int_0^x t^{n + \tfrac{m}2 - 1} dt
    % = x^{-\tfrac{m}2} e^{-x}
    %     \int_0^x t^{\tfrac{m}2 - 1} \sum_{n\geq 0} \frac{t^n}{n!} dt
    \notag \\
    &= x^{-\tfrac{m}2} e^{-x}
        \int_0^x t^{\tfrac{m}2 - 1} e^t dt
    \,.
\end{align}
Substitution $u^2 = t$ on $[0, \infty]$ with $2u du = dt$ yields
\begin{equation}  \label{eq:g-m-deriv-int-3}
  \frac{d}{d x} g_m(x)
    % = x^{-\tfrac{m}2} e^{-x}
    %     \int_0^x t^{\tfrac{m}2 - 1} e^t dt
    = 2 x^{-\tfrac{m}2} e^{-x}
        \int_0^{\sqrt{x}} u^{m - 1} e^{u^2} du
    \,.
\end{equation}
In particular, the derivative of \eqref{eq:g-m-series} for $m=1$ is
\begin{equation}  \label{eq:g-m-deriv-one}
  \frac{d}{d x} g_1(x)
    = 2 \frac{F(\sqrt{x})}{\sqrt{x}}
    \,,
\end{equation}
using Dawson's integral $
  F
  % \colon \mathbb{R} \to \mathbb{R}
  \colon x \mapsto e^{-x^2} \int_0^x e^{u^2} du
$.
%
Hence, the derivative of the expectation in \eqref{eq:improper-kl-div-real} with respect
to $\alpha$ is
$$
\frac{d}{d \alpha} g_1(\tfrac1{2\alpha})
  = -\frac1{2 \alpha^2} g_1'(\tfrac1{2\alpha})
  % = -\frac1{2 \alpha^2} 2 \tfrac{F(\sqrt{\tfrac1{2\alpha}})}{\sqrt{\tfrac1{2\alpha}}}
  % = -\frac1{2 \alpha^2} 2 \tfrac{F(\tfrac1{\sqrt{2\alpha}})}{\tfrac1{\sqrt{2\alpha}}}
  = -\frac1{\alpha} \sqrt{\frac2{\alpha}} F(\tfrac1{\sqrt{2\alpha}})
  \,. $$
Finally, since $\alpha$ is nonnegative, it is typically parameterized via its logarithm.
The gradient of \eqref{eq:improper-kl-div-real} w.r.t $\log \alpha$ is
\begin{equation}  \label{eq:real-kl-div-deriv-log}
  \frac{d \eqref{eq:improper-kl-div-real}}{d\log \alpha}
  % \frac{d}{d\log \alpha}
  %   \frac12 \mathbb{E}_{z \sim \mathcal{N}(0,1)}
  %     \log \lvert z + \tfrac1{\sqrt{\alpha}} \rvert^2
    % = \frac12 \frac{d\alpha}{d\log \alpha}
    %   \frac{d}{d\alpha} \bigl(\mathbb{E}\cdots \bigr)
    % = - \frac\alpha2 \tfrac1{\alpha}
    %   \sqrt{\tfrac2{\alpha}} F(\tfrac1{\sqrt{2\alpha}})
    = - \frac1{\sqrt{2\alpha}} F\bigl(\tfrac1{\sqrt{2\alpha}}\bigr)
    \,.
\end{equation}

We compute the Monte-Carlo estimator of \eqref{eq:improper-kl-div-real} on a sample of $10^7$
draws over an equally spaced grid of $\log \alpha$ in $[-12, +12]$ of size $4096$. The optimal
coefficients of the regression, proposed by \citet{molchanov_variational_2017}, are within
$1\%$ relative tolerance of the reported therein.
\begin{equation}  \label{eq:improper-kl-div-real-approx}
  \eqref{eq:improper-kl-div-real}
% - KL(\mathcal{N}(w \mid \mu, \alpha \mu^2) \|
%     \tfrac1{\lvert w \rvert})
%   \approx
%     k_1 \sigma(k_2 + k_3 \log \alpha)
%     + C \big\vert_{C = -k1}
%     - \tfrac12 \log (1 + e^{-\log \alpha})
  % = \tfrac12 \log \alpha
  %   - \mathbb{E}_{\xi \sim \mathcal{N}(1, \alpha)}
  %   \log{\lvert \xi \rvert} + C
  \approx
    \frac12 \log{\bigl(1 + e^{-\log \alpha}\bigr)}
    + k_1 \sigma\bigl(- (k_2 + k_3 \log \alpha)\bigr)
    % - k_1 \bigl(1 - \sigma(k_2 + k_3 \log \alpha)\bigr)
  \,,
\end{equation}
% in parctial implementations we suggest using softplus to numerically comptue the last
% term, since it is typically implemented in a floating-point stable fashion.
The numerically estimated derivative of the penalty term with respect to $\log \alpha$ using
forward differences seems to be following \eqref{eq:real-kl-div-deriv-log} very closely (up to
sampling error), see fig.~\ref{fig:molchanov-derivative-replica}. The derivative of
\eqref{eq:improper-kl-div-real-approx} with respect to $\log \alpha$ appears to be very close
to \eqref{eq:real-kl-div-deriv-log}. We compute a similar Monte-Carlo estimate for
$\cplx$ variational dropout KL divergence term in \eqref{eq:c-vd-kl-div} with $\beta = 2$,
fit the best approximation \eqref{eq:improper-kl-div-real-approx}, and compare the derivatives.

\begin{figure}[!t]
  \centering
  \includegraphics[width=\columnwidth]{grad_log.pdf}
  \caption{$\tfrac{d f(\alpha)}{d \log{\alpha}}$ of the fit proposed by
  \citet{molchanov_variational_2017}, MC estimate of \eqref{eq:improper-kl-div-real},
  and the exact derivative using \eqref{eq:real-kl-div-deriv-log}.}
  \label{fig:molchanov-derivative-replica}
\end{figure}

% section real-chisq-grad (end)

\section{MNIST-like experiments} % (fold)
\label{sec:mnist_like_experiments}

To supplement the results, reported in the main text (figures \ref{fig:paper__mnist-like__trade-off__VD__fft}
and \ref{fig:paper__mnist-like__trade-off__VD__raw}), we conduct additional experiments
with the same basic set-up, but over a finer grid of KL-divergence term coefficients.
%
These experiments lend further evidence in support of the conclusions, drawn in the main
paper, namely, that the $\cplx$-valued extension of the $\real$-valued variational dropout
can offer significant compression levels without much loss in performance. At the same
time, the test performance of both $\real$ and $\cplx$ compressed networks benefits from
an extra fine-tuning stage, during which sparsity masks are fixed.

One intriguing observation, is that on MNIST-like images the dense network, taking Fourier
features on input, exhibits gradually improving test-split performance. This is in stark
contrast to raw image input, where the performance stays at one level in the small compression
regime, and then gradually decays at high compression levels. Noteworthy is the presence
of a trough in the pre fine-tuning stage performance in the small compression regime on figures
\ref{fig:appendix__mnist-like__trade-off__ARD__raw},
\ref{fig:appendix__mnist-like__trade-off__VD__raw},
\ref{fig:appendix__cmp__mnist-like__trade-off__ARD__raw},
and \ref{fig:appendix__cmp__mnist-like__trade-off__VD__raw}, which then slowly disappears
as compression increases.

The following scatter plots depict the samples from the compression-performance trade-off
curve of simple networks studied in the experiments on the MNIST-like datasets. Transparent
horizontal bands on each plot represent min-max performance spread of an uncompressed
network on the test split. The points illustrate the final trade-off of the compressed
networks after fine-tuning, while their tails show the performance gain or loss due to
the fine-tuning.

We recap the set-up of each additional experiment on MNIST-like datasets:
\begin{itemize}
  \item staged training, with weight and optimizer state transfer
  \item $40$, $75$, and $40$ epochs for ``dense'', ``sparsify'', and ``fine-tune'' stagers, respectively
  \item ADAM optimizer with learning rate $10^{-3}$ before the $10$-th, and $10^{-4}$
  afterwards
  \item global $\ell_2$-norm gradient clipping set at $\tfrac12$
  \item training batches of size $128$ from a fixed randomly subset ($10^4$ images) of the training split
  \item the multiplier of the Kullback-Leibler term in ELBO varies over $
    C \in \{
      \tfrac32 2^{-\tfrac{k}2} \colon k=2, \cdots, 38
    \}
  $
  \item the sparsification threshold $\tau$ is set to $-\tfrac12$
\end{itemize}
Each experiment has been performed $5$ times to obtain a small sample of performance-compression
values for each setting of $C$.

Each figure shows the compression-accuracy trade-off of a particular method and input features
for \textit{SimpleConvModel} and \textit{TwoLayerDenseModel} models for all four of the studied
datasets (described in the main text): \emph{top-left} EMNIST-Letters, \emph{top-right} KMNIST,
\emph{bottom-left} Fashion MNIST, and \emph{bottom-right} MNIST.
%
Figures \ref{fig:appendix__mnist-like__trade-off__ARD__fft}, \ref{fig:appendix__mnist-like__trade-off__VD__fft},
\ref{fig:appendix__mnist-like__trade-off__ARD__raw}, and \ref{fig:appendix__mnist-like__trade-off__VD__raw}
present $\real$ and $\cplx$ models with \emph{the same architecture}. We also report the
trade-off comparison, when the argument by \citet{monning_evaluation_2018} for higher intrinsic
capacity of $\cplx$-valued networks has been taken into account.
%
We compare $\real$ networks against $\tfrac12 \cplx$ with half the number of parameters
for raw input features on figures \ref{fig:appendix__cmp__mnist-like__trade-off__ARD__raw},
and \ref{fig:appendix__cmp__mnist-like__trade-off__VD__raw}, and $2 \real$ with
double the number of parameters against $\cplx$ for Fourier input features on figures
\ref{fig:appendix__cmp__mnist-like__trade-off__ARD__fft} and
\ref{fig:appendix__cmp__mnist-like__trade-off__VD__fft}.

The following observations can be made from the conducted experiment:
\begin{itemize}
  \item $\cplx$-valued networks do not have as much spare capacity as $2 \real$ due to
  constants imposed by multiplication in $\cplx$ numbers (\ref{fig:appendix__cmp__mnist-like__trade-off__ARD__raw},
  \ref{fig:appendix__cmp__mnist-like__trade-off__VD__raw})
  \item Variational Dropout and Automatic Relevance Determination offer similar compression-performance
  trade-off, with negligibly lower compression by the latter (both $\real$ and $\cplx$ versions,
  e.g. figures \ref{fig:appendix__mnist-like__trade-off__ARD__raw} and \ref{fig:appendix__mnist-like__trade-off__VD__raw})
  \item smaller networks tend to be less compressible (figures \ref{fig:appendix__cmp__mnist-like__trade-off__ARD__fft},
  \ref{fig:appendix__cmp__mnist-like__trade-off__VD__fft})
\end{itemize}

\begin{figure}[b]
  \centering
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figure__mnist-like__trade-off/appendix__ARD__emnist_letters__fft__-0.5.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figure__mnist-like__trade-off/appendix__ARD__kmnist__fft__-0.5.pdf}
  \end{subfigure} \\ %
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figure__mnist-like__trade-off/appendix__ARD__fashionmnist__fft__-0.5.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figure__mnist-like__trade-off/appendix__ARD__mnist__fft__-0.5.pdf}
  \end{subfigure}
  \caption{%
    The trade-off of ARD method for $\real$ and $\cplx$ models using Fourier features.
  }
  \label{fig:appendix__mnist-like__trade-off__ARD__fft}
\end{figure}

\begin{figure}[b]
  \centering
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figure__mnist-like__trade-off/appendix__VD__emnist_letters__fft__-0.5.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figure__mnist-like__trade-off/appendix__VD__kmnist__fft__-0.5.pdf}
  \end{subfigure} \\ %
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figure__mnist-like__trade-off/appendix__VD__fashionmnist__fft__-0.5.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figure__mnist-like__trade-off/appendix__VD__mnist__fft__-0.5.pdf}
  \end{subfigure}
  \caption{%
    The trade-off of VD method for $\real$ and $\cplx$ models using Fourier features.
  }
  \label{fig:appendix__mnist-like__trade-off__VD__fft}
\end{figure}

\begin{figure}[b]
  \centering
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figure__mnist-like__trade-off/appendix__ARD__emnist_letters__raw__-0.5.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figure__mnist-like__trade-off/appendix__ARD__kmnist__raw__-0.5.pdf}
  \end{subfigure} \\%
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figure__mnist-like__trade-off/appendix__ARD__fashionmnist__raw__-0.5.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figure__mnist-like__trade-off/appendix__ARD__mnist__raw__-0.5.pdf}
  \end{subfigure}
  \caption{%
    The trade-off of ARD method for $\real$ and $\cplx$ models using raw features.
  }
  \label{fig:appendix__mnist-like__trade-off__ARD__raw}
\end{figure}

\begin{figure}[b]
  \centering
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figure__mnist-like__trade-off/appendix__VD__emnist_letters__raw__-0.5.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figure__mnist-like__trade-off/appendix__VD__kmnist__raw__-0.5.pdf}
  \end{subfigure} \\%
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figure__mnist-like__trade-off/appendix__VD__fashionmnist__raw__-0.5.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figure__mnist-like__trade-off/appendix__VD__mnist__raw__-0.5.pdf}
  \end{subfigure}
  \caption{%
    The trade-off of VD method for $\real$ and $\cplx$ models using raw features.
  }
  \label{fig:appendix__mnist-like__trade-off__VD__raw}
\end{figure}

\begin{figure}[b]
  \centering
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figure__mnist-like__trade-off/appendix__cmp__ARD__emnist_letters__fft__-0.5.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figure__mnist-like__trade-off/appendix__cmp__ARD__kmnist__fft__-0.5.pdf}
  \end{subfigure} \\ %
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figure__mnist-like__trade-off/appendix__cmp__ARD__fashionmnist__fft__-0.5.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figure__mnist-like__trade-off/appendix__cmp__ARD__mnist__fft__-0.5.pdf}
  \end{subfigure}
  \caption{%
    The trade-off of ARD method for $2\real$ and $\cplx$ models using Fourier features.
  }
  \label{fig:appendix__cmp__mnist-like__trade-off__ARD__fft}
\end{figure}

\begin{figure}[b]
  \centering
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figure__mnist-like__trade-off/appendix__cmp__VD__emnist_letters__fft__-0.5.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figure__mnist-like__trade-off/appendix__cmp__VD__kmnist__fft__-0.5.pdf}
  \end{subfigure} \\ %
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figure__mnist-like__trade-off/appendix__cmp__VD__fashionmnist__fft__-0.5.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figure__mnist-like__trade-off/appendix__cmp__VD__mnist__fft__-0.5.pdf}
  \end{subfigure}
  \caption{%
    The trade-off of VD method for $2\real$ and $\cplx$ models using Fourier features.
  }
  \label{fig:appendix__cmp__mnist-like__trade-off__VD__fft}
\end{figure}

\begin{figure}[b]
  \centering
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figure__mnist-like__trade-off/appendix__cmp__ARD__emnist_letters__raw__-0.5.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figure__mnist-like__trade-off/appendix__cmp__ARD__kmnist__raw__-0.5.pdf}
  \end{subfigure} \\%
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figure__mnist-like__trade-off/appendix__cmp__ARD__fashionmnist__raw__-0.5.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figure__mnist-like__trade-off/appendix__cmp__ARD__mnist__raw__-0.5.pdf}
  \end{subfigure}
  \caption{%
    The trade-off of ARD method for $\real$ and $\tfrac12\cplx$ models using raw features.
  }
  \label{fig:appendix__cmp__mnist-like__trade-off__ARD__raw}
\end{figure}

\begin{figure}[b]
  \centering
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figure__mnist-like__trade-off/appendix__cmp__VD__emnist_letters__raw__-0.5.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figure__mnist-like__trade-off/appendix__cmp__VD__kmnist__raw__-0.5.pdf}
  \end{subfigure} \\%
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figure__mnist-like__trade-off/appendix__cmp__VD__fashionmnist__raw__-0.5.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figure__mnist-like__trade-off/appendix__cmp__VD__mnist__raw__-0.5.pdf}
  \end{subfigure}
  \caption{%
    The trade-off of VD method for $\real$ and $\tfrac12\cplx$ models using raw features.
  }
  \label{fig:appendix__cmp__mnist-like__trade-off__VD__raw}
\end{figure}

\begin{figure}[b]
  \centering
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figure__mnist-like__trade-off/legacy__VD__emnist_letters__fft__-0.5.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figure__mnist-like__trade-off/legacy__VD__kmnist__fft__-0.5.pdf}
  \end{subfigure} \\%
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figure__mnist-like__trade-off/legacy__VD__fashionmnist__fft__-0.5.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figure__mnist-like__trade-off/legacy__VD__mnist__fft__-0.5.pdf}
  \end{subfigure}
  \caption{%
    The trade-off of VD method for $\real$ and $\cplx$ models using Fourier features (main text).
  }
  \label{fig:paper__mnist-like__trade-off__VD__fft}
\end{figure}

\begin{figure}[b]
  \centering
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figure__mnist-like__trade-off/legacy__VD__emnist_letters__raw__-0.5.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figure__mnist-like__trade-off/legacy__VD__kmnist__raw__-0.5.pdf}
  \end{subfigure} \\%
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figure__mnist-like__trade-off/legacy__VD__fashionmnist__raw__-0.5.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figure__mnist-like__trade-off/legacy__VD__mnist__raw__-0.5.pdf}
  \end{subfigure}
  \caption{%
    The trade-off of VD method for $\real$ and $\cplx$ models using raw features (main text).
  }
  \label{fig:paper__mnist-like__trade-off__VD__raw}
\end{figure}

% section mnist_like_experiments (end)

\clearpage

\section{CIFAR10 experiments} % (fold)
\label{sec:cifar_experiments}

In addition to the results reported in the main paper, we conduct additional experiment
on CIFAR10 and VGG16 with a much more slowly annealed learning rate. Similar to MNIST-like
experiments, we start with the rate set to $10^{-3}$ and then reduce it by $10$ after the
$10$-the epoch. In summary, for the CIFAR10 experiments we reuse the set-up with the following
changes:
\begin{itemize}
  \item we allocate $20$, $40$, and $20$ epochs to each stage
  \item the weight of the KL divergence term varies over a smaller grid $
    C \in \{
      \tfrac32 2^{-\tfrac{k}2} \colon k=7, \cdots, 15
    \}
  $
  \item we use full $50$k training split of the CIFAR10 dataset for training and raw color
  features only
  \item dataset is randomly augmented: every image within a batch of size $128$ is randomly
  cropped and flipped horizontally
\end{itemize}
Random cropping is done by zero-padding each side of a $32\times 32$ image by four pixels
and then a extracting a random $32\times 32$ patch from the $40\times 40$ intermediate image.

We experiment with $\real$ VGG16 network \citep{simonyan_very_2015} and its $\cplx$-valued
variant, which straightforwardly replaces $\real$ layers with their $\cplx$ counterparts.

\begin{figure}[htb]
  \centering
  \includegraphics[width=\linewidth]{figure__cifar__trade-off/appendix__augmentedcifar10__raw__-0.5.pdf}
  \caption{%
    The compression-accuracy trade-off for $\real$ and $\cplx$ VGG16 using raw input features.
  }
  \label{fig:appendix__cifar__trade-off__VGG16__raw}
\end{figure}

% section cifar_experiments (end)

\section{Backpropagation through $\cplx$-networks} % (fold)
\label{sub:wirtinger_calculus}

% essential intro into Wirtinger calculus (CR)
% https://math.stackexchange.com/a/444493
Wirtinger ($\cplx\real$) calculus relies on the natural identification of $\cplx$ with $
  \real \times \real
$, and regards $
  f\colon \cplx \to \cplx
$ as an equivalent in algebraic sense function $F\colon \real^2 \to \cplx$ defined $
  f(z) = f(u + jv) = F(u, v)
$. Within this framework the differential of $f$ at $z = u + jv \in \cplx$ is
$$
df(z)
  = \frac{\partial f}{\partial z} dz
    + \frac{\partial f}{\partial \conj{z}} d\conj{z}
   % = \frac12\bigl(
   %    \frac{\partial}{\partial u}
   %    - j \frac{\partial}{\partial v}
   % \bigr) F (du - j dv)
   % + \frac12\bigl(
   %    \frac{\partial}{\partial u}
   %    + j \frac{\partial}{\partial v}
   % \bigr) F (du - j dv)
   % = \frac12 \bigl(
   %    \frac{\partial F}{\partial u} du - j \frac{\partial F}{\partial v} du
   %    + j \frac{\partial F}{\partial u} dv + \frac{\partial F}{\partial v} dv
   % \bigr)
   % + \frac12 \bigl(
   %    \frac{\partial F}{\partial u} du + j \frac{\partial F}{\partial v} du
   %    - j \frac{\partial F}{\partial u} dv + \frac{\partial F}{\partial v} dv
   % \bigr)
   = \frac{\partial F}{\partial u} du
     + \frac{\partial F}{\partial v} dv
   = dF(u, v)
  \,, $$
with formally defined derivatives $
  \tfrac{\partial}{\partial z}
    = \tfrac12 \bigl(
      \tfrac{\partial}{\partial u}
      - j \tfrac{\partial}{\partial v}
    \bigr)
$ and $
  \tfrac{\partial}{\partial \conj{z}}
    = \tfrac12 \bigl(
      \tfrac{\partial}{\partial u}
      + j \tfrac{\partial}{\partial v}
    \bigr)
$, and differentials $dz = du + j dv$ and $d\conj{z} = du - j dv$. This implies that the
complex argument and its conjugate are treated as independent variables. Cauchy-Riemann
conditions $
  -j \tfrac{\partial F}{\partial v} = \tfrac{\partial F}{\partial u}
$ can be expressed as $
  \tfrac{\partial f}{\partial \conj{z}} = 0
$ in this notation. Thus $\cplx\real$ calculus subsumes the usual $\cplx$-calculus on
holomorphic functions, when $f(z)$, regarded as $f(z, \conj{z})$, is constant with respect
to $\conj{z}$. The usual rules of calculus, like chain and product rules, follow directly
from the definition of the operators, e.g.
$$
  \frac{\partial (f\circ g)}{\partial z}
    = \frac{\partial f(g(z))}{\partial g} \frac{\partial g(z)}{\partial z}
    + \frac{\partial f(g(z))}{\partial \conj{g}} \frac{\partial \conj{g(z)}}{\partial z}
    % = \nabla G \nabla F
  \,. $$
% Show that $dh(z) = df(g(z)) dg(z)$.
% $h(z) = f(g(z)) = F(G_r(u, v), G_i(u, v)) = H(u, v)$
% $$
% dH
%   = \tfrac{\partial F}{\partial x} \tfrac{\partial G_r}{\partial u} du
%   + \tfrac{\partial F}{\partial x} \tfrac{\partial G_r}{\partial v} dv
%   + \tfrac{\partial F}{\partial y} \tfrac{\partial G_i}{\partial u} du
%   + \tfrac{\partial F}{\partial y} \tfrac{\partial G_i}{\partial v} dv
%   = \tfrac{\partial F}{\partial r} d\Re g(z)
%   + \tfrac{\partial F}{\partial i} d\Im g(z)
%   = df(\Re g(z) + j\Im g(z)) d g(z)
%   = df(g(z)) d g(z)
%   \,. $$

In machine learning tasks the target objective is typically empirical risk, which has to
be real-valued to be minimized. Nevertheless, the expression of the $\cplx\real$ gradient
is compatible with what is expected, when $f$ is treated like a $\real^2$ function. For a
real-valued $f\colon \cplx \to \real$ we have $\conj{f} = f$, which implies $
  \tfrac{\partial f}{\partial \conj{z}}
    = \tfrac{\partial \conj{f}}{\partial \conj{z}}
    = \conj{\tfrac{\partial f}{\partial z}}
$, whence
$$
df
  = \tfrac{\partial f}{\partial z} dz
    + \tfrac{\partial f}{\partial \conj{z}} d\conj{z}
  % = \conj{\tfrac{\partial \conj{f}}{\partial \conj{z}}} dz
  % + \conj{\conj{\tfrac{\partial f}{\partial \conj{z}}} dz}
  = 2 \Re \Bigl(
    \conj{\tfrac{\partial f}{\partial \conj{z}}} dz
  \Bigr)
  = 2 \Re \bigl(
    \tfrac{\partial f}{\partial z} dz
  \bigr)
  \,. $$
Thus, the gradient of $f$ at $z$ is given by $
  \nabla_z f(z)
    = \tfrac{\partial F}{\partial u}
      + j \tfrac{\partial F}{\partial v}
$.

Natural identification of $\cplx$ with $\real\times \real$, storing the real and imaginary
parts in interleaved format, emulation of $\cplx$-algebra using $\real$-valued arithmetic,
and Wirtinger calculus make it possible to reuse $\real$ back-propagation and existing
auto-differentiation frameworks \citep{trabelsi_deep_2017}.

% section wirtinger_calculus (end)

\section{$\cplx$-Linear operator representation} % (fold)
\label{sec:c-linear_operator_representation}

% proof that such operators exist and are unique (well this is an obvious statement)
Consider $L \colon \cplx^m \to \cplx^n$ -- linear in $\cplx$. Then $
  L(u + jv) = L u + j L v
$ for any $u, v \in \real^m$, which implies that the effect of $L$ as $\cplx$-linear
operator is determined by its restriction onto $\real^n$. Let $
  F = L\vert_{\real^m}
  \colon \real^m \to \cplx^n
$ and observe that $F_r = \Re \circ F$ and $F_i = \Im \circ F$ are $\real$-linear operators
such that $F = F_r + j F_i$ (pointwise). Indeed,
$$
  F_r(a + \lambda b)
  % = \Re F(a + \lambda b)
  = \Re L(a + \lambda b)
  % = \Re \bigl( L a + \lambda L b \bigr)
  = \Re L a + \lambda \Re L b
  % = \Re F a + \lambda \Re F b
  = F_r a + \lambda F_r b
  \,. $$
Therefore for any $\cplx$-linear operator $L$ there are $\real$-linear operators $U, V$
such that
$$
L z 
  = (U + j V) (\Re z + j \Im z)
  % = (U + j V) \Re z + j (U + j V) \Im z
  % = U \Re z + j V \Re z + j \bigl(U \Im z + j V \Im z \bigr)
  = U \Re z - V \Im z + j \bigl( V \Re z + U \Im z \bigr)
  % = (U + j V) z
  \,. $$
Uniqueness of these operators follows, if this decomposition is applied to any $z$ with
$\Im z = 0$.

% section c-linear_operator_representation (end)

% section appendix (end)

\end{document}
