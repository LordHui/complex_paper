\documentclass[a4paper,10pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{grffile}

\usepackage{subcaption}

\usepackage{booktabs}

% \usepackage{lipsum}

\newcommand{\important}[1]{\textbf{\color{red} #1}}
\newcommand{\verify}{\important{verify}}
\newcommand{\rewrite}{\important{rewrite}}

\newcommand{\todo}{\important}  % alias

\newcommand{\real}{\mathbb{R}}
\newcommand{\hop}{\dagger}
\newcommand{\cplx}{\mathbb{C}}

\renewcommand{\vec}{\overrightarrow}

\title{Sparsifying $\cplx$-valued networks}
\author{Ivan Nazarov}

\begin{document}
\maketitle

% {\color{red} \lipsum[1-3]}

\section{Introduction} % (fold)
\label{sec:introduction}

Why sparsity matters, why $\cplx$ networks?

Where are $\cplx$-networks used? and do these applications warrant sparsity?

% section introduction (end)

\section{$\cplx$-valued networks} % (fold)
\label{sec:c_valued_networks}

In this section i remind the reader what the complex-valued networks are, how
they are implemented and how much of $\cplx$-ness they have. Mention careful
design.

I will describe the linear layers (dense, masked, and convolutions) and
activations borrowed straight from the $\real$-values networks. Then i shall
mention that this has been tried before and list papers that one way or another
address $\cplx$-data processing with neural networks.

\cite{trabelsi_deep_2017} provide a set of building blocks for deep $\cplx$-valued
networks. The core of their work is structured framework for complex-valued operations
including representation and linear layers (dense and convolutional) as well
as $\cplx$-valued activations and complex batch-normalization and weight initialization.

Following \cite{trabelsi_deep_2017} we identify $\cplx$ with $\real^2$ via $
  z_\mathrm{r} + i z_\mathrm{i} \mapsto (z_\mathrm{r}, z_\mathrm{i})
$, storing the real and imaginary parts in interleaved format, and emulate $\cplx$-algebra
using $\real$-valued arithmetic. Thus a $\cplx$-network is, essentially, a $\real$-network
with \textit{double} the layers' widths to accommodate real and imaginary parts and
with constraints on layers' parameters that make them respect $\cplx$-algebra. Thus
$\cplx$-valued networks can be retrofitted into or built upon the existing $\real$-valued
autograd libraries.

In particular, common linear layers, such as dense layers and convolutions,
$
  L \colon \cplx^{\mathrm{[in]}}
    \to \cplx^{\mathrm{[out]}}
$ act upon their inputs thus:
$$
\real^{\mathrm{[in]} \times 2}
  \to \real^{\mathrm{[out]} \times 2}
  \colon (x_r, x_i)
    \mapsto \Bigl(
      L_r x_r - L_i x_i,
      L_r x_i + L_i x_r
    \Bigr)
  \,, $$
where $
  L_r, L_i
    \colon \real^{\mathrm{[in]}}
      \to \real^{\mathrm{[out]}}
$ are the unique $\real$-linear operators that make up $L = L_r + j L_i$ the overall
effect of the layer. For instance, to respect $\cplx$-arithmetic a linear layer acting
on $x$ should have $W_{ri} = - W_{ir}$ and $W_{rr} = W_{ii}$ in
$$
  \begin{pmatrix}
    y_r \\ y_i
  \end{pmatrix}
    = \begin{pmatrix}
      {\color{red} W_{rr}} & {\color{blue} W_{ri}} \\ 
      {\color{blue} W_{ir}} & {\color{red} W_{ii}}
    \end{pmatrix}
    \begin{pmatrix}
      x_r \\ x_i
    \end{pmatrix}
    + \begin{pmatrix}
      b_r \\ b_i
    \end{pmatrix}
  \,. $$
Other algebraic operations that might be used in a networks are also designed in such a way as
to honour $\cplx$-operations. It is worth pointing out that the key advantage of the $\cplx$-%
constraints is that they permit use of Strassen's $3$M multiplication algorithm, potentially
saving up to $25\%$ on floating point multiplications.

% proof that such operators exist and are unique (well this is an obvious statement)
Consider $L\colon \cplx^m \to \cplx^n$ -- linear in $\cplx$. Then $
  L(u + jv) = L u + j L v
$ for any $u, v \in \real^m$, which implies that the effect of $L$ as $\cplx$-linear
operator is determined by its restriction onto $\real^n$. Let $
  F = L\vert_{\real^m}
  \colon \real^m \to \cplx^n
$ and observe that $F_r = \Re \circ F$ and $F_i = \Im \circ F$ are $\real$-linear operators
such that $F = F_r + j F_i$ (pointwise). Indeed,
$$
  F_r(a + \lambda b)
  % = \Re F(a + \lambda b)
  = \Re L(a + \lambda b)
  % = \Re \bigl( L a + \lambda L b \bigr)
  = \Re L a + \lambda \Re L b
  % = \Re F a + \lambda \Re F b
  = F_r a + \lambda F_r b
  \,. $$
Therefore for any $\cplx$-linear operator $L$ there exist $\real$-linear operators $U, V$
such that
$$
L z 
  = (U + j V) (\Re z + j \Im z)
  % = (U + j V) \Re z + j (U + j V) \Im z
  % = U \Re z + j V \Re z + j \bigl(U \Im z + j V \Im z \bigr)
  = U \Re z - V \Im z + j \bigl( V \Re z + U \Im z \bigr)
  % = (U + j V) z
  \,. $$

Are these operators unique? Yes. Indeed, for $z$ with $\Im z = 0$ 
$$
U_1 \Re z + j V_1 \Re z
  = U_1 \Re z - V_1 \Im z + j \bigl( V_1 \Re z + U_1 \Im z \bigr)
  = L z
  = U_2 \Re z - V_2 \Im z + j \bigl( V_2 \Re z + U_2 \Im z \bigr)
  = U_2 \Re z + j V_2 \Re z
  \,, $$
implies that $U_1 = U_2$ and $V_1 = V_2$ pointwise.

As far as non-linearities are concerned, \cite{trabelsi_deep_2017} consider a few truly complex-%
valued ones, that are nevertheless not $\cplx$-differentiable: the most frequently used activations are the common $\real$-valued non-linearities applied to real and
imaginary components independently.

\todo{something about activations}
modRelu -- relu on the modulus of a complex value (preserves the angle)
$
\mathrm{modReLU}
  \colon z \mapsto z \bigl(
    1 - \tfrac\theta{\lvert z \rvert}
  \bigr)_+
$
$
\mathrm{modReLU}
  \colon r e^{i \phi} \mapsto e^{i \phi} (r - \theta)_+
$

\todo{finish activations}

\subsection{Backprop through $\cplx$-networks} % (fold)
\label{sub:backprop_through_c_networks}

The question of differentiability of complex-valued networks is tricky, since they include
usually non-differentiable non-linearities. However, a more severe problem stems from the loss
objective begin real-valued. Indeed, if $f\colon \cplx \to \cplx$ were $\cplx$-differentiable
(holomorphic), the composition of $f$ with a non-trivial $h \colon \cplx\to \real$ makes it
not $\cplx$-differentiable with respect parameters, since $h$ violates Cauchy-Riemann conditions.
This problem is commonly dealt with by using Wirtinger, or $\cplx\real$ calculus, which subsumes
$\cplx$-calculus, but is also applicable to non-analytic (non-holomorphic) functions of
$\cplx$-argument, \cite{adali_complex-valued_2011} and \cite{trabelsi_deep_2017}. In essence,
this approach views $f$ as a function of two independent variables $z$ and its conjugate $z^\hop$
and extends the derivatives appropriately. This approach nautrally extends the chain rule to
nonanalytic functions and can be directly reformulated in terms of derivatives with respect
to real and imaginary parts, making it possible to use and apply the rules of $\real$-calculus
to the network represented in double-$\real$ fashion and use existing $\real$-valued
backpropagation in autograd software.

Take $f\colon \cplx \to \cplx$ and identify it with $F\colon \real^2 \to \cplx$ function
via $F(u, v) = f(z)\vert_{z=u + j v}$. Wirtinger calculus introduces two formal derivative
operators
$
  \tfrac{\partial}{\partial z}
    = \tfrac12 \bigl(
      \tfrac{\partial}{\partial u}
      - j \tfrac{\partial}{\partial v}
    \bigr)
$ and $
  \tfrac{\partial}{\partial z^\hop}
    = \tfrac12 \bigl(
      \tfrac{\partial}{\partial u}
      + j \tfrac{\partial}{\partial v}
    \bigr)
$, defines $dz = du + j dv$, $dz^\hop = du - j dv$, and the differential of $f$ as
$$
df = \tfrac{\partial f}{\partial z} dz
    + \tfrac{\partial f}{\partial z^\hop} dz^\hop
   % = \tfrac12\bigl(
   %    \tfrac{\partial}{\partial u}
   %    - j \tfrac{\partial}{\partial v}
   % \bigr) F (du - j dv)
   % + \tfrac12\bigl(
   %    \tfrac{\partial}{\partial u}
   %    + j \tfrac{\partial}{\partial v}
   % \bigr) F (du - j dv)
   % = \tfrac12 \bigl(
   %    \tfrac{\partial F}{\partial u} du - j \tfrac{\partial F}{\partial v} du
   %    + j \tfrac{\partial F}{\partial u} dv + \tfrac{\partial F}{\partial v} dv
   % \bigr)
   % + \tfrac12 \bigl(
   %    \tfrac{\partial F}{\partial u} du + j \tfrac{\partial F}{\partial v} du
   %    - j \tfrac{\partial F}{\partial u} dv + \tfrac{\partial F}{\partial v} dv
   % \bigr)
   = \tfrac{\partial F}{\partial u} du
     + \tfrac{\partial F}{\partial v} dv
   = dF
  \,. $$
Thus the complex value and its conjugate are treated as independent variables. In this notation
Cauchy-Riemann conditions are expressed as $
  \tfrac{\partial f}{\partial z^\hop} = 0
$, or $
  -j\tfrac{\partial F}{\partial v} = \tfrac{\partial F}{\partial v}
$. Cauchy-Riemann impose a rigid structure on real and imaginary parts of $F(u, v)$, namely $
  \tfrac{\partial F_{\Re }}{\partial u} = \tfrac{\partial F_{\Im }}{\partial v}
$ and $
  \tfrac{\partial F_{\Re }}{\partial v} = - \tfrac{\partial F_{\Im }}{\partial u}
$. Thus Wirtinger calculus subsumes the usual $\cplx$-calculus, when $
  \tfrac{\partial f}{\partial z^\hop} = 0
$, i.e. $
  f(z) = g(z, z^\hop) = F(\tfrac12 (z + z^\hop), \tfrac1{j 2} (z - z^\hop))
$ does not depend on $z^\hop$.

For a real-valued $f\colon \cplx \to \real$ we have $f^\hop = f$, whence $
  \tfrac{\partial f}{\partial z^\hop}
    = \tfrac{\partial f^\hop}{\partial z^\hop}
    = \bigl(\tfrac{\partial f}{\partial z} \bigr)^\hop
$. Hence
$$
df
  = \tfrac{\partial f}{\partial z} dz
    + \tfrac{\partial f}{\partial z^\hop} dz^\hop
  % = \bigl(
  %   \tfrac{\partial f^\hop}{\partial z^\hop}
  % \bigr)^\hop dz
  % + \bigl(
  %   \bigl( \tfrac{\partial f}{\partial z^\hop} \bigr)^\hop dz
  % \bigr)^\hop
  = 2 \Re \bigl( \tfrac{\partial f}{\partial z^\hop} \bigr)^\hop dz
  \,. $$
Hence the gradient of $f$ at $z$ is given by $
  \tfrac{\partial f}{\partial z^\hop}
    = \tfrac{\partial F}{\partial u}
      + j \tfrac{\partial F}{\partial v}
$. Thus it can be seen, that under Wirtinger calculus it is possible to reuse $\real$-backpropagation.

% Show that $dh(z) = df(g(z)) dg(z)$.
% $h(z) = f(g(z)) = F(G_r(u, v), G_i(u, v)) = H(u, v)$
% $$
% dH
%   = \tfrac{\partial F}{\partial x} \tfrac{\partial G_r}{\partial u} du
%   + \tfrac{\partial F}{\partial x} \tfrac{\partial G_r}{\partial v} dv
%   + \tfrac{\partial F}{\partial y} \tfrac{\partial G_i}{\partial u} du
%   + \tfrac{\partial F}{\partial y} \tfrac{\partial G_i}{\partial v} dv
%   = \tfrac{\partial F}{\partial r} d\Re g(z)
%   + \tfrac{\partial F}{\partial i} d\Im g(z)
%   = df(\Re g(z) + j\Im g(z)) d g(z)
%   = df(g(z)) d g(z)
%   \,. $$
% subsection backprop_through_c_networks (end)

% section c_valued_networks (end)

\section{Related research} % (fold)
\label{sec:realted_research}

\subsection{On DPD} % (fold)
\label{sub:on_dpd}

Physical Radio Frequency power amplifiers, being electronic devices, have imperfections
such as parasitic inductance and capacitance, which make their gain respond non-linearly
to the input modulated signal. Although these effects are minimized by careful circuit
design, they cannot be eliminated entirely. Digital signal predistortion aims at rectifying
non-linear

 A non-linear block generating the reverse transfer
    function to compensate for the non-linearities

% subsection on_dpd (end)

% section realted_research (end)

\section{Complex Variational Dropout} % (fold)
\label{sec:complex_varaitional_dropout}

In this section we outline and derive the variational dropout technique for complex-%
valued networks, which re-traces the evolution of dropout technique for real-valued
weights.

Variational dropout, which enables automatic relevance detection by learning individual
per-weight dropout rates, was proposed by \cite{kingma_variational_2015} as a natural
generalization of the Gaussian Dropout \cite{srivastava_dropout_2014,wang_fast_2013},
itself -- a generalization of the Binary (Bernoulli) Dropout technique \cite{hinton_improving_2012}.

Let $W \in\real^{n\times m}$, and $b\in \real^n$ and consider the action of a linear
layer on its input $x$: $y = W x + b$ for $y\in \real^n$, $x\in \real^m$. The key idea
of Bernoulli dropout is to mask elements in the matrix $W$ of the layer with some given
probability $p$: $W = \theta \odot \xi$, there $\theta$ is the learnt weight matrix,
and is an iid Bernoulli mask $\xi \sim \mathcal{B}(\{0, \tfrac1{1-p}\}, 1-p)$. During
training, a dropout mask $\xi$ is used to compute $W$, and during evaluation the weights
are fixed to the expectation of $W$ which is $\theta$. The discussion and analysis in
\cite{kingma_variational_2015}, suggest that drawing one common dropout mask for all
data in the minibatch reduces statistical efficiency the estimator of the weights'
gradient.

Gaussian dropout works in a similar fashion, except the dropout mask $\xi$ is iid
$\mathcal{N}(1, \alpha=\tfrac{p}{1-p})$, i.e. a multiplicative Gaussian noise is
injected into the network.

% set by step we retell the story of kingmaetal2015

\cite{kingma_variational_2015} also propose the \textit{local reparameterization trick},
which improves efficiency stochastic gradient variational inference by translating
the uncertainty from the weights into local output noise of each linear layer. Without
this it would have been necessary to draw new set of random weights per each element
in a mini-batch.

$$
  \mathcal{N}(\theta_{ij}, \alpha \theta^2_{ij})
  \overset{\mathcal{D}}{\sim}
  \theta_{ij} \mathcal{N}(1, \alpha)
  \,, $$

\cite{molchanov_variational_2017} propose additional step in the local reparametrization,
which further reduces the variance of the gradient estimator for each dropped out
weight. Their simple formal change of variables $(\theta, \alpha) \to (\theta, \sigma^2)$
decouples the stochastic component from the weights \rewrite:
$$
  w_{ij} = \theta_{ij} + \theta_{ij} \sqrt{\alpha} \varepsilon_{ij}
  \,\to\,
  w_{ij} = \theta_{ij} + \sigma^2_{ij} \varepsilon_{ij}
  \,, $$
with the appropriate change of variables in the Kullback-Leibler divergence.
($\alpha_{ij} = \tfrac{\sigma_{ij}^2}{\lvert \theta_{ij}\rvert^2}$).

To recap the derivation in \cite{kingma_variational_2015}, let $W$ be a random martrix
with independently distributed elements, each according to $\mathcal{N}(\theta_{ij}, \sigma^2_{ij})$.
Then the action of a linear layer on $x$ is described as 
$$
  y = W x + b = I W x + b
    = (I \otimes x^\top) \vec{W} + b
  \,, $$
where $\vec{\cdot}$ denotes flattening in lexicographic order of indices (row-major),
for which we have the identity $\vec{A B C} = (A \otimes C^\top) \vec{B}$, \cite{cookbook2012}.
% It follows from the definition of the Kronecker product of $A$ and $C^\top$ and the
% row-major order vectorization.
Next, from $\vec{W}\sim \mathcal{N}_{[n\times m]}(\vec{\theta}, \mathop{diag}\Sigma)$
we get the following distribution for the output:
$$
  y \sim \mathcal{N}_{n}
    \bigl(
      \theta x + b,
      \sum_{i} e_i e_i^\top \sum_j \sigma^2_{ij} x_j^2
    \bigr)
  \,, $$
where the variance follows from the identity $(A\otimes B)(C\otimes D) = AC\otimes BD$,
\cite{cookbook2012}:
$$
  % (I \otimes x^\top) \mathop{diag}{\vec{\Sigma}} (I \otimes x^\top)^\top
  \ldots
    % = \sum_{ij} (I \otimes x^\top) e_i \otimes e_j
    %   \sigma^2_{ij} (e_i \otimes e_j)^\top (I \otimes x^\top)^\top
    = \sum_{ij} e_i (x^\top e_j) \sigma^2_{ij} (x^\top e_j) e_i^\top
    = \sum_{i} e_i e_i^\top \sum_j \sigma^2_{ij} x_j^2
  \,, $$
with $e_i$ being the $i$-th unit vector of dimensionality conforming to the expression
is is involved with.

Criticism of \cite{gale_state_2019} implies that $\ell_0$-variational dropout,
proposed by \cite{louizos_learning_2017} and based on the concrete binary distribution
of \cite{maddison_concrete_2016}, works consistently better than the variational
Gaussian dropout.

Maths and related results \cite{pav_moments_2015,taubock_complex-valued_2012},
and \cite{karseras_caution:_nodate}

% section complex_varaitional_dropout (end)

\section{Experiments} % (fold)
\label{sec:Experiments}

I this section we will compare the complex variational dropout techniques, discussed
above, on the tasks and datasets, that were studied in prior research on complex-%
valued networks.

\subsection{Tasks and datasets} % (fold)
\label{sub:tasks_and_datasets}

\cite{trabelsi_deep_2017} studies the applicability of complex-valued networks to
image classification (CIFAR-10, CIFAR-100, reduced train of SVHN), music-transcription
(MusicNet), and Speech Spectrum prediction (TIMIT).
\begin{itemize}
  \item 
\end{itemize}


\cite{monning_evaluation_2018}
\cite{jankowski_complex-valued_1996}
\cite{amin_complex-valued_nodate}
\cite{sarroff_complex_nodate}
\cite{lapidoth_capacity_2003}

% subsection tasks_and_datasets (end)

\subsection{Results and Discussion} % (fold)
\label{sub:results_and_discussion}

% subsection results_and_discussion (end)

% section Experiments (end)

\clearpage

\bibliographystyle{amsplain}
\bibliography{references}
\nocite{*}

\end{document}