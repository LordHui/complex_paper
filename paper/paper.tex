\documentclass[a4paper,10pt]{article}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{natbib}
% \setcitestyle{authoryear,open={((},close={))}}

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{grffile}

\usepackage{subcaption}

\usepackage{booktabs}

\title{Bayesian Sparsification of Deep $\cplx$-valued networks}
\author{Ivan Nazarov, and Evgeny Burnaev}

%% notation
\newcommand{\real}{\mathbb{R}}
\newcommand{\cplx}{\mathbb{C}}
\newcommand{\tr}[1]{\mathop{tr}{#1}}

\newcommand{\hop}{{\mkern-1.5mu\dagger}}
\newcommand{\conj}[1]{\overline{#1}}

% \renewcommand{\top}{{\mkern-1.5mu\intercal}}
\renewcommand{\vec}[1]{\overrightarrow{#1}}
\newcommand{\diag}[1]{\mathrm{diag}{#1}}

%% drafting macro
\newcommand{\important}[1]{\textbf{\!\colorbox{red}{#1}\!}}
\newcommand{\attn}[2]{\textbf{\color{red} #2~\textsuperscript{\textit{[#1]}}}}
\newcommand{\verify}[1]{\textit{\!\colorbox{red}{#1}\!}}
\newcommand{\rewrite}[1]{\attn{rewrite}{#1}}
\newcommand{\todo}[1]{{\color{blue} [TODO]} \important{#1}}

%% red-highlight missing citations
\usepackage{etoolbox}
\makeatletter
\patchcmd{\@citex}{\bfseries ?}{\colorbox{red}{\bfseries ?}}{}{}
\makeatother

\begin{document}
\maketitle

\begin{abstract}
With continual miniaturization ever more applications of deep learning can be found
in embedded systems, where it is common to encounter data with natural complex domain
representation. To this end we extend Variational Dropout to complex-valued neural
networks. We verify the proposed Bayesian technique and assess the performance-compression
trade-off by conducting a large-scale numerical study of $\cplx$-valued networks on
two tasks: image recognition on MNIST-like and CIFAR10 datasets and music transcription
on MusicNet. We reliably replicate the state-of-the-art result by \citet{trabelsi_deep_2017}
for $\cplx$-valued networks on MusicNet with $50-100\times$ compression rate.
\end{abstract}

\section{Introduction} % (fold)
\label{sec:introduction}

% general intro text with motivation
Deep neural networks are an integral part of machine learning and data science toolset
for practical data-driven problem solving. With continual miniaturization ever more
applications can be found in embedded systems. Common embedded applications include
on-device image recognition and signal processing. Despite recent advances in generalization
and optimization theory specific to deep networks, deploying in actual embedded hardware
remains a challenge due to storage, real-time throughput, and arithmetic (computational)
complexity restrictions \citep{he_amc:_2018}. Therefore compression methods for achieving
high model sparsity and numerical efficiency without losing much in performance are
especially relevant.
% Solving the storage and artihmetic constraints encourages development of model compression
% and sparsification methods, that focus on favourable trade-off between performance and size.

Inherently complex-valued nature of data in signal processing, specifically, acoustic or radio
frequency signal analysis, has been the key driver behind adoption of $\cplx$-networks.
%
\citet{hirose_complex-valued_2009} argues that the key merit of $\cplx$-valued neural
networks is simultaneous phase rotation and amplitude adjustment, intrinsic to multiplication
in $\cplx$ field. Despite inherently higher arithmetic complexity, which stems from up to four
floating point operations per single $\cplx$ multiplication \citep{monning_evaluation_2018}
$\cplx$-arithmetic reduces ineffective degrees of freedom in comparison with an analogous
double-dimensional $\real$-valued network. \citet{hirose_complex-valued_2009} list numerous
applications related to acoustic and radio signal processing and demonstrate superiority of
$\cplx$-networks in the task of shallow landmine detection using ground penetrating radar
imaging. More recently they were applied to music transcription and speech recognition
\citep{trabelsi_deep_2017}. \citet{arjovsky_unitary_2016} propose $\cplx$-valued unitary-RNNs
to deal with exploding~/~vanishing gradients and apply the recurrent models to long-term
dependence prediction tasks. \citet{wisdom_full-capacity_2016} expand on their work by optimizing
recurrent weights on the $\cplx^{n \times n}$ Stiefel manifold. And \citet{wolter_complex_2018}
investigate different $\cplx$-valued gating mechanisms unitary-RNNs and conduct experiments
on human motion prediction and music transcription. \citet{yang_complex_2019} develop complex
transformer, with $\cplx$-attention and complex encoder-decoder, and apply it to music
transcription and wireless signal classification.

% complex-valued distributions \citep{pav_moments_2015,taubock_complex-valued_2012},
% and \citep{karseras_caution:_2014}

% motivation and other methods
However, in spite of relative success of $\cplx$-valued networks, compression methods tailored
to $\cplx$-domain remain a niche field of research.
%
Computational efficiency improvement and model compression methods such as quantization
\citep{uhlich_differentiable_2019}, integer-based arithmetic \citep{lin_fixed_2016,chen_fxpnet_2017},
and knowledge distillation \citep{hinton_distilling_2015} appear to be directly applicable
to $\cplx$-valued networks.
%
Certain model sparsification methods, e.g. sparsity inducing regularizers, and magnitude
pruning \citep{zuo_compression_2019}, which reactivates zeroed parameters based on gradients,
can be adapted directly to $\cplx$-valued models. Other methods require additional considerations.
% the more explicitly zero parameters there is, the less computations is needed.

% on dropout
Dropout \citep{hinton_improving_2012} prevents overfitting by injecting multiplicative
binary noise into layer's weights, which breaks up co-adaptations that could occur
during training. \citet{wang_fast_2013} argue that the overall effect of binary dropout
on the intermediate outputs via the Central Limit Theorem can be approximated by a Gaussian
with weight-input dependent mean and variance. \citet{srivastava_dropout_2014} propose
using independent $\mathcal{N}(1,1)$ noise, arguing that higher entropy of a Gaussian has
better regularizing effect. In multitask learning \citet{cheung_superposition_2019}
have shown the possibility of storing task-specific parameters in non-destructive superposition
within a single network. Their argument implies that if the single task setting is viewed
as replicated multitask learning, then by sampling uncorrelated weight masks dropout acts
as a superposition method, which better utilizes the learning capacity of the network.
% many identical copies of the same task; \cite[Appendix A.1]{cheung_superposition_2019}
% (fast dropout gauss-approx) wang-manning.tex
% (gauss dropout) srivastava-et-al.tex

% segue into bayesian inference
Bayesian inference is a very rich field of research with a large body of research and
techniques that enable practical quantification of predictive, parameter, or model
uncertainty and its propagation into solutions of the problems of interest.
%
Dropout resembles averaging models with shared parameters: each model is weighted equally
since masks are randomly sampled, unlike Bayesian model averaging, which weighs according to
the posterior distribution. \citet{kingma_variational_2015} specifically consider dropout
as a Bayesian Inference technique: a principled framework of reasoning about uncertainty,
which updates prior beliefs in accordance with the likelihood of empirical observations
into posterior belief. Bayesian methods with suitable modelling and prior assumptions, such
as Variational Dropout, can be used towards model sparsification
\citep{kingma_variational_2015,molchanov_variational_2017,kharitonov_variational_2018}.

% (sparsity tech large-scale models) gale-elsen-hooker.tex
\citet{gale_state_2019} compare variational dropout \citep{kingma_variational_2015},
magnitude pruning, and $\ell_0$ regularization \citep{louizos_learning_2017} on large-scale
models. The latter is a probabilistic regularization method, which penalizes the expected
number of non-zero entries in $[0, 1]$-valued stochastic parameter masks with learnable
hard-concrete distribution with an atom at $0$ \citep{maddison_concrete_2016,jang_categorical_2017}.
Their results suggest that although variational dropout may achieve good accuracy-sparsity balance
and outperform pruning and $\ell_0$ on the studied architectures, pruning is preferable for
its simplicity and speed. They also observe that, despite dropout showing more variable
performance than magnitude pruning, it induces non-uniform sparsity throughout the model,
which has been shown by \citet{he_amc:_2018} to be essential for superior compression.
% Magnitude pruning induces user-specified sparsity distributions.
%
% L0 and dropout work in the low-to-mid sparsity range, variational dropout consistently
% ranked behind L0 on Transformer, and was bested by magnitude pruning for sparsity
% levels of 80\% and up. Variational dropout consistently produces models on-par or better
% than magnitude pruning on ResNet-50, and l0 fails to produce sparse models at all.
%

% our contribution
Inspired by variational and stochastic sparsification techniques we extend Bayesian dropout
for $\cplx$-valued networks. We assess the performance-compression trade-off of the proposed
Bayesian technique by conducting a large-scale numerical study of $\cplx$-valued networks
on two tasks: image recognition on MNIST-like and CIFAR10 datasets and music transcription
on MusicNet \citep{thickstun_learning_2017}. We reproduce the state-of-the-art result by
\citet{trabelsi_deep_2017} for $\cplx$-valued networks on the latter with $\approx 5\%$
of remaining non-zero parameters.

The paper is structured as follows. In sec.~\ref{sec:bayesian_dropout} we review Bayesian
dropout techniques, and in sec.~\ref{sec:c_valued_networks} we provide a brief summary of
the inner working of complex-valued networks as functions of complex argument. The main
contribution of this study is presented in sec.~\ref{sec:dropout_for_c_valued_layers},
where we consider different variational approximations and priors, outline the tricks
and derive expressions for the divergence term. In sec.~\ref{sec:experiments} we evaluate
the sparsification rate, compare the resulting performance of various $\cplx$-networks
proposed in prior work, and discuss the outcomes.

% section introduction (end)


\section{Bayesian Dropout} % (fold)
\label{sec:bayesian_dropout}

In general, the core of Bayesian Inference can be summarized as follows: given a prior
distribution $\pi(m)$ on models (hypotheses) $\mathcal{M}$, utilize the empirical evidence
$D$ to update the assumptions by considering the likelihood of the observations under each
$m \in \mathcal{M}$. Models are represented by a parametric family indexed by parameters
$\omega \in \Omega$ and each $m_\omega(\cdot)$ specifies the conditional distribution of
the data $
  D = (z_i)_{i=1}^N
$. The posterior distribution $p(\omega \mid D)$, derived using the Bayes rule $
  p(m \mid D) = \tfrac{p(D \mid m) \pi(m)}{p(D)}
$ with $
  p(D) = \mathbb{E}_{\pi(m)} p(D \mid m)
$, provides useful information about yet unobserved data and model parameters, e.g
classification uncertainty, predictive statistics, and parameter relevance.

The posterior distribution itself is intractable, save for the relatively simple cases, and
therefore exact inference is traded for tractability and scalability offered by Variational
Inference. Instead of deriving $p(\omega \mid D)$ using the rule, this approach recasts
posterior inference into variational optimization problem: finding $q$ that is close to
$p(\omega \mid D)$ in terms of a proximity score $\rho$
\begin{equation}  \label{eq:variational-progam}
  q_*
    \in \arg \min_{\theta} \rho\bigl(
      q_\theta(\omega), p(\omega \mid D)
    \bigr)
    \,,
\end{equation}
over a tractable parametric family of distributions on the parameter space $
  \mathcal{Q} = \{Q_\theta(d\omega) \colon \theta \in \Theta\}
$, $\theta$ -- generic variational parameter. The class $\mathcal{Q}$ is picked so that
its members have tractable densities $
  q_\theta(\omega) d\omega
$, can be sampled from and possess tractable $\log$-derivatives $
  \omega \mapsto \nabla_\theta \log q_\theta(\omega)
$ \citep{williams_simple_1992} or can be represented as differentiable transformations
of non-parametric noise \citep{kingma_auto-encoding_2014,figurnov_implicit_2019}.
% (reparameterization trick, pathwise gradieent) $q_{\theta}(d\omega)$ is a push-forward
% of $p(d\varepsilon)$ by a differentiable map $g(\varepsilon; \theta)$.
% % http://stillbreeze.github.io/REINFORCE-vs-Reparameterization-trick/#fn:1
%
The natural choice for $\rho$ is Kullback-Leibler divergence of density $q$ from $p$,
\begin{equation}  \label{eq:kl-div-def}
  KL(q \| p)
    % = \int \frac{dQ}{dP} \log{\frac{dQ}{dP}} P(d\omega)
    % = \int \frac{q(\omega)}{p(\omega)} \log{\frac{q(\omega)}{p(\omega)}} p(\omega) d\omega
    % = \mathbb{E}_{\omega \sim q}
    %   \log\frac{q(\omega)}{p(\omega)}
    = \mathbb{E}_{\omega \sim q}
      \log{q(\omega)}
    - \mathbb{E}_{\omega \sim q}
      \log{p(\omega)}
    \,,
\end{equation}
% and, despite it not being a metric and requiring distributions with nested support,
the main reason being that it satisfies the following identity for well-behaved densities:
% $(q, p, \pi)$
\begin{multline}  \label{eq:kl-div-master}
  % \log p(D)
    % = \mathbb{E}_{q(\omega)} \log \frac{p(D, \omega)}{p(\omega \mid D)}
    % = \mathbb{E}_{q(\omega)} \log \frac{
    %   p(D \mid \omega) \pi(\omega)
    % }{p(\omega \mid D)}
    % = \mathbb{E}_{q(\omega)} \log \frac{
    %   p(D \mid \omega) \pi(\omega)
    % }{q(\omega)}
    % + \mathbb{E}_{q(\omega)} \log \frac{q(\omega)}{p(\omega \mid D)}
    % - \mathbb{E}_{q(\omega)} \log \frac{q(\omega)}{\pi(\omega)}
    % = \mathbb{E}_{q(\omega)} \log{p(D \mid \omega)}
    % + \mathbb{E}_{q(\omega)} \log \frac{q(\omega)}{p(\omega \mid D)}
  \log p(D)
    - KL(q \| p(\omega \mid D))
    \\ = \mathbb{E}_{q(\omega)} \log{p(D \mid \omega)}
    - KL(q \| \pi)
    \,.
\end{multline}
This identity yields an equivalent problem for \eqref{eq:variational-progam}:
maximizing the \textit{evidence lower bound} (ELBO)
\begin{equation}  \label{eq:elbo_general}
  \mathcal{L}(\theta; \phi, \lambda)
    = - KL(q_{\theta} \| \pi_{\lambda})
      + \mathbb{E}_{\omega \sim q_{\theta}}
        \log p_{\phi}(D \mid \omega)
      % \mathbb{E}_{\omega \sim q_{\theta}}
        % \log\frac{q_\theta(\omega)}{\pi_{\lambda}(\omega)}
  % = \mathbb{E}_{\omega \sim q_{\theta}}
  %     \log{p_{\phi}(D \mid \omega) \pi_{\lambda}(\omega)}
  %   + \mathbb{H}(q_{\theta})
  \,,
\end{equation}
where the prior and likelihood are allowed to depend on explicit parameters. The most
commonly used algorithm to optimize \eqref{eq:elbo_general} is EM-algorithm. Other
variational objectives are possible, provided $p(\omega \mid D)$ is probed only through
$\log p(D \mid \omega)$ and $\log \pi(\omega)$ \citep{ranganath_operator_2018}.

\textit{Stochastic Gradient Variational Bayes} (SGVB), proposed by \citet{kingma_auto-encoding_2014},
is a differentiable unbiased Monte-Carlo estimator of \eqref{eq:elbo_general}, which
makes it possible to employ stochastic gradient-based optimization methods to solve
\eqref{eq:variational-progam}. If $
  \omega \sim q_{\theta}
$ is equivalent in distribution to $
  \omega = g(\varepsilon; \theta)
$ for some non-parametric random variable $
    \varepsilon \sim p_\varepsilon
$ and differentiable $
  g(\varepsilon; \theta)
$, then the SGVB is
\begin{equation}  \label{eq:sgvb}
  \widetilde{\mathcal{L}}(\theta; \phi, \lambda)
    = - KL(q_{\theta} \| \pi_{\lambda})
      % + \frac{N}{\lvert B \rvert} \sum_{i\in B}
        % \log p(z_i \mid \omega_i)b
      + \frac1{L} \sum_{l=1}^L
        \log p_{\phi}(D \mid g(\varepsilon_{l}; \theta))
        % \log p_{\phi}(D \mid \omega_{l})
        %   \Big\vert_{\omega_{l} = g(\varepsilon_{l}; \theta)}
      % =
      % - KL(q_{\theta} \| \pi)
      % + N \mathbb{E}_{B \sim \{D\}_M \otimes q_\theta^M}
        % \hat{\mathbb{E}}_{z,\omega \sim B}
          % \log p(z \mid \omega)
    \,, 
\end{equation}
where $
  (\varepsilon_{l})_{l=1}^L
$ is an iid sample from $p_\varepsilon$. Intractable KL-divergence terms can be replaced
by differentiable unbiased sampling estimators \citep{kingma_auto-encoding_2014}.
%
For the purposes of this study, we assume that the observed data is independent, and
use minibatch version of \eqref{eq:sgvb} with $L=1$:
\begin{equation}  \label{eq:elbo}
  \widetilde{\mathcal{L}}(\theta; \phi, \lambda)
    = - KL(q_{\theta} \| \pi_{\lambda})
      + \frac{N}{M} \sum_{i=1}^M
        \log p_{\phi}(z_i \mid g(\varepsilon_i; \theta))
    \,,
\end{equation}
for a random subsample $(z_i)_{i=1}^M$ from $D$ and iid $\varepsilon_i \sim p_\varepsilon$.
%
% (DSVI) titsias-lazaro.tex \todo{DSVI} \citep{titsias_doubly_2014}

\subsection{Dropout} % (fold)
\label{sub:dropout}

With a special family of posterior approximation, variational inference can be used as
a regularization method and as a model sparsification technique. \citet{kingma_variational_2015}
consider Dropout \citep{hinton_improving_2012}, DropConnect \citep{wan_regularization_2013},
and Gaussian dropout \citep{srivastava_dropout_2014,wang_fast_2013} through the lens
of Bayesian inference methods and proposes \textit{Variational Dropout}. They argue that
the multiplicative noise introduced by these methods induces a distribution equivalent to
a fully factorized variational posterior of the form $
  q_\theta(\omega) = \prod_j q_{\theta}(\omega_j)
$, where $q_{\theta}(\omega_j)$ is $\omega_j = \mu_j \xi_j$ with $
  \xi_j \sim p_\theta(\xi_j)
$ iid from some $p_\theta(\xi)$.
%
% Binary dropout with rate $p \in (0, 1)$ uses $
%   p_\xi
%     = \mathcal{B}\bigl(
%       \{0, \tfrac1{1-p}\}, 1-p
%     \bigr)
% $, whereas in Gaussian dropout $
%   \xi \sim \mathcal{N}(1, \alpha)
% $ for $\alpha = \tfrac{p}{1-p}$.
% This suggests making $\alpha$ a variational parameter
% and optimizing it in \eqref{eq:elbo_general} with an appropriate penalty term. Thus,
%
Variational Dropout assumes fully factorized Gaussian approximate posterior $
  q_\theta(\omega)
    = \prod_j \mathcal{N}(\mu_j, \alpha_j \mu_j^2)
$ and factorized log-uniform prior $\pi(\omega)$ with $
  \pi(\omega_j) \propto \lvert \omega_j \rvert^{-1}
$. The divergence term in \eqref{eq:elbo} unravels into a sum of the following
analytically intractable terms:
\begin{equation}  \label{eq:improper-kl-div-real}
  KL(q_\theta \| \pi)
    \propto
      \frac12 \mathbb{E}_{\varepsilon \sim \mathcal{N}(0, 1)}
        \log{\Bigl\lvert \tfrac1{\sqrt{\alpha}} + \varepsilon \Bigr\rvert^2}
      % \frac12 \mathbb{E}_{\varepsilon \sim \mathcal{N}(0, 1)}
      %   \log{\bigl\lvert 1 + \sqrt{\alpha} \varepsilon \bigr\rvert^2}
      % - \frac12 \log{\alpha}
  \,.
\end{equation}
\citet{kingma_variational_2015} approximate this expression by a non-linear polynomial
regression for truncated $\alpha \in (0, 1)$. \citet{molchanov_variational_2017}
refine the approximation by using a nonlinear regression based on sigmoid and soft-plus.
% see p.6 sec 3.4 of kingma_variational_2015
We independently verify their approximation against Monte-Carlo estimates for $\alpha$
varying over a fine logarithmic grid and the exact expression for the derivative of
\eqref{eq:improper-kl-div-real}, which we derive in appendix~\ref{sub:real-chisq-grad}.
% (!) good for SGVB: gradient-based methods require unbiased gradient estimates
%
% (in) it is in poor taste to brag about this begin the first time anyone has derived a
% derivative of an intractable function.

Being the ratio of the squared mean to the effective variance, $\alpha_j$ regulates the
relevance of the parameter $\omega_j$ it is associated to. Thus \citet{molchanov_variational_2017}
focus on the model sparsification capabilities of Variational Dropout, optimizing $\alpha$
for each individual parameter. Flexibility in specification of $q_\theta$ enables structured
dropout with shared $\alpha$ across groups of parameters $\omega$. Another Bayesian dropout
method, called \textit{Automatic Relevance Determination}, is developed in \citep{kharitonov_variational_2018}.
The key difference is that the log-uniform factor $\pi(\omega_{ij})$ is replaced by a proper
Gaussian prior $
  \mathcal{N}(0, \tau^{-1}_{ij})
$ with learnable precision $\tau_{ij} > 0$. This method, known as \textit{Empirical Bayes},
fits a prior distribution while performing variational inference. Maximizing \eqref{eq:elbo}
over $\tau$, holding other parameters fixed, yields $
  \tau^* = {(\mu^2 + \sigma^2)}^{-1}
$, whence
\begin{equation}  \label{eq:ard-kl-div-real}
  KL(q_\theta \| \pi)
    = \log{\bigl(1 + \tfrac1{\alpha} \bigr)}
    \,.
\end{equation}

Another contribution of \citep{kingma_variational_2015} is the analysis of the effects of the
\textit{local reparameterization trick} on the variance of the gradient of \eqref{eq:elbo}.
This trick, proposed in \citep{wang_fast_2013} to speed up Gaussian dropout, utilizes the
closure of Gaussians under affine transformations, thereby translating uncertainty from the
global parameter noise to local noise in intermediate outputs within the network.
%
The stochastic output of a linear layer $
  y = W^\top x + b
$ with $
  W \in \mathbb{R}^{n\times m}
$ and independent $
  W_{ij} \sim \mathcal{N}(\mu_{ij}, \alpha_{ij} \mu_{ij}^2)
$ is equivalent in distribution to
\begin{equation}  \label{eq:r-gauss-trick}
    y \sim \prod_i \mathcal{N}\Bigl(
          \sum_j \mu_{ij} x_j + b_i,
          \sum_j \alpha_{ij} \mu_{ij}^2 x_j^2
      \Bigr)
    \,.
\end{equation}
%
\citet{kingma_variational_2015} show, that for the case of fully factorized Gaussian
posterior approximations, the trick decorrelates ELBO estimates within the minibatch,
making the gradient estimator $\nabla_\theta \tilde{\mathcal{L}}$ more statistically
efficient. \citet{molchanov_variational_2017} propose an improvement by introducing
\textit{additive noise parameterization}. They argue for rejecting the $(\mu, \alpha)$
parameterization in favour of the equivalent $(\mu, \sigma^2)$, since the latter renders $
  \nabla_\mu \tilde{\mathcal{L}}
$ independent from the local output noise, injected by the trick \eqref{eq:r-gauss-trick},
and further stabilizes gradients in SGVB \eqref{eq:elbo}. The relevance $\alpha$ is
calculated using the ratio $
  \tfrac{\sigma^2}{\mu^2}
$ when needed in the divergence term.

% subsection dropout (end)

% section bayesian_dropout (end)


\section{$\cplx$-valued networks} % (fold)
\label{sec:c_valued_networks}

In this section we briefly review $\cplx$-valued networks, their distinction from the
ordinary $\real$-valued networks.

\citet{trabelsi_deep_2017} outline the building blocks for deep $\cplx$-valued networks
and describe suitable representation and operations including convolutional and dense layers,
$\cplx$-valued activations, complex batch-normalization and suitable weight initialization.
Their approach makes a complex-valued network into an intricately connected $\real$-valued
computational graph that respects $\cplx$-arithmetic.
%
In particular, natural identification of $\cplx$ and $\real^2$ implies that linear layers $
  L \colon \cplx^m \to \cplx^n
$ act upon their inputs as follows:
\begin{equation}  \label{eq:cplx-lin-op}
  {(\real^n)}^{2}
    \to {(\real^m)}^{2}
    \colon (u, v)
      \mapsto \Bigl(
        P u - Q v,
        P v + Q u
      \Bigr)
    \,,
\end{equation}
where $
  P, Q \colon \real^{n} \to \real^{m}
$ are unique operators such that $L = P + j Q$. A $\cplx$-convolutional layer with kernel
$W$ can be implemented as two $\real$-convolutions with kernels $\Re{W}$ and $\Im{W}$ using
\eqref{eq:cplx-lin-op}. Activations include trigonometric functions and representation-dependent
non-linearities \citep{hirose_complex-valued_2009}: polar (phase-amplitude) $
  z \mapsto \sigma(\lvert z \rvert) e^{j \arg z}
$ or planar (real-imaginary) $
  z \mapsto \sigma(\Re z) + j \sigma(\Im z)
$ for some $\real$-valued non-linearity $\sigma$, possibly with learnable parameters
\citep{trabelsi_deep_2017,wolter_complex_2018}.

Complex networks are, in general, non-holomorphic, i.e. not $\cplx$-differentiable, which is
exacerbated by the loss being real-valued. Indeed a $\cplx$-differentiable real-valued function is
necessarily trivial by Cauchy-Riemann conditions. This issue is dealt with by employing Wirtinger,
or $\cplx\real$ calculus, which generalizes holomorphic calculus to non-holomorphic functions
of $\cplx$-argument \citep{adali_complex-valued_2011,boeddeker_computation_2019}. It satisfies
product and chain rules, respects complex conjugation and linearity, but most importantly the
$\cplx\real$-differential of a function on $\cplx$ coincides with its classical differential as
a function on $\real^2$. Essentially, it allows straightforward retrofiting of $\cplx$-valued
networks into existing $\real$ deep learning auto-differentiation frameworks. It was considered
as a basis for $\cplx$ version of back-propagation by \citet{benvenuto_complex_1992} and
\citet{trabelsi_deep_2017}, (see appendix~\ref{sub:wirtinger_calculus} for details).
% \todo{read on holomorphic nets}

% section c_valued_networks (end)


\section{Dropout for $\cplx$-valued layers} % (fold)
\label{sec:dropout_for_c_valued_layers}

In this section we review the complex Gaussian distribution and discuss its properties. Then
we introduce factorized dropout posterior for complex-valued parameters and the relevant
version of the local reparameterization trick. Then we move on to choosing suitable priors
and then onward to deriving explicit expressions for the divergence penalties in \eqref{eq:elbo}
in terms of special functions.

\subsection{Complex Gaussians and Local Reparameterization} % (fold)
\label{sub:c_gauss_and_local_rep}

%notation
In the following $M^{\top}$ is the matrix transpose, $\conj{M}$ is elementwise complex
conjugation, and $M^{\hop} = (\conj{M})^\top$ denotes Hermitian conjugate.

A vector $z\in \cplx^m$ has complex Gaussian distribution, $
  q(z) = \mathcal{N}^{\cplx}_m(\mu, \Gamma, C)
$ with mean $\mu \in \cplx^m$ and $\cplx^{m\times m}$ covariance and relation matrices
$\Gamma$ and $C$, respectively, if
\begin{equation}  \label{eq:cn-paired-real-density}
  \begin{pmatrix}
    \Re z \\ \Im z
  \end{pmatrix}
    \sim \mathcal{N}_{2 m}\biggl(
      \Bigl(
        \begin{smallmatrix}
          \Re \mu \\ \Im \mu
        \end{smallmatrix}
      \Bigr),
      \tfrac12 \Bigl(
        \begin{smallmatrix}
          \Re{(\Gamma + C)} & \Im{(C - \Gamma)} \\
          \Im{(\Gamma + C)} & \Re{(\Gamma - C)}
        \end{smallmatrix}
      \Bigr)
    \biggr)
    \,,
\end{equation}
provided $\Gamma \succeq 0$, $\Gamma^{\hop} = \Gamma$, $C^\top = C$, and $
  \conj{\Gamma} \succeq C^{\hop} \Gamma^{-1} C
$. Matrices $\Gamma$ and $C$ are $
  \mathbb{E} (z - \mu)(z - \mu)^{\hop}
$ and $
  \mathbb{E} (z - \mu)(z - \mu)^{\top}
$, respectively.
%
% $\cplx$-Gaussians are closed under affine transformations: if $y = A z + b$
% for $A \in \cplx^{n \times m}$ and $b \in \cplx^{n}$, then $
%   y \sim \mathcal{N}_n^{\cplx}\bigl(
%       A\mu + b, A \Gamma A^{\hop}, A C A^\top
%     \bigr)
% $.
%
$\cplx$-Gaussian random variables are closed under affine transformations: for $
  A \in \cplx^{n \times m}
$ and $b \in \cplx^{n}$
\begin{equation}  \label{eq:cn-affine}
  A z + b \sim \mathcal{N}_n^{\cplx}\bigl(
      A\mu + b, A \Gamma A^{\hop}, A C A^\top
    \bigr)
  \,.
\end{equation}
%
The entropy of a $\cplx$-Gaussian in terms of $\Gamma$ and $C$ is
\begin{align}  \label{eq:c-gauss-entropy-derivation}
  \mathbb{H}(q)
    & = - \mathbb{E}_{z \sim q} \log{q(z)}
    \notag \\
    % &
    % entropy of \mathcal{N}_n(\mu, \Sigma)
    %   % = \tfrac12 \log \det{(2 \pi \Sigma)}
    %   % + \tfrac12 \mathop{tr}{(
    %   %     \Sigma^{-1}
    %   %     \mathbb{E}_{x\sim q(x)}
    %   %       (x - \mu) (x - \mu)^\top
    %   %   )}
    %   = \tfrac12 \log \det{(2 \pi e \Sigma)}
    % = \tfrac12 \log \det{\biggl(
    %   2 \pi e \tfrac12
    %   \begin{pmatrix}
    %     \Re{(\Gamma + C)} & - \Im{(\Gamma - C)} \\
    %     \Im{(\Gamma + C)} &   \Re{(\Gamma - C)}
    %   \end{pmatrix}
    % \biggr)}
    % \notag \\
    % &
    % = \begin{bmatrix}
    %   r_1 + j r_2 \to r_1 \\  % sufficient for C=0 case
    %   j r_1 + r_2 \to r_2     % j conj of transformation for r1, det x 2
    % \end{bmatrix}
    % = \tfrac12 \log \det{\biggl(
    %   % \det{[I & j I \\ j I & I]} = \det{I} \det{I - jI I^{-1} jI} = \det{2 I}
    %   % left apply : thus dividing by \sqrt{2}
    %   \tfrac1{\sqrt{2}} 2 \pi e \tfrac12
    %   \begin{pmatrix}
    %     \Gamma + C & j (\Gamma - C) \\
    %     j (\conj{\Gamma} + \conj{C}) & \conj{\Gamma} - \conj{C}
    %   \end{pmatrix}
    % \biggr)}
      % % Implicitly using the fact that for a non-singular $\Omega$ we have $
      % %   \det{A}
      % %     = \tfrac{\det{(\Omega A)}}{\det{\Omega}}
      % %     = \tfrac{\det{(A \Omega)}}{\det{\Omega}}
      % % $.
    % \notag \\
    % &
    % = \begin{bmatrix}
    %   c_1 - j c_2 \to c_1 \\  % -j conj of transformation for c2
    %   c_2 - j c_1 \to c_2     % sufficient for C=0 case
    % \end{bmatrix}
    % = \tfrac12 \log \det{\biggl(
    %   % \det{[I & - j I \\ - j I & I]} = \det{I} \det{I - (-j)I I^{-1} (-j)I} = \det{2 I}
    %   % right apply : thus dividing by \sqrt{2}
    %   \tfrac12 2 \pi e \tfrac12
    %   \begin{pmatrix}
    %     2 \Gamma     & - 2 j C \\
    %     2 j \conj{C} & 2 \conj{\Gamma}
    %   \end{pmatrix}
    % \biggr)}
    % \notag \\
    % &
    % = \begin{bmatrix}
    %   r_2 - j \conj{C} \Gamma^{-1} r_1 \to r_2
    % \end{bmatrix}
    % = \tfrac12 \log \det{\biggl(
    %   % \det{[I & 0 \\ - j \conj{C} \Gamma^{-1} & I]} = \det{I} = 1
    %   \pi e
    %   \begin{pmatrix}
    %     \Gamma & - j C \\
    %     0 & \conj{\Gamma} - \conj{C} \Gamma^{-1} C
    %   \end{pmatrix}
    % \biggr)}
    % \notag \\
    &
    = \tfrac12 \log \det{(\pi e \Gamma)}
      \det{(\pi e (\conj{\Gamma} - \conj{C} \Gamma^{-1} C))}
    \,.
\end{align}
A $\cplx$-Gaussian vector $z$ is \textit{proper} if $z$ and $\conj{z}$ are uncorrelated, i.e.
$C = 0$. From \eqref{eq:c-gauss-entropy-derivation} its entropy is
\begin{equation}  \label{eq:cn-proper-entropy}
  % determinant commutes with complex conjugation, since it is multilinear
  \mathbb{H}(q)
    % = \tfrac12 \log \det{(\pi e \Gamma)} \det{(\pi e (\conj{\Gamma} - \conj{C} \Gamma^{-1} C))}
    = \tfrac12 \log \det{(\pi e \Gamma)} \det{(\pi e \conj{\Gamma})}
    % = \tfrac12 \log \det{(\pi e \Gamma)} \conj{\det{(\pi e \Gamma)}}
    % = \tfrac12 \log \bigl\lvert \det{(\pi e \Gamma)} \bigr\rvert^2
    = \log \bigl\lvert \det{(\pi e \Gamma)} \bigr\rvert
    \,.
    \tag{\ref{eq:c-gauss-entropy-derivation}'}
\end{equation}
%
% The roles of $\Gamma$ and $C$ are more evident in univariate case: here $
%   \Gamma = \sigma^2 \geq 0
% $, and $C = \sigma^2 \rho \in \cplx$ with $
%   \lvert \rho \rvert \leq 1
% $. $\sigma^2$ determines the ``dispersion'', whereas $\rho$ is determines the
% ``strength'' and ``direction'' of the linear relation between real and imaginary
% parts \citep{lapidoth_capacity_2003}.
% $$
% \frac{\sigma^2}2
%   \begin{pmatrix}
%     1 + \Re{\rho} & \Im{\rho} \\
%     \Im{\rho} & 1 - \Re{\rho}
%   \end{pmatrix}
%   = \frac{\sigma^2}2 
%       \begin{pmatrix}
%         1 + \lvert \rho \rvert \cos\theta
%           & \lvert \rho \rvert \sin\theta \\
%         \lvert \rho \rvert \sin\theta
%           & 1 - \lvert \rho \rvert \cos\theta
%       \end{pmatrix}
%   \,. $$

The local reparameterization trick provides both faster computations, and better statistical
efficiency of the SGVB gradient estimator \citep{wang_fast_2013,kingma_variational_2015}.
Suppose that weights $W \in \cplx^{n\times m}$ of a linear layer are drawn from a mean
field $\cplx$-Gaussian approximation
\begin{equation}  \label{eq:c-gauss-vi-general}
  W_{ij}
    \sim \mathcal{N}^{\cplx} \bigl(
      \mu_{ij}, \Sigma_{ij}, C_{ij}
    \bigr)
  \,,
\end{equation}
where mean, \textit{variance} and \textit{relation} are given by matrices $
  \mu, \Sigma, C \in \cplx^{n\times m}
$ with $\Sigma_{ij} \in \real$, and $
  \lvert C_{ij} \rvert^2 \leq \Sigma_{ij}
$. The property \eqref{eq:cn-affine} and independence of elements of $W$ imply that for any
$x \in \cplx^m$ and $b \in \cplx^n$ the components of $y = W x + b$ are independent with
distributions
\begin{equation}  \label{eq:cplx-gauss-trick}
  y_i
    \sim \mathcal{N}^{\cplx}
      \Bigl(
        % e_i^\top \mu x + b_i,
        b_i + \sum_{j=1}^m \mu_{ij} x_j,
        \, \sum_{j=1}^m \Sigma_{ij} \lvert x_j \rvert^2,
        \, \sum_{j=1}^m C_{ij} x_j^2
      \Bigr)
    \,.
\end{equation}
The trick requires three matrix operations: $\mu x + b$, $\Sigma \lvert x \rvert^2$ and
$C x^2$ with the modulus and power applied elementwise.

The trick and dropout can be applied to any layer, the output of which depends linearly on
its parameters, e.g. convolutional layers, affine, and bilinear transformations $
  (x, z) \mapsto x^\top W^{(j)} z + b_j
$. Similar to $\real$ case, variational dropout for $\cplx$ convolutions draws independent
realizations of $W$ for each spatial patch in the input. The rationale is simpler computations
and variance reduction. The latter effect stems from decorrelated gradient estimates from
overlapping patches \citep{kingma_variational_2015}, which allows \eqref{eq:cplx-gauss-trick}
to use $\cplx$ convolutions of elementwise complex squares $x^2$ and amplitudes $\lvert x \rvert^2$
with the relation and variance kernels.
% Weight sharing is essentially pushed to shared variational parameters $\theta$ in the approximation $q_\theta$.
% A convolution is a matrix product of $W$, unrolled into a toeplitz matrix, and im-to-col
% flattened intput $x$

% subsection c_gauss_and_local_rep (end)

\subsection{The choice of priors and Kullback-Leibler divergence} % (fold)
\label{sub:priors_and_kullback_leibler_divergence}

For $\cplx$-variational dropout we propose using mean-field proper $\cplx$-Gaussian approximate
posterior, e.g. parameters $W \in \cplx^{n\times m}$ of a dense linear layer follow
\begin{equation}  \label{eq:c-gauss-vi}
  % (we omit the subscript $m=1$ for brevity)
  W \sim q(W)
    % = \prod_{ij} q(W_{ij})
    = \prod_{ij} \mathcal{N}^{\cplx}(
      W_{ij} \mid
        \mu_{ij},
        \alpha_{ij} \lvert \mu_{ij} \rvert^2,
        % \sigma^2_{ij},
        0
    )
  \,,
\end{equation}
and we use similar factorized approximation for convolutions and other, effectively, linear
layers. For this approximation the relation coefficients \eqref{eq:cplx-gauss-trick} are
zero. We use MAP estimates of bias terms, i.e. assume an independent fully factorized Dirac
approximation.

We consider two priors for Bayesian dropout: \verify{log-uniform} and $\cplx$-Gaussian empirical
Bayes \citep{kingma_variational_2015,molchanov_variational_2017,kharitonov_variational_2018}.
For a fully factorized approximation $q(\omega)$ and factorized prior belief $\pi(\omega)$,
the divergence term in ELBO \eqref{eq:elbo} is
\begin{equation}  \label{eq:elbo-general-kl-div}
  KL(q \| \pi)
    % = \mathbb{E}_{\oemga \sim q(\oemga)} \log \tfrac{q(\oemga)}{\pi(\oemga)}
    % = \sum_{ij} \mathbb{E}_{\omega_{ij} \sim q(\omega_{ij})}
    %   \log \tfrac{q(\omega_{ij})}{\pi(\omega_{ij})}
    % = \sum_{ij} KL\bigl( q(\omega_{ij}) \| \pi(\omega_{ij}) \bigr)
    = - \sum_{ij}
        \mathbb{H}(q(\omega_{ij}))
        + \mathbb{E}_{q(\omega_{ij})} \log{\pi(\omega_{ij})}
    \,.
\end{equation}
We omit subscripts ${ij}$ for brevity in the next sections.

\subsubsection{Variational dropout} % (fold)
\label{ssub:variational_dropout}

From \eqref{eq:cn-proper-entropy} the KL-divergence for \verify{log-uniform prior} $
  \pi(\omega) \propto {\lvert \omega \rvert}^{-\beta}
$ with $\beta \geq 1$ is
\begin{equation}  \label{eq:c-vd-kl-div-raw}
  KL(q\| \pi)
    \propto
    %   \mathbb{E}_{\omega \sim q(\omega)} \log q(\omega)
    %   + \tfrac{\beta}2 \mathbb{E}_{\omega \sim q(\omega)} \log \lvert \omega \rvert^2
    % =
    %   - \log \lvert \det{(\pi e \sigma^2)} \rvert
    %   + \tfrac{\beta}2 \mathbb{E}_{\omega \sim q(\omega)} \log \lvert \omega \rvert^2
    % =
      - \log{\sigma^2}
      + \tfrac{\beta}2 \mathbb{E}_{\omega \sim q(\omega)} \log \lvert \omega \rvert^2
    \,.
\end{equation}
Property \eqref{eq:cn-affine} implies that $
  \mathcal{N}^{\cplx}(\mu, \sigma^2, 0)
  \sim \mu \cdot \mathcal{N}^{\cplx}(1, \alpha, 0)
$ for $\mu \neq 0$ and $
  \sigma^2 = \alpha \lvert \mu \rvert^2
$, whence
\begin{equation}  \label{eq:expect-improper-term-cplx}
  \mathbb{E}_{\omega \sim q(\omega)} \log \lvert \omega \rvert^2
    % = \mathbb{E}_{\xi \sim \mathcal{N}^{\cplx}(\xi \vert 1, \alpha, 0)}
    %   \log \lvert \mu \xi \rvert^2
    % = \log \lvert \mu \rvert^2
    %   + \mathbb{E}_{\xi \sim \mathcal{N}^{\cplx}(\xi \vert 1, \alpha, 0)}
    %       \log \lvert \xi \rvert^2
    = \log \alpha \lvert \mu \rvert^2
      + \mathbb{E}_{\xi \sim \mathcal{N}^{\cplx}(\xi \vert 0, 1, 0)}
          \log{\bigl\lvert \tfrac1{\sqrt{\alpha}} + \xi \bigr\rvert^2}
    \,.
\end{equation}
Let $
  (z_i)_{i=1}^m \sim \mathcal{N}^{\cplx}(0, 1, 0)
$ iid and $\theta \in \cplx^m$. Then $
  \sum_i \lvert \theta_i + z_i \rvert^2
    \sim \chi^2_{2m}(s^2)
$ with $
  s^2 = \sum_i \lvert \theta_i \rvert^2
$, i.e. a non-central $\chi^2_{2m}$ with parameter $s^2$. Its log-moments for general
integer $m \geq1$ are given in \cite[p.~2466]{lapidoth_capacity_2003}. In particular,
% in fact Appendix X, Lemma 10.1
for $m=1$ and $\theta\in \cplx$ we have
\begin{equation}  \label{eq:log-moment-for-chi-2}
  \mathbb{E}_{z \sim \mathcal{N}^{\cplx}(0, 1, 0)}
    \log \lvert \theta + z \rvert^2
    = \log \lvert \theta \rvert^2 - \mathop{Ei}( - \lvert \theta \rvert^2)
    \,,
\end{equation}
where $
  \mathop{Ei}(x) = \int^x_{-\infty} t^{-1} e^t dt
$ for $x < 0$ is the Exponential Integral. Note that
$
  \tfrac{d}{dx} \mathop{Ei}(x) = \tfrac{e^x}{x}
$, $
  \mathop{Ei}(x) \leq \log{(-x)}
$, $
  % as x to 0 from below
  \mathop{Ei}(x) \approx \log{(-x)} - \gamma
$ as $x\to 0$ ($\gamma$ is Euler's constant) and $
  \mathop{Ei}(x) \geq -e^x
$ for $x \leq -1$.
% 1. by Leibniz rule $Ei(x)$ has -ve derivative on $x < 0$
% 2. $Ei(x) \leq 0$ on $x < 0$, since $t^{-1} e^t \leq t^{-1} < 0$ on $t < 0$
% 3. $Ei(x) \leq \log{(-x)}$ on $x < 0$, since on $x \in [-1, 0)$
% $$
%   \mathop{Ei}(x)
%     = \mathop{Ei}(-1) + \int_{-1}^x t^{-1} e^t dt
%     \leq \mathop{Ei}(-1) + \int_{-1}^x t^{-1} dt
%     = \mathop{Ei}(-1) + \log{(-x)}
%     \leq \log{(-x)}
%   \,. $$
% 3. by l'H{\^o}pital rule $Ei(x)$ is asymptotically $\log{-x}$ as $x \to 0-$
% $$
% % t = -e^{-x} ,  -\log{(-t)} = x
%   \lim_{x\to 0-} \frac{\mathop{Ei}(x)}{\log{(-x)}}
%     = \lim_{x\to 0-} \frac{x^{-1} e^x}{x^{-1}}
%     = 1
%   \,. $$
% 4. \citep{lapidoth_capacity_2003} yields $\lim_{x\to 0-} \log{(-x)} - \mathop{Ei}(x) = \gamma$
% 5. $Ei(x) \geq -e^x$ for any $x < -1$, since $t^{-1} e^t \geq - e^t$ for $t < -1$
% $$
% \mathop{Ei}(x)
%   = \int_{-\infty}^x t^{-1} e^t dt
%   \geq \int_{-\infty}^x -e^t dt = -e^x
%   \,. $$
% 6. $Ei(x) \leq \log{(-x)} - \gamma$ on $x < 0$ and $-e^x \leq Ei(x) \leq 0$ on $x < -1$.
%
Through \eqref{eq:expect-improper-term-cplx} and \eqref{eq:log-moment-for-chi-2} we get
\begin{equation}  \label{eq:c-vd-kl-div}
  KL(q\| \pi)
    \propto
    %   - \log \lvert \det{(\pi e \sigma^2)} \rvert
    %   + \tfrac{\beta}2 \log \alpha \lvert \mu \rvert^2
    %   + \tfrac{\beta}2 \mathbb{E}_{\xi \sim \mathcal{N}^{\cplx}(\xi \vert 0, 1, 0)}
    %       \log{\bigl\lvert \tfrac1{\sqrt{\alpha}} + \xi \bigr\rvert^2}
    % =
    %   - \log{\sigma^2}  % - \log{\pi e}
    %   + \tfrac{\beta}2 \log \alpha \lvert \mu \rvert^2
    %   + \tfrac{\beta}2 \bigl(
    %     - \log \alpha - \mathop{Ei}( -\tfrac1{\alpha})
    %   \bigr)
    % =
    %   - \log{\sigma^2}
    %   + \tfrac{\beta}2 \log \alpha \lvert \mu \rvert^2
    %   - \tfrac{\beta}2 \log \alpha
    %   - \tfrac{\beta}2 \mathop{Ei}( -\tfrac1{\alpha})
    % =
      % \log \tfrac{\lvert \mu \rvert^\beta}{\sigma^2}
      % \log \tfrac{\lvert \mu \rvert^\beta}{\alpha \lvert \mu \rvert^2}
      \tfrac{\beta-2}2 \log{\lvert \mu \rvert^2}
      + \log{\tfrac1{\alpha}}
      - \tfrac{\beta}2 \mathop{Ei}(- \tfrac1{\alpha})
      \tag{\ref{eq:c-vd-kl-div-raw}'} \,.
\end{equation}
We set $\beta = 2$ to make the divergence term depend only on $\alpha$ and add $\gamma$
so that the right-hand side is nonegative \citep[eq.(84)]{lapidoth_capacity_2003}.
%
Since $\mathop{Ei}(x)$ has simple analytic derivative and \eqref{eq:elbo} depends additively
on \eqref{eq:c-vd-kl-div}, it is possible to back-propagate through the divergence without
forward evaluation.
% Gradient-based optimiziation of \eqref{eq:elbo} usually does not use the value of the objective. 
% In stochastic gradient methods are concerned more with the gradient, induced by the objective
% function, rather than its value.

% subsubsection variational_dropout (end)

\subsubsection{Empirical Bayes} % (fold)
\label{ssub:empirical_bayes}

Let's consider fully factorized proper $\cplx$-Gaussian prior $
  \pi_\tau(\omega)
    \propto \mathcal{N}^{\cplx}\bigl(
      \omega \vert 0, \tau^{-1}, 0
    \bigr)
$. The per element divergence term in \eqref{eq:elbo-general-kl-div} is
\begin{equation}  \label{eq:emp-bayes-kl-div}
  KL(q \| \pi_\tau)
    = - 1 - \log{(\tau \sigma^2)}
      + \tau \bigl(
        \sigma^2 + \lvert \mu \rvert^2
      \bigr)
    \,.
\end{equation}
%
% This follows from the KL-divergence expression for two multivariate Gaussians
% $$
% % \mathbb{E}_{\omega\sim q_1(\omega)} \log \tfrac{q_1(\omega)}{q_2(\omega)}
% %   =
% KL(q_1\| q_2)
%   =
%   % - \tfrac12 \log\det{(2\pi e \Sigma_1)} + \tfrac12 \log\det{(2\pi \Sigma_2)}
%   - \tfrac12 \log \det{e I}
%   + \tfrac12 \log \frac{\det{\Sigma_2}}{\det{\Sigma_1}}
%   + \tfrac12 \tr{\bigl( \Sigma_2^{-1} \Sigma_1 \bigr)}
%   + \tfrac12 (\mu_1 - \mu_2)^\top \Sigma_2^{-1} (\mu_1 - \mu_2)
%   \,. $$
%
In Empirical Bayes the prior adapts to the observed data, i.e. \eqref{eq:elbo} is optimized
w.r.t. $\tau$ of each weight's prior. Since divergence sits in \eqref{eq:elbo} with negative
sign, optimal $
  \tau^\ast = (\sigma^2 + \lvert \mu \rvert^2)^{-1}
$ is found by minimizing \eqref{eq:emp-bayes-kl-div}, which gives
\begin{equation}  \label{eq:emp-bayes-opt-kl}
  KL(q \| \pi_{\tau^\ast})
    % = - 1 - \log{({\tau^\ast} \sigma^2)}
    %   + {\tau^\ast} \sigma^2 + {\tau^\ast} \lvert \mu \rvert^2
    % = \log{((\sigma^2 + \lvert \mu \rvert^2) \tfrac1{\sigma^2})}
    = \log{\bigl(1 + \tfrac{\lvert \mu \rvert^2}{\sigma^2}\bigr)}
    = \log{\bigl(1 + \tfrac1\alpha \bigr)}
    \,.
\end{equation}

% subsubsection empirical_bayes (end)

\subsubsection{Bayesian $\cplx$-dropout via $\real$-scaling} % (fold)
\label{ssub:real_scaling_dropout}

Consider the following parameterization of $W$: $
  W = \mu \odot \xi
$ where $
  \xi_{ij} \sim \mathcal{N}(1, \alpha_{ij})
$, yet $\mu \in \cplx^{n \times m}$. This case corresponds to inference regarding
relevance coefficients $\xi$ rather than the parameters themselves after. Each weight
under this parameterization is an independent degenerate $\cplx$-Gaussian distribution
\begin{equation}  \label{eq:real-scal-gauss-vi}
  W_{ij}
    \sim \mathcal{N}^{\cplx} \bigl(
      \mu_{ij},
      \, \alpha_{ij} \lvert \mu_{ij} \rvert^2,
      \, \alpha_{ij} \mu_{ij}^2
    \bigr)
  \,,
\end{equation}
which implies by \eqref{eq:cplx-gauss-trick} the following:
\begin{equation}  \label{eq:real-scal-gauss-trick}
  y_i
    \sim \mathcal{N}^{\cplx}\bigl(
      % e_i^\top \mu x + b,
      b_i + \sum_{j=1}^m \mu_{ij} x_j,
      \, \sum_j \alpha_{ij} \lvert x_{ij} \mu_{ij}\rvert^2,
      \, \sum_j \alpha_{ij} (x_{ij} \mu_{ij})^2
    \bigr)
    \,.
\end{equation}
The KL-divergence term for this approximation coincides with \eqref{eq:improper-kl-div-real},
which is approximated by \citet{molchanov_variational_2017}. The major drawback of this method
is inapplicability of the additive noise reparameterization in \eqref{eq:real-scal-gauss-trick},
which disentangles the gradient with respect to $\mu$ from the local noise.

% subsubsection real_scaling_dropout (end)

% subsection priors_and_kullback_leibler_divergence (end)

% section dropout_for_c_valued_layers (end)


\section{Experiments} % (fold)
\label{sec:experiments}

In this section we study the complex variational dropout techniques discussed above
on the tasks and datasets, that were studied in prior research on complex-valued networks.

% common experiment settings
Every experiment is replicated several times to take random effects into account, e.g.
random network initialization prior to training, randomly ordered batches during SGD.
Unless specified otherwise, every experiment uses global gradient clipping within an
$\ell_2$-norm ball of radius $0.5$, runs SGD with ADAM optimizer with the base learning
rate of $10^{-3}$. We note that since our experiments are on classification tasks,
$\cplx$-valued networks ultimately have to output a real-valued logit score. Following
\citet{wolter_complex_2018} and \citet{trabelsi_deep_2017}, the class scores are taken
as real parts of the $\cplx$ output.


\subsection{Stagewise fitting and sparsification} % (fold)
\label{sub:fitting_and_sparsification}

The goal of this section to study the compression-performance trade-off of the proposed Bayesian
sparsification methods for $\cplx$-valued networks. To this end we trained each network in
three successive stages in every experiment: ``dense'' $\to$ ``sparsify'' $\to$ ``fine-tune''.
Model parameters and the optimizer are copied between stages, except before the final stage
optimizer's internal state is reset.

At the \textit{``dense''} stage every network is fit ``as-is'', retaining its original
non-Bayesian architecture and using only the likelihood term from \eqref{eq:elbo} without
variational dropout.
%
During the \textit{``sparsify''} stage we make every layer Bayesian and apply $\cplx$-variational
dropout sec.~\ref{ssub:variational_dropout} (or sec.~\ref{ssub:empirical_bayes}) or its $\real$
counterpart. To be able to study the attainable sparsity levels we inject a coefficient $
  C \in (0, 1]
$ at the KL divergence term in \eqref{eq:elbo}:
\begin{equation}  \label{eq:elbo_with_coef}
    - \frac{C}N KL(q_{\theta} \| \pi_{\lambda})
    + \frac1{M} \sum_{i=1}^M
        \log p_{\phi}(z_i \mid g(\varepsilon_i; \theta))
    \,. \tag{\ref{eq:elbo}'}
\end{equation}
Higher $C$ yields increasingly more sparse layers upon termination of this stage.

The \textit{``fine-tune''} stage begins with computing the sparsity masks from the fitted
approximation $q_\theta(W)$ and proceeds to train the non-zero parameters similarly to
the ``dense'' stage, i.e. with non-Bayesian layers and fixed sparsity masks. 
%
The benefit of the ``fine-tune'' stage appears to be strongly influenced by the compression
rate. In this experiment and more prominently in MusicNet (sec.~\ref{sub:musicnet}), we
can observe that the more ``holes'' Bayesian dropout poked in the model, the more regularized
it becomes, the less capacity it maintains for overfitting and the more ``fine-tune'' serves
as extended training.

% on compresssion
Between \textit{``sparsify''} and \textit{``fine-tune''} stages we compute masks of non-zero
weights in each layer based on the relevance scores $\alpha$ (sec.~\ref{sub:dropout}). Since
the $q_\theta$ factorizes into univariate distributions, a $\cplx$ or $\real$ parameter is
considered non-zero if $\log \alpha \leq \tau$ for $
  \alpha = \tfrac{\sigma^2}{\lvert\mu\rvert^2}
$, or zero otherwise. Figure~\ref{fig:hist__and__threshold__tradeoff} (top) displays the
general shape of the distribution of $\log \alpha$ on the Fashion-MNIST dataset in
sec.~\ref{sub:mnist_like_datasets}. There always appears to be a mode in the moderately
negative range, a valley through zero between the peaks of the negative mode and the next
positive one. The relative mass of each mode changes with $C$ and the higher the coefficient
is, the more mass is shifted to the right.
%
The bottom graph on figure~\ref{fig:hist__and__threshold__tradeoff} is constructed
for $C \in \{\tfrac1{20}, \frac1{200}\}$ before running the ``fine-tune'' stage. From
\eqref{eq:elbo_with_coef}, the relative positions of the compression curves on this
plot and the results of experiments below, it can be concluded that $C$ has much more
substantial impact on the sparsity and performance, than the choice of $\tau$. Furthermore,
for $\tau > 0$ the performance quickly saturates while compression creeps downwards.

\begin{figure}[ht]
  \centering
  \begin{subfigure}[b]{0.95\textwidth}  % imcl2019-style swears at this
    \centering
    \includegraphics[width=\linewidth]{../assets/log-alpha__sum__fashion-mnist__20200131-205926.pdf}
  \end{subfigure} \\%
  % \hfill
  \begin{subfigure}[b]{0.95\textwidth}  % imcl2019-style swears at this
    \centering
    \includegraphics[width=\linewidth]{../assets/figure__musicnet__threshold.pdf}
  \end{subfigure}
  \caption{%
    (\textit{top}) Smoothed density of $\log\alpha$ for some convolutional and dense layer
    from a model in sec.~\ref{sub:mnist_like_datasets};
    (\textit{bottom}) Test split performance and compression rate for a $\cplx$-valued
    net on MusicNet.
  }
  \label{fig:hist__and__threshold__tradeoff}
\end{figure}

% on chosing the threshold
The threshold $\tau$ is picked so, that the remaining \textit{``non-zero''} parameters
are within $\delta$ relative tolerance of their mean with high probability under the
approximate posterior $q_\theta(W)$. This ensures that stochastic parameters are sufficiently
concentrated around $\mu$, which are used as stand-in values for network weights $W$,
when it runs in inference mode. Since $W$ is $\real$- or a proper $\cplx$-Gaussian, the
random variable $
  \tfrac{k \lvert W - \mu \rvert^2}
        {\alpha \lvert \mu \rvert^2}
$ is $\chi^2_k$ distributed with $k=1$ ($\real$) or $2$ ($\cplx$), whence
\begin{equation} \label{eq:prob_relevant}
  \Pr\bigl(
    \lvert w - \mu \rvert \leq \delta \lvert \mu \rvert
  \bigr)
    % = \Pr\biggl(
    %   \frac{k \lvert w - \mu \rvert^2}{\alpha \lvert \mu \rvert^2}
    %     \leq \frac{\delta^2 k}{\alpha}
    % \biggr)
    = \chi^2_k\biggl(\frac{\delta^2 k}{\alpha}\biggr)
  \,.
\end{equation}
For a generous $\delta = 50\%$ tolerance all values of $\log \alpha$ below $-2.5$ yield
at least $90\%$ chance of a non-zero $\real$/$\cplx$ parameter. We pick $\tau = -\tfrac12$
to exclude noisy parameters and encourage higher sparsity, at the same time being aware
that $q_\theta$ is merely an approximation. In comparison, $\tau = 3$ is commonly used as
the threshold \citep{molchanov_variational_2017,kingma_variational_2015}.

% subsection fitting_and_sparsification (end)


\subsection{Metrics and compression} % (fold)
\label{sub:metrics_and_compression}

% perfromance metrics
We use accuracy as the performance metric for multi-class image classification on MNIST-like
and CIFAR10 datasets. Similarly to \citet{trabelsi_deep_2017} performance in experiments with
MusicNet is measured with (pooled) average precision: predicted scores and target binary labels
for each output are pooled, and average precision is computed based on the precision-recall
curve as if the task was single-output. The evaluation is done after ``dense'' stage, before
the beginning and upon termination of the ``fine-tune'' stage.

% write a note on the incompressible parameters (bias, batch norm etc.)
The compression rate is computed based on the number of floating point values needed to
specify the model and equals $
  \tfrac{n_\mathtt{par}}{n_\mathtt{par} - n_\mathtt{zer}}
$, where $n_\mathtt{zer}$ is the number of explicit zeros after sparsification, $n_\mathtt{par}$
is the total number of floats. Thus each parameter is counted once in a $\real$-valued
network, but twice in a $\cplx$-network, being a pair of real-imaginary floats. Each model
has incompressible parameters, such as layers' biases, shift and scaling from $\real$-and
$\cplx$-batch normalization layers, if present.

% subsection metrics_and_compression (end)


\subsection{MNIST-like datasets} % (fold)
\label{sub:mnist_like_datasets}

We conduct a moderately sized experiment on MNIST and similar $28\times 28$ greyscale image
datasets to study the compression-performance trade-off of the proposed $\cplx$-variational
dropout on simple architectures.
% dataset size
We deliberately use a {fixed} random subset of $10k$ images from the train split of each
dataset to fit the networks, and gauge the final performance on the usual test split.
Limiting the size was done primarily to test the regularization strength of compression,
and to make bulk experimentation faster.

% model architectures
% discussion of TwoLayerDenseModel (TLDM) and SimpleConvModel (SCM)
The $\real$/$\cplx$ networks, studied in this experiment, have been chosen for the purpose 
of illustrating the compression and understanding the effects of experiment parameters.
\textit{SimpleConvModel} (SCM) is $2d$ ReLU convolutional networks with average pooling
($2\times 2$) after non-linearities with two convolutions (kernel $5\times 5$, stride $1$)
with filters $n_1 \to n_2$ followed by dense layers $n_3 \to n_\mathtt{out}$. The second
network, \textit{TwoLayerDenseModel} (TLDM), is a wide dense network $n_1 \to n_\mathtt{out}$.
For SCM model $n_1, n_2, n_3$ are $20, 50, 500$, respectively, and for TLDM model $
  n_1 = 4096
$ regardless of their type ($\real$ or $\cplx$), although \citet{monning_evaluation_2018}
argues that this renders models incomparable.

% image features
Since image data is not naturally $\cplx$-valued, we preprocess images by either embedding
$\real$-data into $\cplx$ assuming $\Re z = 0$ (\texttt{raw}), or applying the two-dimensional
Fourier Transform (\texttt{fft}), shifting lower frequencies to the centre of the image. We
do not investigate the option, proposed by \citet{trabelsi_deep_2017}, to train an auxiliary
model to synthesize the imaginary component from the real data.

% first experiment: 40-75-40 with fast scheduler
In this experiment stages last for $40$, $75$ and $40$ epochs, respectively, and the base
learning rate of ${10}^{-3}$ is reduced after the $10$-th epoch to ${10}^{-4}$.
%
Plots~\ref{fig:mnist-like__trade-off__kmnist} and \ref{fig:mnist-like__trade-off__emnist}
depict samples from the performance-compression curve on KMNIST and EMNIST-Letters datasets
for the studied models and VD compression method (sec.~\ref{ssub:variational_dropout}).
Besides that we tried \texttt{raw} or \texttt{fft} input features, varied $
  C \in \{\tfrac3{2^{k+1}}\colon k=1..16\}
$, and repeated the experiment $5$ times.

We make a couple of observations based on these results. First, Fourier features appear to
perform on par with raw colour channel data in the convolutional networks across most
compression rates, but require high compression to make wide shallow dense network work
as well. The upward trend in manifests itself in MNIST and Fashion MNIST experiments as well
(see appendix~\ref{sub:other_mnist_like_datasets}). Finally, overall $\cplx$ variational
dropout methods seems to compress network adequately well, without much loss in performance.

% a couple of plots from MNIST and Fashion-MNIST
\begin{figure}[!h]
  \centering
  \begin{subfigure}[b]{1.\textwidth}  % imcl2019-style swears at this
    \centering
    % \includegraphics[width=\linewidth]{../assets/figure__mnist-like__trade-off/legacy__VD__fashionmnist__fft__-0.5.pdf}
    \includegraphics[width=\linewidth]{../assets/figure__mnist-like__trade-off/legacy__VD__kmnist__fft__-0.5.pdf}
    % \includegraphics[width=\linewidth]{../assets/figure__mnist-like__trade-off/VD__kmnist__fft__-0.5.pdf}
  \end{subfigure} \\%
  % \hfill
  \begin{subfigure}[b]{1.\textwidth}  % imcl2019-style swears at this
    \centering
    % \includegraphics[width=\linewidth]{../assets/figure__mnist-like__trade-off/legacy__VD__fashionmnist__raw__-0.5.pdf}
    \includegraphics[width=\linewidth]{../assets/figure__mnist-like__trade-off/legacy__VD__kmnist__raw__-0.5.pdf}
    % \includegraphics[width=\linewidth]{../assets/figure__mnist-like__trade-off/VD__kmnist__raw__-0.5.pdf}
  \end{subfigure}
  \caption{%
    The compression-accuracy trade-off on KMNIST dataset.
    (\textit{top}) \texttt{fft} features, (\textit{bottom}) \texttt{raw} features.
  }
  \label{fig:mnist-like__trade-off__kmnist}
\end{figure}

\begin{figure}[!h]
  \centering
  \begin{subfigure}[b]{1.\textwidth}  % imcl2019-style swears at this
    \centering
    % \includegraphics[width=\linewidth]{../assets/figure__mnist-like__trade-off/legacy__VD__mnist__fft__-0.5.pdf}
    \includegraphics[width=\linewidth]{../assets/figure__mnist-like__trade-off/legacy__VD__emnist_letters__fft__-0.5.pdf}
    % \includegraphics[width=\linewidth]{../assets/figure__mnist-like__trade-off/ARD__kmnist__fft__-0.5.pdf}
  \end{subfigure} \\%
  % \hfill
  \begin{subfigure}[b]{1.\textwidth}  % imcl2019-style swears at this
    \centering
    % \includegraphics[width=\linewidth]{../assets/figure__mnist-like__trade-off/legacy__VD__mnist__raw__-0.5.pdf}
    \includegraphics[width=\linewidth]{../assets/figure__mnist-like__trade-off/legacy__VD__emnist_letters__raw__-0.5.pdf}
    % \includegraphics[width=\linewidth]{../assets/figure__mnist-like__trade-off/VD__emnist_letters__raw__-0.5.pdf}
  \end{subfigure}
  \caption{%
    The compression-accuracy trade-off on the letters part of the Extended MNIST dataset.
    (\textit{top}) \texttt{fft} features, (\textit{bottom}) \texttt{raw} features.
  }
  \label{fig:mnist-like__trade-off__emnist}
\end{figure}

% subsection mnist_like_datasets (end)


\subsection{CIFAR10} % (fold)
\label{sub:cifar10}

% mention the same experimental setup as in MNIST
Experiments on CIFAR10 dataset are set up similarly to MNIST-like experiment, except
that we study VGG16 model \citep{simonyan_very_2015} and its $\cplx$-counterpart and
replicate each experiment only twice. Each network is trained stagewise for $20$, $40$,
and $20$ epochs on the full training split of $50k$ colour $32\times32$ images. We use
only raw colour features (\texttt{raw}). We use fast learning rate scheduler and apply
random crop and horizontal flips as augmentation. The scheduler scales the base learning
rate after $5$, $10$, and $20$-th epoch by $\tfrac1{10}$, $\tfrac1{20}$ and $\tfrac1{100}$,
respectively.

% describe the VGG16 model and its cplx-valued modification
The $\cplx$-VGG16 model is architecturally identical to VGG16, except for all layers are
replaced by their $\cplx$-valued versions. To maintain comparability, we halve the filter
sizes of $\cplx$ model \citep{monning_evaluation_2018}.

% draw some conclusions
Figure~\ref{fig:figure__cifar10__trade-off} shows that although the prior used in ARD method
offers slightly less compression, it makes up for it by equally slightly better accuracy.
Furthermore, the results imply that that the halved $\cplx$ network visibly underperforms
in comparison with its $\real$ counterpart, although in MNIST-like experiment $\cplx$-valued
network with the same inner feature dimensions performed on close to $\real$-valued, which
is half the size of the former.

% compare compressed cplx-vgg against real-vgg
\begin{figure}[!h]
  \centering
  \begin{subfigure}[b]{0.9\textwidth}  % imcl2019-style swears at this
    \centering
    \includegraphics[width=\linewidth]{../assets/figure__cifar__trade-off/augmentedcifar10__raw__-0.5.pdf}
  \end{subfigure}
  \caption{%
    Compression-accuracy frontier for $\real$ and $\tfrac12 \cplx$ versions of VGG16 on CIFAR10.
  }
  \label{fig:figure__cifar10__trade-off}
\end{figure}

\begin{figure}[!h]
  \centering
  \begin{subfigure}[b]{1.\textwidth}  % imcl2019-style swears at this
    \centering
    \includegraphics[width=\linewidth]{../assets/figure__musicnet__trade-off/musicnetram__fft__-0.5.pdf}
  \end{subfigure}
  \caption{%
    Performance-compression trade-off using \textbf{\color{blue} VD} and \textbf{\color{orange} ARD}.
    The \textbf{\color{pink} version} of the network with kernel $3$ in the first convolution is
    compressed with VD only.
  }
  \label{fig:musicnet__trade-off}
\end{figure}

% subsection cifar10_100 (end)


\subsection{musicNet} % (fold)
\label{sub:musicnet}

% describe musicnet as in Trabelsi et al. (2017) also get some hints from thickstun_learning_2017
MusicNet \citep{thickstun_learning_2017} is an annotated audio dataset, consisting of $330$
compositions from the public domain. Folowing \citet{trabelsi_deep_2017}, we use this
dataset for automatic music transcription task and study the $1d$ VGG-like $\cplx$-convent
proposed therein. Similarly to this study, we downsample the audio from 44.1kHz to 11kHz,
retaining only $84$ out of $128$ labels, and hold-out the same validation and test splits.
During each training epoch we sample $1000$ mini-batches of the remaining $321$ compositions
thus: a random full window of $4096$ samples is picked within each waveform and transformed
from $\real$ to $\cplx$ with $1d$ FFT. Annotation labels are taken at the mid-point of the
window.

Preliminary experiments with the model without sparsification have shown that early stopping
almost always terminated the training process within the first $10-20$ epochs, and that the
validation performance peaked at about $10-15$ epochs, much sooner than $200$ epochs used by
\citet{trabelsi_deep_2017}. Thus we adopt shorter lasting stages: $12$, $32$ and $50$ epochs,
which essentially turns the ``dense'' stage into pretraining step. Early stopping, tracking
the validation split performance, is used only during the ``fine-tune'' stage. To keep the
learning rate schedule synchronised with such abbreviated training, we allow the base rate
of $10^{-3}$ to be scaled after $5$, $10$, and $20$-th epoch by $\tfrac1{10}$, $\tfrac1{20}$
and $\tfrac1{100}$, respectively. Other key deviations from the setup used by \citet{trabelsi_deep_2017}
include: shifting lower frequencies to the centre of the window to maintain locality after
applying FFT (sec.~\ref{sub:mnist_like_datasets}), and clipping $\ell_2$ norm of parameters'
gradients to $0.05$.

In each experiment we varied the method and the value $C$, while keeping $\tau$ at $-\tfrac12$.
Each full experiment is replicated five times to take random effects from network initialization
before ``dense'', stochastic batch sequence during SGD, and randomness of Bayesian forward
pass at ``sparsify'' stage.

Figure~\ref{fig:musicnet__trade-off} shows the resulting performance-compression frontier
for the deep network of \citet{trabelsi_deep_2017} and its version, in which we purposefully
halve the spatial size of the first $1d$ convolution kernel from $6$ to $3$ (denoted by
suffix \textit{k}$3$). The motivation is to test if the performance handicap introduced be
the forced compression of the most upstream layer can be alleviated through non-uniform
compression, induced by variational dropout. We test only VD (sec.~\ref{ssub:variational_dropout})
method in this sub-experiment, since prior results have not demonstrated significant difference
in performance-compression trade-off between VD and ARD (sec.~\ref{ssub:empirical_bayes}).

Shaded horizontal bands represent the min-max spread of the performance of the uncompressed
network upon termination of ``dense'' stage. Each point represents a precision-compression
value attained by a network after ``fine-tune'' stage. The end of the tail of each point
represents the performance gain or loss from the ``fine-tune'' stage itself.
%
For compression levels lower than $50$, the final stage overfits the network, since
undercompresed remain overparameterized. For rates higher than $\approx 100$ the ``fine-tune''
stage serves its purpose, essentially, due to the network's being sufficiently regularized
though sparsity. There is evidence in favour of achievability of the SOTA average precision
of \citet{trabelsi_deep_2017} within the $50$-$100$ compression region with $\cplx$-valued
automatic relevance derivative (sec.~\ref{ssub:empirical_bayes}).

% subsection musicnet (end)

% section experiments (end)


\section{Conclusion} % (fold)
\label{sec:conclusion}

Despite having higher arithmetic complexity, complex-valued models provide built-in
joint amplitude-phase transformations and regularization \citep{hirose_complex-valued_2009}
and enjoy superior performance on naturally $\cplx$-valued datasets \citep{tarver_design_2019},
that occur in ultra high-frequency radio communications and audio signal processing. In this
study we address the issue of excessive the arithmetic complexity of complex-valued networks
by extending the existing Bayesian compression techniques by \citet{kingma_variational_2015},
\citet{molchanov_variational_2017} and \citet{kharitonov_variational_2018} to $\cplx$-valued
model parameters. To this end we have conducted a large scale numerical study of such networks
to assess the performance-compression rate trade-off and achieved performance, comparable
to SOTA on the MusicNet dataset.

Much like $\real$-valued variational dropout, the proposed $\cplx$-valued extension can be
straightforwardly enhanced to provide structured sparsity. The results of this study have
direct implications for real-time signal processing with embedded deep learning applications
both in terms of lower storage requirements and higher throughput stemming from fewer
floating-point multiplications.

% section conclusion (end)

\clearpage

\bibliographystyle{abbrvnat}
\bibliography{references}
% \nocite{*}

\clearpage

\section{Appendix} % (fold)
\label{sec:appendix}

\subsection{Complex-valued local reparameterization} % (fold)
\label{sub:complex_valued_local_reparameterization}

%notation
By $e_i$ we denote the $i$-th real unit vector of dimensionality \textit{conforming} to the
matrix-vector expression it is used in. Let $\vec{M}$ denote \textit{row-major} flattening
of $M$ into a vector, i.e. in lexicographic order of its indices. $\diag{(\cdot)}$ embeds
$\cplx^n$ into $n\times n$ diagonal matrices, and $\otimes$ denotes the Kronecker product,
for which we note the following identities $
  \vec{P Q R} = (P \otimes R^\top) \vec{Q}
$, and $
  (P \otimes R) (C \otimes S) = P Q \otimes R S
$ \citep{petersen_matrix_2012}.
% It follows from the definition of the Kronecker product
% of $P$ and $R^\top$ and the row-major order vectorization.

If we assume a mean field $\cplx$-Gaussian approximate posterior for $
  W \in \cplx^{n\times m}
$, then
\begin{equation}  \label{eq:c-gauss-vi-general-vec}
  \vec{W}
    \sim \mathcal{N}^{\cplx}_{nm} \bigl(
      \vec{\mu}, \diag{\vec{\Sigma}}, \diag{\vec{C}}
    \bigr)
  \,,
\end{equation}
where mean, \textit{variance} and relation are given by matrices $
  \mu, \Sigma, C \in \cplx^{n\times m}
$ with $\Sigma_{ij} \geq 0$, and $
  \lvert C_{ij} \rvert^2 \leq \Sigma_{ij}
$. For any $x \in \cplx^m$ and $b\in \cplx^n$ $
  y = W x + b
    = (I_n \otimes x^\top) \vec{W} + b
$, whence the covariance and relation matrices of $y$ are
\begin{align}
  (I \otimes x^\top) \diag{\vec{\Sigma}} (I \otimes x^\top)^{\hop}
    % = \sum_{ij} (I \otimes x^\top) e_i \otimes e_j
    %   \Sigma_{ij} (e_i \otimes e_j)^\top (I \otimes x^\top)^{\hop}
    % = \sum_{ij} (e_i \otimes x^\top e_j)
    %   \Sigma_{ij} (e_i \otimes x^\top e_j)^{\hop}
    % = \sum_{ij} (e_i \otimes x_j) \Sigma_{ij} (e_i^\top \otimes \conj{x}_j)
    & = \sum_{i=1}^n (e_i e_i^\top) \sum_{j=1}^m \Sigma_{ij} x_j \conj{x}_j
    \,,  \label{eq:cn-gauss-lrt-cov} \\
  (I \otimes x^\top) \diag{\vec{C}} (I \otimes x^\top)^\top
    % = \sum_{ij} (e_i \otimes x_j) C_{ij} (e_i \otimes x_j)^\top
    & = \sum_{i=1}^n (e_i e_i^\top) \sum_{j=1}^m C_{ij} x_j^2
    \,.  \label{eq:cn-gauss-lrt-rel}
\end{align}
Since \eqref{eq:cn-gauss-lrt-cov} and \eqref{eq:cn-gauss-lrt-rel} are diagonal, $(y_i)_{i=1}^n$
are independent whence \eqref{eq:cn-affine} implies
\begin{equation}  \label{eq:cplx-gauss-trick-appendix}
  y_i
    \sim \mathcal{N}^{\cplx}
      \bigl(
        e_i^\top \mu x + b_i,
        \, \sum_{j=1}^m \Sigma_{ij} \lvert x_j \rvert^2,
        \, \sum_{j=1}^m C_{ij} x_j^2
      \bigr)
    \,.
\end{equation}
Hence \eqref{eq:cplx-gauss-trick} follows.

% subsection complex_valued_local_reparameterization (end)

\subsection{Gradient of the KL-divergence in $\real$ case} % (fold)
\label{sub:real-chisq-grad}  % gradient_of_the_kl_divergence_in_R_case

In this section we derive the exact gradient of \eqref{eq:improper-kl-div-real}. Even
though, ultimately, the divergence involves a hypergeometric function, and its gradient
requires evaluating the Gauss error function, the analysis provides independent verification
of the approximation, proposed by \citet{molchanov_variational_2017}. The logic of the
derivation follows \citet{lapidoth_capacity_2003}.
% \citet{pav_moments_2015} correctly notice that \cite[p. 2466]{lapidoth_capacity_2003} has a missing $\log{2}$ term.

For $(z_i)_{i=1}^m \sim \mathcal{N}(0, 1)$ iid and $
  (\mu_i)_{i=1}^m \in \mathbb{R}
$. The random variable $W = \sum_i (\mu_i + z_i)^2$ noncentral $\chi^2$ with shape $m$
and noncentrality parameter $\lambda = \sum_i \mu_i^2$, i.e. $W\sim \chi^2_m(\lambda)$.
Its density is
\begin{equation}  \label{eq:nonchisq-density}
  f_W(x)
    % = e^{- \tfrac\lambda2} \sum_{n \geq 0} \tfrac{\bigl(\tfrac\lambda2\bigr)^n}{n!}
    %   \tfrac{
    %     x^{n + \tfrac{m}2 - 1} e^{-\tfrac{x}2}
    %   }{
    %     2^{n + \tfrac{m}2} \Gamma(n + \tfrac{m}2)
    %   }
    % = e^{- \tfrac\lambda2} \sum_{n \geq 0} \tfrac{\bigl(\tfrac\lambda2\bigr)^n}{n!}
    %     \tfrac{
    %         \bigl(\tfrac{x}2\bigr)^{n + \tfrac{m}2 - 1} e^{-\tfrac{x}2}
    %     }{
    %         2 \Gamma(n + \tfrac{m}2)
    %     }
    = \frac12 e^{- \tfrac{x + \lambda}2} \bigl(\tfrac{x}2\bigr)^{\tfrac{m}2 - 1}
      \sum_{n \geq 0} \frac{
        \bigl(\tfrac{x \lambda}4\bigr)^n
      }{
        n! \Gamma(n + \tfrac{m}2)
      }
    % = \frac12 e^{- \tfrac{x + \lambda}2}
    % \bigl(\tfrac{x}\lambda\bigr)^{\tfrac{m}4 - \tfrac12}
    % \bigl(\tfrac{x \lambda}4\bigr)^{\tfrac{m}4 - \tfrac12}
    % \sum_{n \geq 0} \tfrac{
    %         \bigl(\tfrac{x \lambda}4\bigr)^n
    %     }{
    %         n! \Gamma(n + \tfrac{m}2)
    %     }
    % = \frac12 e^{- \tfrac{x + \lambda}2}
    % \bigl(\tfrac{x}\lambda\bigr)^{\tfrac{m}4 - \tfrac12}
    % \bigl(\tfrac{\sqrt{x \lambda}}2\bigr)^{\tfrac{m}2 - 1}
    % \sum_{n \geq 0} \tfrac{
    %         \bigl(\tfrac{x \lambda}4\bigr)^n
    %     }{
    %         n! \Gamma(n + \tfrac{m}2)
    %     }
    % = \frac12 e^{- \tfrac{x + \lambda}2}
    % \bigl(\tfrac{x}\lambda\bigr)^{\tfrac{m - 2}4}
    % I_{\bigl(\tfrac{m}2 - 1\bigr)}(\sqrt{x \lambda})
    \,.
\end{equation}
% where $I_k$ is the modified Bessel function of the first kind
% $$
% I_k(s)
%   = \Bigl(\frac{s}2\Bigr)^k
%       \sum_{n \geq 0} \tfrac{
%           \bigl( \tfrac{s}2 \bigr)^{2n}
%       }{n! \Gamma(n + k + 1)}
%   \,. $$
By integrating the power series \eqref{eq:nonchisq-density} with substitution $x \to 2u$,
the expectation $
  \mathbb{E}_{W\sim \chi^2_m(\lambda)} \log W
$, needed in \eqref{eq:improper-kl-div-real}, equals
\begin{multline}  \label{eq:exp-log-chisq-proof}
    % &= \int_0^\infty f_W(x) \log x dx
    % \notag \\
    % &\frac12 e^{- \tfrac\lambda2}
    % \int_0^\infty \sum_{n \geq 0} \biggl(
    %     \frac{
    %       e^{- \tfrac{x}2} \bigl(\tfrac\lambda2\bigr)^n
    %     }{
    %       n! \Gamma(n + \tfrac{m}2)
    %     }
    %     \bigl(\tfrac{x}2\bigr)^{n + \tfrac{m}2 - 1}
    % \biggr) \log{(x)} dx
    % = [\text{ absolute summability and Fubini, or any other conv. thm for integrals of non-negative integrals}]
    % = e^{- \tfrac\lambda2} \sum_{n \geq 0}
    %     \tfrac{\bigl(\tfrac\lambda2\bigr)^n}{n!}
    %     \tfrac1{\Gamma(n + \tfrac{m}2)}
    % \int_0^\infty
    %     e^{- \tfrac{x}2} \bigl(\tfrac{x}2\bigr)^{n + \tfrac{m}2 - 1}
    %     (\log2 + \log \tfrac{x}2) \tfrac{dx}2
    % = [u = \tfrac{x}2]
    % \notag \\
    e^{- \tfrac\lambda2} \sum_{n \geq 0}
        \frac{\bigl(\tfrac\lambda2\bigr)^n}{
          n! \Gamma(n + \tfrac{m}2)
        }
    \int_0^\infty
        e^{- u} u^{n + \tfrac{m}2 - 1}
        \log{(2 u)} du
    % = [\text{definitions:} \Gamma, \psi]
    % = e^{- \tfrac\lambda2} \sum_{n \geq 0}
    %     \tfrac{\bigl(\tfrac\lambda2\bigr)^n}{n!}
    % (\psi(n + \tfrac{m}2) + \log2)
    % \notag
    \\
    = \log{2}
    + e^{- \tfrac\lambda2} \sum_{n \geq 0}
        \frac{\bigl(\tfrac\lambda2\bigr)^n}{n!}
        \psi(n + \tfrac{m}2)
    \,,
\end{multline}
where $\psi(x)$ is the digamma function, i.e.
\begin{equation}  \label{eq:digamma}
  \psi(x)
    % = \frac{d}{dx} \log \Gamma(x)
    = \frac1{\Gamma(x)}
      \int_0^\infty
        u^{x-1} e^{-u} \log{u}
      \, du
    \,,
\end{equation}
which satisfies $
  \psi(z+1) = \psi(z) + \tfrac1z
$, $
  \psi(x)\leq \log x - \tfrac1{2x}
$, and $
  \psi(\tfrac12) = -\gamma - 2\log 2
$ ($\gamma$ is Euler's constant). If we put
\begin{equation}  \label{eq:g-m-series}
  g_m(x)
    % alpha: e^{-x} \sum_{n=0}^{\infty} \frac{x^n}{n!} \psi(n + m / 2)
    % = e^{-x} \sum_{n \geq 0} \frac{x^n}{n! \Gamma(n + \tfrac{m}2)}
    %     \int_0^\infty e^{-t} t^{n + \tfrac{m}2-1} \log{t} dt
    = e^{-x} \sum_{n \geq 0} \frac{x^n}{n!} \psi(n + \tfrac{m}2)
    \,,
\end{equation}
then the desired expectation in \eqref{eq:improper-kl-div-real} equals $
  \log{2} + g_1(\tfrac1{2\alpha})
$.
%
% We can differentiate the series in $g_m(x)$, since the sum converges on $\mathbb{R}$.
% Indeed, it is a power series featuring nonnegative coefficients, which is dominated by $
%   \sum_{n \geq 1} \tfrac{x^n}{n!} \log{(n+\tfrac{m}2)}
% $. By the ratio test, the dominating
% series has infinite radius:
% $$
% \lim_{n\to\infty}
%   \biggl\lvert
%     \frac{
%       n! x^{n+1} \log{(n + 1 + \tfrac{m}2)}
%     }{
%       x^n \log{(n + \tfrac{m}2)} (n+1)!
%     }
%   \biggr\rvert
%   = \lim_{n\to\infty}
%     \lvert x \rvert 
%     \biggl\lvert
%       \frac{
%         \log{(n + 1 + \tfrac{m}2)}
%       }{
%         \log{(n + \tfrac{m}2)} (n+1)
%       }
%     \biggr\rvert
%   % = \lim_{n\to\infty}
%   %   \lvert x \rvert 
%   %   \biggl\lvert
%   %     \frac{
%   %       n + \tfrac{m}2
%   %     }{
%   %       (n + 1 + \tfrac{m}2)((n+1) + (n + \tfrac{m}2) \log{(n + \tfrac{m}2)})
%   %     }
%   %   \biggr\rvert
%   = 0 < 1
%   \,, $$
% since
% $$
% \frac{\log{(x + a + 1)}}{x \log{(x + a)}}
%   \sim \frac{\log{(x+1)}}{(x-a) \log{x}}
%   \sim \frac{\log{(x+1)}}{x \log{x}}
%   \sim \frac{\tfrac1{(x+1)}}{1 + \log{x}}
%   \to 0
%   \,. $$
% A theorem from calculus states, that the formal series derivative (integral)
% coincides with the derivative (integral) of the function, corresponding to
% the power series (everywhere on the convergence region). And the convergence
% regions of derivative (integral) coincide with the region of the original
% power series.
%
Formally differentiating the convergent power series within $g_m$ yields
\begin{equation}  \label{eq:g-m-deriv}
  \frac{d}{d x} g_m(x)
    % = e^{-x} \sum_{n\geq 1} \frac{x^{n-1}}{(n-1)!} \psi(n + \tfrac{m}2) - g_m(x)
    % = e^{-x} \sum_{n\geq 0} \frac{x^n}{n!} \psi(n + \tfrac{m}2 + 1) - g_m(x)
    % = e^{-x} \sum_{n\geq 0} \frac{x^n}{n!} (
    %     \psi(n + \tfrac{m}2 + 1) - \psi(n + \tfrac{m}2)
    % )
    % = e^{-x} \sum_{n\geq 0} \frac{x^n}{n!} \tfrac1{n + \tfrac{m}2}
    % = e^{-x} \sum_{n\geq 0} \frac{x^n}{n!} (n + \tfrac{m}2)^{-1}
    = x^{-\tfrac{m}2} e^{-x} \sum_{n\geq 0}
        \frac{x^{n + \tfrac{m}2}}{n!} (n + \tfrac{m}2)^{-1}
    \,.
\end{equation}
From $
    e^t  = \sum_{n\geq 0} \tfrac{t^n}{n!}
$ on $\mathbb{R}$ and $
  \tfrac{x^\alpha}{\alpha}
    = \int_0^x t^{\alpha-1} dt
$ for $\alpha\neq 0$ we get
% by the monotone convergence theorem on ([0, x], \mathcal{B}([0, x]), dx)
\begin{align}  \label{eq:g-m-deriv-int-2}
  \eqref{eq:g-m-deriv}
    % = x^{-\tfrac{m}2} e^{-x} \sum_{n\geq 0}
    %     \frac{x^{n + \tfrac{m}2}}{n!} (n + \tfrac{m}2)^{-1}
    &= x^{-\tfrac{m}2} e^{-x} \sum_{n\geq 0}
        \frac1{n!} \int_0^x t^{n + \tfrac{m}2 - 1} dt
    % = x^{-\tfrac{m}2} e^{-x}
    %     \int_0^x t^{\tfrac{m}2 - 1} \sum_{n\geq 0} \frac{t^n}{n!} dt
    \notag \\
    &= x^{-\tfrac{m}2} e^{-x}
        \int_0^x t^{\tfrac{m}2 - 1} e^t dt
    \,.
\end{align}
Substitution $u^2 = t$ on $[0, \infty]$ with $2u du = dt$ yields
\begin{equation}  \label{eq:g-m-deriv-int-3}
  \frac{d}{d x} g_m(x)
    % = x^{-\tfrac{m}2} e^{-x}
    %     \int_0^x t^{\tfrac{m}2 - 1} e^t dt
    = 2 x^{-\tfrac{m}2} e^{-x}
        \int_0^{\sqrt{x}} u^{m - 1} e^{u^2} du
    \,.
\end{equation}
In particular, the derivative of \eqref{eq:g-m-series} for $m=1$ is
\begin{equation}  \label{eq:g-m-deriv-one}
  \frac{d}{d x} g_1(x)
    = 2 \frac{F(\sqrt{x})}{\sqrt{x}}
    \,,
\end{equation}
using Dawson's integral $
  F
  % \colon \mathbb{R} \to \mathbb{R}
  \colon x \mapsto e^{-x^2} \int_0^x e^{u^2} du
$.
%
Hence, the derivative of the expectation in \eqref{eq:improper-kl-div-real} with respect
to $\alpha$ is
$$
\frac{d}{d \alpha} g_1(\tfrac1{2\alpha})
  = -\frac1{2 \alpha^2} g_1'(\tfrac1{2\alpha})
  % = -\frac1{2 \alpha^2} 2 \tfrac{F(\sqrt{\tfrac1{2\alpha}})}{\sqrt{\tfrac1{2\alpha}}}
  % = -\frac1{2 \alpha^2} 2 \tfrac{F(\tfrac1{\sqrt{2\alpha}})}{\tfrac1{\sqrt{2\alpha}}}
  = -\frac1{\alpha} \sqrt{\frac2{\alpha}} F(\tfrac1{\sqrt{2\alpha}})
  \,. $$
Finally, since $\alpha$ is nonnegative, it is typically parameterized via its logarithm.
The gradient of \eqref{eq:improper-kl-div-real} w.r.t $\log \alpha$ is
\begin{equation}  \label{eq:real-kl-div-deriv-log}
  \frac{d \eqref{eq:improper-kl-div-real}}{d\log \alpha}
  % \frac{d}{d\log \alpha}
  %   \frac12 \mathbb{E}_{z \sim \mathcal{N}(0,1)}
  %     \log \lvert z + \tfrac1{\sqrt{\alpha}} \rvert^2
    % = \frac12 \frac{d\alpha}{d\log \alpha}
    %   \frac{d}{d\alpha} \bigl(\mathbb{E}\cdots \bigr)
    % = - \frac\alpha2 \tfrac1{\alpha}
    %   \sqrt{\tfrac2{\alpha}} F(\tfrac1{\sqrt{2\alpha}})
    = - \frac1{\sqrt{2\alpha}} F\bigl(\tfrac1{\sqrt{2\alpha}}\bigr)
    \,.
\end{equation}

We compute the Monte-Carlo estimator of \eqref{eq:improper-kl-div-real} on a sample of $10^7$
draws over an equally spaced grid of $\log \alpha$ in $[-12, +12]$ of size $4096$. The optimal
coefficients of the regression, proposed by \citet{molchanov_variational_2017}, are within
$1\%$ relative tolerance of the reported therein.
\begin{equation}  \label{eq:improper-kl-div-real-approx}
  \eqref{eq:improper-kl-div-real}
% - KL(\mathcal{N}(w \mid \mu, \alpha \mu^2) \|
%     \tfrac1{\lvert w \rvert})
%   \approx
%     k_1 \sigma(k_2 + k_3 \log \alpha)
%     + C \big\vert_{C = -k1}
%     - \tfrac12 \log (1 + e^{-\log \alpha})
  % = \tfrac12 \log \alpha
  %   - \mathbb{E}_{\xi \sim \mathcal{N}(1, \alpha)}
  %   \log{\lvert \xi \rvert} + C
  \approx
    \frac12 \log{\bigl(1 + e^{-\log \alpha}\bigr)}
    + k_1 \sigma\bigl(- (k_2 + k_3 \log \alpha)\bigr)
    % - k_1 \bigl(1 - \sigma(k_2 + k_3 \log \alpha)\bigr)
  \,,
\end{equation}
% in parctial implementations we suggest using softplus to numerically comptue the last
% term, since it is typically implemented in a floating-point stable fashion.
The numerically estimated derivative of the penalty term wrt $\log \alpha$ using forward
differences seems to be following \eqref{eq:real-kl-div-deriv-log} very closely (up to
sampling error) see fig.~\ref{fig:molchanov-derivative-replica}. The derivative of
\eqref{eq:improper-kl-div-real-approx} with respect to $\log \alpha$ appears to be very close
to \eqref{eq:real-kl-div-deriv-log}. We compute a similar Monte-Carlo estimate for
$\cplx$ variational dropout KL divergence term in \eqref{eq:c-vd-kl-div} with $\beta = 2$,
fit the best approximation \eqref{eq:improper-kl-div-real-approx} and compare the derivatives.

\begin{figure}[!h]
  \centering
  \includegraphics[width=\linewidth]{../notebooks/assets/grad_log.pdf}
  \caption{$\tfrac{d f(\alpha)}{d \log{\alpha}}$ of the fit proposed in
  \citet{molchanov_variational_2017}, MC estimate of \eqref{eq:improper-kl-div-real},
  and the exact derivative using \eqref{eq:real-kl-div-deriv-log}.}
  \label{fig:molchanov-derivative-replica}
\end{figure}

% subsection real-chisq-grad (end)

\subsection{Backpropagation through $\cplx$-networks} % (fold)
\label{sub:wirtinger_calculus}

% essential intro into Wirtinger calculus (CR)
% https://math.stackexchange.com/a/444493
Wirtinger ($\cplx\real$) calculus relies on the natural identification of $\cplx$ with $
  \real \times \real
$, and regards $
  f\colon \cplx \to \cplx
$ as an equivalent in algebraic sense function $F\colon \real^2 \to \cplx$ defined $
  f(z) = f(u + jv) = F(u, v)
$. Within this framework the differential of $f$ at $z = u + jv \in \cplx$ is
$$
df(z)
  = \frac{\partial f}{\partial z} dz
    + \frac{\partial f}{\partial \conj{z}} d\conj{z}
   % = \frac12\bigl(
   %    \frac{\partial}{\partial u}
   %    - j \frac{\partial}{\partial v}
   % \bigr) F (du - j dv)
   % + \frac12\bigl(
   %    \frac{\partial}{\partial u}
   %    + j \frac{\partial}{\partial v}
   % \bigr) F (du - j dv)
   % = \frac12 \bigl(
   %    \frac{\partial F}{\partial u} du - j \frac{\partial F}{\partial v} du
   %    + j \frac{\partial F}{\partial u} dv + \frac{\partial F}{\partial v} dv
   % \bigr)
   % + \frac12 \bigl(
   %    \frac{\partial F}{\partial u} du + j \frac{\partial F}{\partial v} du
   %    - j \frac{\partial F}{\partial u} dv + \frac{\partial F}{\partial v} dv
   % \bigr)
   = \frac{\partial F}{\partial u} du
     + \frac{\partial F}{\partial v} dv
   = dF(u, v)
  \,, $$
with formally defined derivatives $
  \tfrac{\partial}{\partial z}
    = \tfrac12 \bigl(
      \tfrac{\partial}{\partial u}
      - j \tfrac{\partial}{\partial v}
    \bigr)
$ and $
  \tfrac{\partial}{\partial \conj{z}}
    = \tfrac12 \bigl(
      \tfrac{\partial}{\partial u}
      + j \tfrac{\partial}{\partial v}
    \bigr)
$, and differentials $dz = du + j dv$ and $d\conj{z} = du - j dv$. This implies that the
complex argument and its conjugate are treated as independent variables. Cauchy-Riemann
conditions $
  -j \tfrac{\partial F}{\partial v} = \tfrac{\partial F}{\partial u}
$ can be expressed as $
  \tfrac{\partial f}{\partial \conj{z}} = 0
$ in this notation. Thus $\cplx\real$ calculus subsumes the usual $\cplx$-calculus on
holomorphic functions, when $f(z)$, regarded as $f(z, \conj{z})$, is constant with respect
to $\conj{z}$. The usual rules of calculus, like chain and product rules, follow directly
from the definition of the operators, e.g.
$$
  \frac{\partial (f\circ g)}{\partial z}
    = \frac{\partial f(g(z))}{\partial g} \frac{\partial g(z)}{\partial z}
    + \frac{\partial f(g(z))}{\partial \conj{g}} \frac{\partial \conj{g(z)}}{\partial z}
    % = \nabla G \nabla F
  \,. $$
% Show that $dh(z) = df(g(z)) dg(z)$.
% $h(z) = f(g(z)) = F(G_r(u, v), G_i(u, v)) = H(u, v)$
% $$
% dH
%   = \tfrac{\partial F}{\partial x} \tfrac{\partial G_r}{\partial u} du
%   + \tfrac{\partial F}{\partial x} \tfrac{\partial G_r}{\partial v} dv
%   + \tfrac{\partial F}{\partial y} \tfrac{\partial G_i}{\partial u} du
%   + \tfrac{\partial F}{\partial y} \tfrac{\partial G_i}{\partial v} dv
%   = \tfrac{\partial F}{\partial r} d\Re g(z)
%   + \tfrac{\partial F}{\partial i} d\Im g(z)
%   = df(\Re g(z) + j\Im g(z)) d g(z)
%   = df(g(z)) d g(z)
%   \,. $$

In machine learning tasks the target objective is typically empirical risk, which has to
be real-valued to be minimized. Nevertheless, the expression of the $\cplx\real$ gradient
is compatible with what is expected, when $f$ is treated like a $\real^2$ function. For a
real-valued $f\colon \cplx \to \real$ we have $\conj{f} = f$, which implies $
  \tfrac{\partial f}{\partial \conj{z}}
    = \tfrac{\partial \conj{f}}{\partial \conj{z}}
    = \conj{\tfrac{\partial f}{\partial z}}
$, whence
$$
df
  = \tfrac{\partial f}{\partial z} dz
    + \tfrac{\partial f}{\partial \conj{z}} d\conj{z}
  % = \conj{\tfrac{\partial \conj{f}}{\partial \conj{z}}} dz
  % + \conj{\conj{\tfrac{\partial f}{\partial \conj{z}}} dz}
  = 2 \Re \Bigl(
    \conj{\tfrac{\partial f}{\partial \conj{z}}} dz
  \Bigr)
  = 2 \Re \bigl(
    \tfrac{\partial f}{\partial z} dz
  \bigr)
  \,. $$
Thus, the gradient of $f$ at $z$ is given by $
  \nabla_z f(z)
    = \tfrac{\partial F}{\partial u}
      + j \tfrac{\partial F}{\partial v}
$.

Natural identification of $\cplx$ with $\real\times \real$, storing the real and imaginary
parts in interleaved format, emulation of $\cplx$-algebra using $\real$-valued arithmetic,
and Wirtinger calculus make it possible to reuse $\real$ back-propagation and existing
auto-differentiation frameworks \citep{trabelsi_deep_2017}.

% subsection wirtinger_calculus (end)

\subsection{$\cplx$-Linear operator representation} % (fold)
\label{sub:c-linear_operator_representation}

% proof that such operators exist and are unique (well this is an obvious statement)
Consider $L \colon \cplx^m \to \cplx^n$ -- linear in $\cplx$. Then $
  L(u + jv) = L u + j L v
$ for any $u, v \in \real^m$, which implies that the effect of $L$ as $\cplx$-linear
operator is determined by its restriction onto $\real^n$. Let $
  F = L\vert_{\real^m}
  \colon \real^m \to \cplx^n
$ and observe that $F_r = \Re \circ F$ and $F_i = \Im \circ F$ are $\real$-linear operators
such that $F = F_r + j F_i$ (pointwise). Indeed,
$$
  F_r(a + \lambda b)
  % = \Re F(a + \lambda b)
  = \Re L(a + \lambda b)
  % = \Re \bigl( L a + \lambda L b \bigr)
  = \Re L a + \lambda \Re L b
  % = \Re F a + \lambda \Re F b
  = F_r a + \lambda F_r b
  \,. $$
Therefore for any $\cplx$-linear operator $L$ there are $\real$-linear operators $U, V$
such that
$$
L z 
  = (U + j V) (\Re z + j \Im z)
  % = (U + j V) \Re z + j (U + j V) \Im z
  % = U \Re z + j V \Re z + j \bigl(U \Im z + j V \Im z \bigr)
  = U \Re z - V \Im z + j \bigl( V \Re z + U \Im z \bigr)
  % = (U + j V) z
  \,. $$
Uniqueness of these operators follows, if this decomposition is applied to any $z$ with
$\Im z = 0$.

% subsection c-linear_operator_representation (end)

\subsection{Other MNIST-like datasets} % (fold)
\label{sub:other_mnist_like_datasets}

We provide the figures of the MNIST-like experiments, omitted from the main paper
due to size limitations.

\begin{figure}[!h]
  \centering
  \begin{subfigure}[b]{1.\textwidth}  % imcl2019-style swears at this
    \centering
    \includegraphics[width=\linewidth]{../assets/figure__mnist-like__trade-off/legacy__VD__fashionmnist__fft__-0.5.pdf}
  \end{subfigure} \\%
  % \hfill
  \begin{subfigure}[b]{1.\textwidth}  % imcl2019-style swears at this
    \centering
    \includegraphics[width=\linewidth]{../assets/figure__mnist-like__trade-off/legacy__VD__fashionmnist__raw__-0.5.pdf}
  \end{subfigure}
  \caption{%
    The compression-accuracy trade-off on the letters part of the Fashion MNIST dataset.
    (\textit{top}) \texttt{fft} features, (\textit{bottom}) \texttt{raw} features.
  }
  \label{fig:mnist-like__trade-off__fashionmnist}
\end{figure}

\begin{figure}[!h]
  \centering
  \begin{subfigure}[b]{1.\textwidth}  % imcl2019-style swears at this
    \centering
    \includegraphics[width=\linewidth]{../assets/figure__mnist-like__trade-off/legacy__VD__mnist__fft__-0.5.pdf}
  \end{subfigure} \\%
  % \hfill
  \begin{subfigure}[b]{1.\textwidth}  % imcl2019-style swears at this
    \centering
    \includegraphics[width=\linewidth]{../assets/figure__mnist-like__trade-off/legacy__VD__mnist__raw__-0.5.pdf}
  \end{subfigure}
  \caption{%
    The compression-accuracy trade-off on the letters part of the MNIST dataset.
    (\textit{top}) \texttt{fft} features, (\textit{bottom}) \texttt{raw} features.
  }
  \label{fig:mnist-like__trade-off__mnist}
\end{figure}

% subsection other_mnist_like_datasets (end)

% section appendix (end)

\end{document}
