\documentclass[a4paper,10pt]{article}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{grffile}

\usepackage{subcaption}

\usepackage{booktabs}

% \usepackage{lipsum}

\newcommand{\important}[1]{\textbf{\color{red} #1}}
\newcommand{\attn}[2]{\textbf{\color{red} #2~\textsuperscript{\textit{[#1]}}}}
\newcommand{\verify}[1]{\attn{verify}{#1}}
\newcommand{\rewrite}[1]{\attn{rewrite}{#1}}
\newcommand{\todo}[1]{{\color{blue} [TODO]} \important{#1}}

\newcommand{\real}{\mathbb{R}}
\newcommand{\cplx}{\mathbb{C}}
\newcommand{\tr}[1]{\mathop{tr}{#1}}

\newcommand{\hop}{{\mkern-1.5mu\dagger}}
\newcommand{\conj}[1]{\overline{#1}}
% \renewcommand{\top}{{\mkern-1.5mu\intercal}}
\renewcommand{\vec}[1]{\overrightarrow{#1}}
\newcommand{\diag}[1]{\mathrm{diag}{#1}}

\title{Sparsifying $\cplx$-valued networks}
\author{Ivan Nazarov}

\begin{document}
\maketitle

% {\color{red} \lipsum[1-3]}

\section{Introduction} % (fold)
\label{sec:introduction}

Why sparsity matters, why $\cplx$ networks?

Where are $\cplx$-networks used? and do these applications warrant sparsity?

% section introduction (end)

\section{$\cplx$-valued networks} % (fold)
\label{sec:c_valued_networks}

In this section i remind the reader what the complex-valued networks are, how
they are implemented and how much of $\cplx$-ness they have. Mention careful
design.

I will describe the linear layers (dense, masked, and convolutions) and
activations borrowed straight from the $\real$-values networks. Then i shall
mention that this has been tried before and list papers that one way or another
address $\cplx$-data processing with neural networks.

\cite{trabelsi_deep_2017} provide a set of building blocks for deep $\cplx$-valued
networks. The core of their work is structured framework for complex-valued operations
including representation and linear layers (dense and convolutional) as well
as $\cplx$-valued activations and complex batch-normalization and weight initialization.

Following \cite{trabelsi_deep_2017} we identify $\cplx$ with $\real^2$ via $
  z_\mathrm{r} + j z_\mathrm{i} \mapsto (z_\mathrm{r}, z_\mathrm{i})
$, storing the real and imaginary parts in interleaved format, and emulate $\cplx$-algebra
using $\real$-valued arithmetic. Thus designed $\cplx$-network is, essentially, a
$\real$-network with \textit{double} the layers' widths to accommodate real and imaginary
parts and with constraints on layers' parameters that make them respect $\cplx$-algebra.
This makes it possible to retrofit $\cplx$-valued networks into the existing $\real$-valued
auto-differentiation libraries.

In particular, common linear layers, such as dense layers and convolutions,
$
  L \colon \cplx^{\mathrm{[in]}}
    \to \cplx^{\mathrm{[out]}}
$ act upon their inputs as follows:
$$
\real^{\mathrm{[in]} \times 2}
  \to \real^{\mathrm{[out]} \times 2}
  \colon (x_r, x_i)
    \mapsto \Bigl(
      L_r x_r - L_i x_i,
      L_r x_i + L_i x_r
    \Bigr)
  \,, $$
where $
  L_r, L_i
    \colon \real^{\mathrm{[in]}}
      \to \real^{\mathrm{[out]}}
$ are the unique $\real$-linear operators that make up $L = L_r + j L_i$ the overall
effect of the layer. For instance, to respect $\cplx$-arithmetic a linear layer acting
on $x$ should have $W_{ri} = - W_{ir}$ and $W_{rr} = W_{ii}$ in
$$
  \begin{pmatrix}
    y_r \\ y_i
  \end{pmatrix}
    = \begin{pmatrix}
      {\color{red} W_{rr}} & {\color{blue} W_{ri}} \\ 
      {\color{blue} W_{ir}} & {\color{red} W_{ii}}
    \end{pmatrix}
    \begin{pmatrix}
      x_r \\ x_i
    \end{pmatrix}
    + \begin{pmatrix}
      b_r \\ b_i
    \end{pmatrix}
  \,. $$
Other algebraic operations that might be of use in deep networks are also designed in such
a way as to respect $\cplx$-operations. It is worth pointing out that the key advantage of
the $\cplx$-constraints is that they permit use of Strassen's $3$M multiplication algorithm,
potentially \verify{saving up} to $25\%$ on floating point multiplications. \todo{
  in contrast to what? to na{\:i}ve (obvious)? to paired-$\real$ ($2 \times 2$-$2\times 1$
  -- 4 fmul)?
}

% proof that such operators exist and are unique (well this is an obvious statement)
Consider $L \colon \cplx^m \to \cplx^n$ -- linear in $\cplx$. Then $
  L(u + jv) = L u + j L v
$ for any $u, v \in \real^m$, which implies that the effect of $L$ as $\cplx$-linear
operator is determined by its restriction onto $\real^n$. Let $
  F = L\vert_{\real^m}
  \colon \real^m \to \cplx^n
$ and observe that $F_r = \Re \circ F$ and $F_i = \Im \circ F$ are $\real$-linear operators
such that $F = F_r + j F_i$ (pointwise). Indeed,
$$
  F_r(a + \lambda b)
  % = \Re F(a + \lambda b)
  = \Re L(a + \lambda b)
  % = \Re \bigl( L a + \lambda L b \bigr)
  = \Re L a + \lambda \Re L b
  % = \Re F a + \lambda \Re F b
  = F_r a + \lambda F_r b
  \,. $$
Therefore for any $\cplx$-linear operator $L$ there are $\real$-linear operators $U, V$
such that
$$
L z 
  = (U + j V) (\Re z + j \Im z)
  % = (U + j V) \Re z + j (U + j V) \Im z
  % = U \Re z + j V \Re z + j \bigl(U \Im z + j V \Im z \bigr)
  = U \Re z - V \Im z + j \bigl( V \Re z + U \Im z \bigr)
  % = (U + j V) z
  \,. $$
Uniqueness of these operators follows, if this decomposition is applied to any $z$ with
$\Im z = 0$.

As far as non-linearities are concerned, \cite{trabelsi_deep_2017} consider a few truly
complex-valued ones, that are nevertheless not $\cplx$-differentiable: the most frequently
used activations are the common $\real$-valued non-linearities applied to real and
imaginary components independently.

\todo{something about activations}
modRelu -- relu on the modulus of a complex value (preserves the angle)
$
\mathrm{modReLU}
  \colon z \mapsto z \bigl(
    1 - \tfrac\theta{\lvert z \rvert}
  \bigr)_+
$
$
\mathrm{modReLU}
  \colon r e^{j \phi} \mapsto e^{j \phi} (r - \theta)_+
$

\todo{finish activations}

\subsection{Backprop through $\cplx$-networks} % (fold)
\label{sub:backprop_through_c_networks}

The question of differentiability of complex-valued networks is tricky, since they usually
use non-differentiable non-linearities. However, a more severe problem stems from the loss
objective being real-valued. Indeed, if $f\colon \cplx \to \cplx$ were $\cplx$-differentiable
(holomorphic), the composition of $f$ with a non-trivial $h \colon \cplx\to \real$ makes it
not $\cplx$-differentiable with respect parameters, since $h$ violates Cauchy-Riemann conditions.
This problem is commonly dealt with by using Wirtinger, or $\cplx\real$ calculus, which subsumes
$\cplx$-calculus, but is also applicable to non-holomorphic functions of $\cplx$-argument, see
\cite{adali_complex-valued_2011} and \cite{trabelsi_deep_2017}. In essence, this approach
regards $f$ as a function of two independent variables $z$ and its conjugate $\conj{z}$ and
symbolically redefines the derivatives appropriately. This approach naturally extends the
chain rule to non-holomorphic functions and can be directly reformulated in terms of derivatives
with respect to real and imaginary parts, making it possible to use and apply the rules of
$\real$-calculus to the networks represented in paired $\real$ fashion and use existing
$\real$-valued backpropagation in autograd software.

% essential intro into Wirtinger calculus (CR)
Take $f\colon \cplx \to \cplx$ and identify it with $F\colon \real^2 \to \cplx$ function
via $F(u, v) = f(z)\vert_{z=u + j v}$. Wirtinger calculus introduces two formal derivative
operators \rewrite{(this seems to be very verbatim)}
$
  \tfrac{\partial}{\partial z}
    = \tfrac12 \bigl(
      \tfrac{\partial}{\partial u}
      - j \tfrac{\partial}{\partial v}
    \bigr)
$ and $
  \tfrac{\partial}{\partial \conj{z}}
    = \tfrac12 \bigl(
      \tfrac{\partial}{\partial u}
      + j \tfrac{\partial}{\partial v}
    \bigr)
$, defines $dz = du + j dv$, $d\conj{z} = du - j dv$, and the differential of $f$ as
$$
df = \tfrac{\partial f}{\partial z} dz
    + \tfrac{\partial f}{\partial \conj{z}} d\conj{z}
   % = \tfrac12\bigl(
   %    \tfrac{\partial}{\partial u}
   %    - j \tfrac{\partial}{\partial v}
   % \bigr) F (du - j dv)
   % + \tfrac12\bigl(
   %    \tfrac{\partial}{\partial u}
   %    + j \tfrac{\partial}{\partial v}
   % \bigr) F (du - j dv)
   % = \tfrac12 \bigl(
   %    \tfrac{\partial F}{\partial u} du - j \tfrac{\partial F}{\partial v} du
   %    + j \tfrac{\partial F}{\partial u} dv + \tfrac{\partial F}{\partial v} dv
   % \bigr)
   % + \tfrac12 \bigl(
   %    \tfrac{\partial F}{\partial u} du + j \tfrac{\partial F}{\partial v} du
   %    - j \tfrac{\partial F}{\partial u} dv + \tfrac{\partial F}{\partial v} dv
   % \bigr)
   = \tfrac{\partial F}{\partial u} du
     + \tfrac{\partial F}{\partial v} dv
   = dF
  \,. $$
Thus the complex value and its conjugate are treated as independent variables. In this notation
Cauchy-Riemann conditions are expressed as $
  \tfrac{\partial f}{\partial \conj{z}} = 0
$, or $
  -j\tfrac{\partial F}{\partial v} = \tfrac{\partial F}{\partial v}
$. These conditions impose a rigid structure on real and imaginary parts of $F(u, v)$, namely $
  \tfrac{\partial F_{\Re }}{\partial u} = \tfrac{\partial F_{\Im }}{\partial v}
$ and $
  \tfrac{\partial F_{\Re }}{\partial v} = - \tfrac{\partial F_{\Im }}{\partial u}
$. Thus Wirtinger calculus subsumes the usual $\cplx$-calculus, when $
  \tfrac{\partial f}{\partial \conj{z}} = 0
$, i.e. $
  f(z)
    % = g(z, \conj{z})
    = F(\tfrac12 (z + \conj{z}), \tfrac1{j 2} (z - \conj{z}))
$ does not depend on $\conj{z}$.

For a real-valued $f\colon \cplx \to \real$ we have $\conj{f} = f$, whence $
  \tfrac{\partial f}{\partial \conj{z}}
    = \tfrac{\partial \conj{f}}{\partial \conj{z}}
    = \conj{\tfrac{\partial f}{\partial z}}
$. Hence
$$
df
  = \tfrac{\partial f}{\partial z} dz
    + \tfrac{\partial f}{\partial \conj{z}} d\conj{z}
  % = \conj{\tfrac{\partial \conj{f}}{\partial \conj{z}}} dz
  % + \conj{\conj{\tfrac{\partial f}{\partial \conj{z}}} dz}
  = 2 \Re \Bigl(
    \conj{\tfrac{\partial f}{\partial \conj{z}}} dz
  \Bigr)
  = 2 \Re \bigl(
    \tfrac{\partial f}{\partial z} dz
  \bigr)
  \,. $$
Hence the gradient, being the direction of \rewrite{maximal growth} of $f$ at $z$ is given by $
  \tfrac{\partial f}{\partial \conj{z}}
    = \tfrac{\partial F}{\partial u}
      + j \tfrac{\partial F}{\partial v}
$. Thus under Wirtinger calculus it is possible to reuse $\real$-backpropagation.

% Show that $dh(z) = df(g(z)) dg(z)$.
% $h(z) = f(g(z)) = F(G_r(u, v), G_i(u, v)) = H(u, v)$
% $$
% dH
%   = \tfrac{\partial F}{\partial x} \tfrac{\partial G_r}{\partial u} du
%   + \tfrac{\partial F}{\partial x} \tfrac{\partial G_r}{\partial v} dv
%   + \tfrac{\partial F}{\partial y} \tfrac{\partial G_i}{\partial u} du
%   + \tfrac{\partial F}{\partial y} \tfrac{\partial G_i}{\partial v} dv
%   = \tfrac{\partial F}{\partial r} d\Re g(z)
%   + \tfrac{\partial F}{\partial i} d\Im g(z)
%   = df(\Re g(z) + j\Im g(z)) d g(z)
%   = df(g(z)) d g(z)
%   \,. $$
% subsection backprop_through_c_networks (end)

% section c_valued_networks (end)

\section{Related research} % (fold)
\label{sec:realted_research}

\subsection{On DPD} % (fold)
\label{sub:on_dpd}

Radio Frequency power amplifiers, being physical electronic devices, have imperfections such
as parasitic inductance and capacitance. This coupled with them being operated in saturation,
make their gain respond non-linearly to the input signal, adding undesirable interference and
spectral spreading. Although these effects can be minimized by careful circuit design, they
cannot be eliminated entirely \cite{citation_needed}. Digital signal predistortion aims at
linearizing the amplifier, by introducing signal-dependent compensating component to the input.
At the same time the non-linearities should be corrected on-device efficiently and with low
latency in order to reduce cross-channel interference and enable higher bandwidth utilization
in wireless transmitters.

\cite{traverso_low_2019} uses \rewrite{some group delay estimators of something};
\cite{schoukens_obtaining_2017} develops an iterative approach to inference of the predistortion for a given
signal \todo{unclear};

\cite{tarver_design_2019} models the predistorter as a paired real dense shallow neural network and
demonstrates viability and competitiveness of this approach against polynomial based DPD on an
FPGA implementation.

% About rfWEbLab
The ``RFWebLab'' platform is remote RF power amplifier setup with web-based API, that feeds
the user-submitted quadrature signal through a class AB GaN amplifier and sends-back the measured
output \cite{dpdcompetiton2018}. It is maintained by the GHz Centre at Chalmers University of
Technolgy and National Instruments \cite{landin_rfweblab_2015}.

We use a custom-written python interface to the WebLab setup that is compatible with the MATLAB
interface form the setup owner.


% subsection on_dpd (end)

% section realted_research (end)

\section{Complex Variational Dropout} % (fold)
\label{sec:complex_variational_dropout}

In this section we outline and derive the variational dropout technique for complex-%
valued networks, which re-traces the evolution of dropout technique for real-valued
weights.

Variational dropout proposed by \cite{kingma_variational_2015} enables automatic parameter
relevance detection by learning individual per-parameter dropout rates in Gaussian dropout,
\cite{srivastava_dropout_2014,wang_fast_2013}, which generalizes Binary (Bernoulli) Dropout
\cite{hinton_improving_2012}.

\begin{quote}
% exceprts from kingmaetal2015
Bernoulli dropout alleviates the problem of overfitting
Gaussian dropout is in fact a variational approximation with fixed rate and thus optimizes ELBO
Allowing per-parameter rates enables flexible posterior approximations.

Bayesian posterior inference over the parameters of a network allows some control of overfitting;
``exact inference is intractable, but efficient approximation schemes can be designed''

Explore a trick that translates the uncertainty about global parameters into local noise, that
is independent across the elements of the minibatch.

Effectively regularizes by adding random noise.

bayesian inference: update prior belief using model likelihood of iid data into posterior belief
Exact psoterior -- intractable, approximations needed
variational inference: cast posterior inference as optimization over the approximation family

%sgvb
the gradient in sgvb is unbiased estimate of the $\log$-likelihood part of the elbo

in practice the performance of stochastic gradient methods depends on the variance of the gradient
field. The variance of (the value of) the minibatch estiamte of the likelihood term has a
non-negligible intrabatch covariance term. To ensure zero covariance we can sample a separate
$\omega\sim q_\omega(\theta)$ for each element in the minibatch, 

%lrt
whenever source of uncertainty can be translated to local noise in the intermediate states
since the expected likelihood depends on the parameters through the activations of the layer.

sampling the activations directly, without the parameter randomness gives a low cost efficient
MC estimator (statistical efficiency).

Factorized Gaussian -- closure under linear transformations -- sample the local activation noise
from an appropriate Gaussian 

% on dropout
variational dropout: a reinterpretation of dropout with continuous noise as a variational inference
method. allow adaptive,data dependent dropout rates.

\end{quote}


% a comment on Bernoulli dropout
The key idea of Bernoulli dropout is to randomly mask input features of a layer during
training with in order to learn decorrelated feature maps. In a recent study of network
capacity for multitask learning, \cite{multitask2019}, it was shown, that associating
each dataset (task) with a context, that masks a subset of the parameters, allows non-%
destructive `packing' of the tasks in orthogonal on average `subsets' of a single set
of weights, without much interference between tasks. This lends support for beneficial
effects of Bernulli Dropout on test performance, since, in effect, the network learns
to perform well on identical copies of the same task with different (binary) contexts
with a single parameter set.

% exposition
To simplify exposition of the proposed technique we shall consider the linear dense layer,
and afterwards outline the caveats for convolutional layers. We begin with $\real$-valued
layer and later focus on $\cplx$-valued layer.

, that helps with model overfitting.
% plan

A linear layer is parameterized by $W \in\real^{n \times m}$ and bias $b\in \real^n$ and
acts on its input $x\in \real^m$ via $y = W x + b$ with $y\in \real^n$. The key idea of
Bernoulli dropout is to mask elements in the matrix $W$ of the layer
with some given probability $p$: $W = \theta \odot \xi$ with $\theta$ being the learnt
weight matrix, and $
  \xi \sim \mathcal{B}(\{0, \tfrac1{1-p}\}, 1-p)
$ -- an iid Bernoulli mask. During training, a dropout mask $\xi$ is used to compute $W$,
and during evaluation the weights are fixed to the expectation of $W$ which is $\theta$.
The discussion and analysis in \cite{kingma_variational_2015}, suggest that drawing one
common dropout mask for all data in the minibatch reduces statistical efficiency the estimator
of the weights' gradient in the stochastic approximation of the evidence lower bound. 

Gaussian dropout works in a similar fashion, except the dropout mask $\xi$ is iid $
  \mathcal{N}(1, \alpha=\tfrac{p}{1-p})
$, i.e. a multiplicative Gaussian noise is injected into the network.

The key idea of Variational Dropout is to assume a Gaussian Mean field variational
approximation of the posterior distribution of linear layer's weights (or convolutional
kernels, with a caveat\footnote).  \footnotetext{
  at each spatial patch of input data the kernel is independently redrawn form the
  VI distribution. This again helps with gradient variance reduction.
}
Weight $W$ are assumed to have independently distributed elements with $
  w_{ij} \sim \mathcal{N}(\theta_{ij}, \sigma^2_{ij})
$.

In fact multiplicative Gaussian dropout on weights $W$ is in fact itself a variational
approximation with variance $\sigma^2_{ij} = \theta_{ij}^2 \tfrac{p}{1-p}$ determined
by the dropout rate.

% set by step we retell the story of kingmaetal2015

Gaussian variational approximation enabled \cite{kingma_variational_2015} propose a
\textit{local reparameterization trick}, which reduces the variance of the gradients
in SGVB and does not resample $W$ per element of a minibatch. It relies the closure
of Gaussian distribution under linear transformations. Indeed, if $W$ is a Gaussian
matrix with independently distributed $
  W_{ij} \sim \mathcal{N}(\theta_{ij}, \sigma^2_{ij})
$, then we have \important{moved}.



and instead
translating the uncertainty from the weights into local output noise of each linear layer.
Without this it would have been necessary to draw new set of random weights per each element
in a mini-batch.

$$
  \mathcal{N}(\theta_{ij}, \alpha \theta^2_{ij})
  \overset{\mathcal{D}}{\sim}
  \theta_{ij} \mathcal{N}(1, \alpha)
  \,, $$

\cite{molchanov_variational_2017} propose additional step in the local reparametrization,
which further reduces the variance of the gradient estimator for each dropped out
weight. Their simple formal change of variables $(\theta, \alpha) \to (\theta, \sigma^2)$
decouples the stochastic component from the weights \rewrite:
$$
  w_{ij} = \theta_{ij} + \theta_{ij} \sqrt{\alpha} \varepsilon_{ij}
  \,\to\,
  w_{ij} = \theta_{ij} + \sigma^2_{ij} \varepsilon_{ij}
  \,, $$
with the appropriate change of variables in the Kullback-Leibler divergence.
($\alpha_{ij} = \tfrac{\sigma_{ij}^2}{\lvert \theta_{ij}\rvert^2}$).

Criticism of \cite{gale_state_2019} implies that $\ell_0$-variational dropout,
proposed by \cite{louizos_learning_2017} and based on the concrete binary distribution
of \cite{maddison_concrete_2016}, works consistently better than the variational
Gaussian dropout.

Maths and related results \cite{pav_moments_2015,taubock_complex-valued_2012},
and \cite{karseras_caution:_2014}

% section complex_variational_dropout (end)

\section{Experiments} % (fold)
\label{sec:Experiments}

In this section we will compare the complex variational dropout techniques, discussed
above, on the tasks and datasets, that were studied in prior research on complex-%
valued networks.

\subsection{Tasks and datasets} % (fold)
\label{sub:tasks_and_datasets}

\cite{trabelsi_deep_2017} studies the applicability of complex-valued networks to
image classification (CIFAR-10, CIFAR-100, reduced train of SVHN), music-transcription
(MusicNet), and Speech Spectrum prediction (TIMIT).
\begin{itemize}
  \item 
\end{itemize}


\cite{monning_evaluation_2018}
\cite{jankowski_complex-valued_1996}
\cite{amin_complex-valued_2012}
\cite{sarroff_complex_2018}

% subsection tasks_and_datasets (end)

\subsection{Results and Discussion} % (fold)
\label{sub:results_and_discussion}

% subsection results_and_discussion (end)

% section Experiments (end)

\section{Trunk} % (fold)
\label{sec:trunk}

\subsection{$\cplx$-Gaussian Distribution} % (fold)
\label{sub:c_gaussian_distribution}

A random vector $z\in \cplx^m$ is said to have complex Gaussian distribution, if its characteristic
function is
$$
z \sim \mathcal{N}^{\cplx}_m(z \vert \mu, \Gamma, C)
  \Leftrightarrow
  \mathbb{E}_{z} e^{j \Re{(t^{\hop} z)}}
  = \exp{\{
    j \Re{(t^{\hop} \mu)}
    - \tfrac14 \bigl(
      t^{\hop} \Gamma t + \Re{(t^{\hop} C \conj{t})}
    \bigr)
  \}}
  \,, $$
where $\mu \in \cplx^m$ is the mean, $\Gamma, C \in \cplx^{m\times m}$ are complex covariance
and relation matrices satisfying $C^\top = C$, $\Gamma^{\hop} = \Gamma$, $\Gamma \succeq 0$,
and $
  \conj{\Gamma} \succeq C^{\hop} \Gamma^{-1} C
$. The matrices $\Gamma$ and $C$ correspond to $
  \mathbb{E} (z - \mu)(z - \mu)^{\hop}
$ and $
  \mathbb{E} (z - \mu)(z - \mu)^\top
$, respectively. Here ${\cdot}^{\hop}$ denotes Hermitian conjugate, i.e. $
  A^{\hop} = (\conj{A})^\top
$, ${\cdot}^{\top}$ -- the matrix transpose, and $\conj{\cdot}$ -- the complex conjugation
elementwise.
%
% The intuition behind them is more apparent in the complex univariate case ($m=1$):
% here $\Gamma = \sigma^2 \geq 0$, and $C$ can be represented as $\sigma^2 \rho \in \cplx$ with $
%   \lvert \rho \rvert \leq 1
% $. Therefore $\sigma^2$ affects the overall ``concentration'' of the complex random variable,
% whereas $\rho$ is responsible for the strength and ``direction'' of the linear relation between
% the real and imaginary parts, \cite{lapidoth_capacity_2003}.
% $$
% \frac{\sigma^2}2
%   \begin{pmatrix}
%     1 + \Re{\rho} & \Im{\rho} \\
%     \Im{\rho} & 1 - \Re{\rho}
%   \end{pmatrix}
%   = \frac{\sigma^2}2 I
%   + \frac{\sigma^2 \lvert \rho \rvert}2 
%   \begin{pmatrix}
%     \cos\theta & \sin\theta \\
%     \sin\theta & - \cos\theta
%   \end{pmatrix}
% $$

A characterization of a $\cplx$-Gaussian vector, more useful for the current study, is via the
equivalence with the paired real multivariate Gaussian distribution:
\begin{equation}  \label{eq:cn-paired-real-density}
z \sim \mathcal{N}^{\cplx}_m(\mu, \Gamma, C)
  \Leftrightarrow
  \begin{pmatrix}
    \Re z \\ \Im z
  \end{pmatrix}
    \sim \mathcal{N}_{2 m}\biggl(
      \begin{pmatrix}
        \Re \mu \\ \Im \mu
      \end{pmatrix},
      \frac12
      \begin{pmatrix}
        \Re{(\Gamma + C)} & - \Im{(\Gamma - C)} \\
        \Im{(\Gamma + C)} &   \Re{(\Gamma - C)}
      \end{pmatrix}
    \biggr)
  \,.
\end{equation}
It is possible to recover the $\cplx$ covariance and relation matrices from a block covariance
matrix $\Sigma$ of a \textit{non-degenerate} paired real Gaussian vector through
$$
\begin{pmatrix}
  \Sigma_{rr} & \Sigma_{ri} \\ 
  \Sigma_{ir} & \Sigma_{ii} 
\end{pmatrix}
  = \frac12
    \begin{pmatrix}
      \Re{(\Gamma + C)} & - \Im{(\Gamma - C)} \\
      \Im{(\Gamma + C)} &   \Re{(\Gamma - C)}
    \end{pmatrix}
  \, \Leftrightarrow
  \,
  \begin{cases}
    \Gamma
      = \Sigma_{rr} + \Sigma_{ii} + j (\Sigma_{ir} - \Sigma_{ri})
    \,, \\
    C = \Sigma_{rr} - \Sigma_{ii} + j (\Sigma_{ir} + \Sigma_{ri}) 
  \end{cases}
  \,. $$

A $\cplx$-Gaussian random vector $z$ is called \textit{proper}, when $C = 0$, i.e.
the random vector $z$ and its conjugate $\conj{z}$ are uncorrelated. This implies
that in the paired real representation $
  \Sigma_{ir} = -\Sigma_{ri} = -\Sigma_{ir}^\top
$ and $\Sigma_{rr} = \Sigma_{ii}$, i.e. real and imaginary component of the same
random element are uncorrelated, since skew-symmetric matrices necessarily have
zero-diagonal.

The $\cplx$-Gaussian family of distrubutions is closed under affine transformations:
if $
  x \sim \mathcal{N}_m^{\cplx}(\mu, \Gamma, C)
$ and $y = A x + b$, for $
  A \in \cplx^{n\times m}
$ and $b \in \cplx^{n}$, then$
  y \sim \mathcal{N}_n^{\cplx}(A\mu + b, A \Gamma A^\hop, A C A^\top)
$. This follows from the efects of the affine transformation on the characteristic
function.


% subsection c_gaussian_distribution (end)

\subsection{Entropy of a $\cplx$-gaussian} % (fold)
\label{sub:entropy_of_a_c_gaussian}

The (differential) entropy of a multivariate Gaussian $
  x \sim q(x) = \mathcal{N}_n(x \vert \mu, \Sigma)
$ is
$$
- \mathbb{E}_{x\sim q(x)} \log q(x)
  % = \tfrac12 \log \det{(2 \pi \Sigma)}
  % + \tfrac12 \mathop{tr}{(
  %     \Sigma^{-1}
  %     \mathbb{E}_{x\sim q(x)}
  %       (x - \mu) (x - \mu)^\top
  %   )}
  = \tfrac12 \log \det{(2 \pi e \Sigma)}
  \,, $$
whence follows the expression for the entropy of a general $\cplx$-Gaussian random vector
$
  z \sim q(z)
    = \mathcal{N}^{\cplx}_n(z\vert \mu, \Gamma, C)
$
\begin{align}
\cdots
  &
  % = \tfrac12 \log \det{(2 \pi e \Sigma)}
  = \tfrac12 \log \det{\biggl(
    2 \pi e \tfrac12
    \begin{pmatrix}
      \Re{(\Gamma + C)} & - \Im{(\Gamma - C)} \\
      \Im{(\Gamma + C)} &   \Re{(\Gamma - C)}
    \end{pmatrix}
  \biggr)}
  \notag \\
  &
  = \begin{bmatrix}
    r_1 + j r_2 \to r_1 \\  % sufficient for C=0 case
    j r_1 + r_2 \to r_2     % j conj of transformation for r1, det x 2
  \end{bmatrix}
  = \tfrac12 \log \det{\biggl(
    % \det{[I & j I \\ j I & I]} = \det{I} \det{I - jI I^{-1} jI} = \det{2 I}
    % left apply : thus dividing by \sqrt{2}
    \tfrac1{\sqrt{2}} 2 \pi e \tfrac12
    \begin{pmatrix}
      \Gamma + C & j (\Gamma - C) \\
      j (\conj{\Gamma} + \conj{C}) & \conj{\Gamma} - \conj{C}
    \end{pmatrix}
  \biggr)}
  \notag \\
  &
  = \begin{bmatrix}
    c_1 - j c_2 \to c_1 \\  % -j conj of transformation for c2
    c_2 - j c_1 \to c_2     % sufficient for C=0 case
  \end{bmatrix}
  = \tfrac12 \log \det{\biggl(
    % \det{[I & - j I \\ - j I & I]} = \det{I} \det{I - (-j)I I^{-1} (-j)I} = \det{2 I}
    % right apply : thus dividing by \sqrt{2}
    \tfrac12 2 \pi e \tfrac12
    \begin{pmatrix}
      2 \Gamma     & - 2 j C \\
      2 j \conj{C} & 2 \conj{\Gamma}
    \end{pmatrix}
  \biggr)}
  \notag \\
  &
  = \begin{bmatrix}
    r_2 - j \conj{C} \Gamma^{-1} r_1 \to r_2
  \end{bmatrix}
  = \tfrac12 \log \det{\biggl(
    % \det{[I & 0 \\ - j \conj{C} \Gamma^{-1} & I]} = \det{I} = 1
    \pi e
    \begin{pmatrix}
      \Gamma & - j C \\
      0 & \conj{\Gamma} - \conj{C} \Gamma^{-1} C
    \end{pmatrix}
  \biggr)}
  \notag \\
  &
  = \tfrac12 \log \det{(\pi e \Gamma)} \det{(\pi e (\conj{\Gamma} - \conj{C} \Gamma^{-1} C))}
  \,,
\end{align}
implicitly using the fact that for a non-singular $\Omega$ we have $
  \det{A}
    = \tfrac{\det{(\Omega A)}}{\det{\Omega}}
    = \tfrac{\det{(A \Omega)}}{\det{\Omega}}
$. The entropy of a proper $\cplx$-Gaussian random vector ($C = 0$) is thus 
\begin{equation}  \label{eq:cn-proper-entropy}
\cdots
  % = \tfrac12 \log \det{(\pi e \Gamma)} \det{(\pi e (\conj{\Gamma} - \conj{C} \Gamma^{-1} C))}
  = \tfrac12 \log \det{(\pi e \Gamma)} \det{(\pi e \conj{\Gamma})}
  % = \tfrac12 \log \det{(\pi e \Gamma)} \conj{\det{(\pi e \Gamma)}}
  = \tfrac12 \log \bigl\lvert \det{(\pi e \Gamma)} \bigr\rvert^2
  = \log \bigl\lvert \det{(\pi e \Gamma)} \bigr\rvert
  \,,
\end{equation}
since $\det$ is a multilinear operator and commutes with complex conjugation.

% subsection entropy_of_a_c_gaussian (end)

\subsection{Kulback-Leibler divergence} % (fold)
\label{sub:kulback_leibler_divergence}

Recall that we use mean field variational approximation for the parameters: $
  W \sim q(W)
    = \bigotimes_{ij} q(\omega_{ij})
$ with proper complex Gaussian distributions of each element $
  q(\omega_{ij})
    = \mathcal{N}^{\cplx}\bigl(
      \omega_{ij} \big\vert
      \mu_{ij}, \sigma^2_{ij}, 0
    \bigr)
$. Assuming factorized prior belief $\pi(W)$ on the parameters $W$, the KL-divergence
term in ELBO \eqref{eq:elbo} is
\begin{equation}  \label{eq:elbo-general-kl-div}
KL(q(W) \| \pi(W))
  % = \mathbb{E}_{W \sim q(W)} \log \tfrac{q(W)}{\pi(W)}
  % = \sum_{ij} \mathbb{E}_{\omega_{ij} \sim q(\omega_{ij})}
  %   \log \tfrac{q(\omega_{ij})}{\pi(\omega_{ij})}
  = \sum_{ij} KL\bigl(
    q(\omega_{ij}) \| \pi(\omega_{ij})
  \bigr)
  \,.
\end{equation}
We shall omit the subscripts for brevity in the derivations in the next section .

\todo{why this is better than a general $\cplx$-gaussian? Three parameters?}

\subsubsection{Variational dropout} % (fold){}
\label{ssub:variational_dropout}

Let's get the divergence term for the improper prior, that most closely recovers the variational
dropout penalty term in \cite{kingma_variational_2015} and \cite{molchanov_variational_2017}.

If we assume $
  \pi(\omega) \propto \tfrac1{\lvert \omega \rvert^\beta}
$ for some $\beta > 1$, then we get the following KL-divergence term (up to an unknown, or
non-existent constant):
\begin{equation}  \label{eq:var-do-kl-div-raw}
KL(q\| \pi)
  \propto
  %   \mathbb{E}_{\omega \sim q(\omega)} \log q(\omega)
  %   + \tfrac{\beta}2 \mathbb{E}_{\omega \sim q(\omega)} \log \lvert \omega \rvert^2
  % =
  %   - \log \lvert \det{(\pi e \sigma^2)} \rvert
  %   + \tfrac{\beta}2 \mathbb{E}_{\omega \sim q(\omega)} \log \lvert \omega \rvert^2
  % =
    - \log{\sigma^2}
    + \tfrac{\beta}2 \mathbb{E}_{\omega \sim q(\omega)} \log \lvert \omega \rvert^2
  \,,
\end{equation}
where we have used \eqref{eq:cn-proper-entropy} with $\Gamma = \sigma^2 \geq 0$. Note
that for $\mu \neq 0$, we have $\omega \sim q(\omega)$ equal in distribution to $\mu \xi$
with $\xi \sim \mathcal{N}^{\cplx}(\xi \vert 1, \alpha, 0)$, whence
\begin{equation}  \label{eq:expect-improper-term}
\mathbb{E}_{\omega \sim q(\omega)} \log \lvert \omega \rvert^2
  % = \mathbb{E}_{\xi \sim \mathcal{N}^{\cplx}(\xi \vert 1, \alpha, 0)}
  %   \log \lvert \mu \xi \rvert^2
  % = \log \lvert \mu \rvert^2
  %   + \mathbb{E}_{\xi \sim \mathcal{N}^{\cplx}(\xi \vert 1, \alpha, 0)}
  %       \log \lvert \xi \rvert^2
  = \log \alpha \lvert \mu \rvert^2
    + \mathbb{E}_{\xi \sim \mathcal{N}^{\cplx}(\xi \vert 0, 1, 0)}
        \log{\bigl\lvert \tfrac1{\sqrt{\alpha}} + \xi \bigr\rvert^2}
  \,.
\end{equation}

In the case of $\real$-valued variational dropout \cite{kingma_variational_2015} approximate
\eqref{eq:expect-improper-term} by a non-linear regression, fit to the Monte-Carlo estimates
of the expectation for $\alpha$ varying over a logarithmic grid. This approximation was greatly
refined in \cite{molchanov_variational_2017} -- an improvement that enabled doing away with
the clipping of the dropout rate (relevance score) $\alpha$. In fact in the real case the analogue
of the last term in \eqref{eq:expect-improper-term} is half the log-moment of a non-central
$\chi^2$ distribution with shape $1$ and non-centrality $\lambda = \tfrac1{\alpha}$. Unfortunately,
there is no known expression (even involving special functions) for this expectation for
$\chi^2_m$ with odd shape parameter $m$ (degrees of freedom) \verify{this}. Quite fortunately
there is one for even degrees of freedom.

Let $(\mu_i)_{i=1}^m \in \cplx$ and $
  (z_i)_{i=1}^m \sim \mathcal{N}^{\cplx}(0, 1, 0)
$ iid. Then $W = \sum_i \lvert \theta_i + z_i \rvert^2$ has a non-central $\chi^2_{2m}$
distribution with non-centrality parameter $s^2 = \sum_i \lvert \theta_i \rvert^2$. The
log-moments of $W$ for general integer $m \geq1$ are given in
\cite[p.~2248]{lapidoth_capacity_2003}.  % in fact Appendix X, Lemma 10.1
In particular, for $m=1$ and $\theta\in \cplx$ we have
\begin{equation}  \label{eq:log-moment-for-chi-2}
\mathbb{E}_{z \sim \mathcal{N}^{\cplx}(0, 1, 0)}
  \log \lvert \theta + z \rvert^2
  = \log \lvert \theta \rvert^2 - \mathop{Ei}( - \lvert \theta \rvert^2)
  \,,
\end{equation}
where $\mathop{Ei}(x) = \int^x_{-\infty} t^{-1} e^t dt$ for $x < 0$ is the exponential integral
function. Note that the limiting value of \eqref{eq:log-moment-for-chi-2} as $
  \lvert \theta \rvert \to 0
$ is $- \gamma$, i.e. negative Euler's constant. Therefore after cancellation the expression
of the KL-divergence in \eqref{eq:var-do-kl-div-raw} becomes
\begin{equation}  \label{eq:var-do-kl-div}
KL(q\| \pi)
  \propto
  %   - \log \lvert \det{(\pi e \sigma^2)} \rvert
  %   + \tfrac{\beta}2 \log \alpha \lvert \mu \rvert^2
  %   + \tfrac{\beta}2 \mathbb{E}_{\xi \sim \mathcal{N}^{\cplx}(\xi \vert 0, 1, 0)}
  %       \log{\bigl\lvert \tfrac1{\sqrt{\alpha}} + \xi \bigr\rvert^2}
  % =
  %   - \log{\sigma^2}  % - \log{\pi e}
  %   + \tfrac{\beta}2 \log \alpha \lvert \mu \rvert^2
  %   + \tfrac{\beta}2 \bigl(
  %     - \log \alpha - \mathop{Ei}( -\tfrac1{\alpha})
  %   \bigr)
  % =
    % \log \tfrac{\lvert \mu \rvert^\beta}{\sigma^2}
    % \log \tfrac{\lvert \mu \rvert^\beta}{\alpha \lvert \mu \rvert^2}
    \tfrac{\beta-2}2 \log{\lvert \mu \rvert^2}
    + \log{\tfrac1{\alpha}}
    - \tfrac{\beta}2 \mathop{Ei}(- \tfrac1{\alpha})
  \,.
\end{equation}

The exponential integral is readily available in most scientific computations packages nowadays,
and, what is especially useful for the stochastic variational Bayes purposes, has a simple
expression for the derivative: $x \mapsto \tfrac{e^x}{x}$ for $x < 0$. This observation enables
embedding it into autograd software by computing the term with bogus output values, yet correct
(on device) gradient, provided no downstream computations depend on the value of the KL-divergence
term itself.
%
\important{``pytorch'' specific}
Otherwise we have to offload the values from the device to cpu, compute the special
function there, and transfer back.

% subsubsection variational_dropout (end)

\subsubsection{Empirical Bayes} % (fold)
\label{ssub:empirical_bayes}

For this model we use an empirical Bayes prior. We assume that our initial belief is that
parameters are independent and each one is proper $\cplx$-Gaussian with its own precision: $
  \pi(\omega)
    \propto \mathcal{N}^{\cplx}\bigl(
      \omega \vert 0, \tau^{-1}, 0
    \bigr)
$. The per element KL-divergence term in the ELBO is computed thus:
\begin{equation}  \label{eq:emp-bayes-kl-div}
KL(q \| \pi_\tau)
  = - 1 - \log{(\tau \sigma^2)}
    + \tau \bigl(
      \sigma^2 + \lvert \mu \rvert^2
    \bigr)
  \,,
\end{equation}
which follows from the KL-divergence expression for two multivariate Gaussians
$$
% \mathbb{E}_{\omega\sim q_1(\omega)} \log \tfrac{q_1(\omega)}{q_2(\omega)}
%   =
KL(q_1\| q_2)
  =
  % - \tfrac12 \log\det{(2\pi e \Sigma_1)} + \tfrac12 \log\det{(2\pi \Sigma_2)}
  - \tfrac12 \log \det{e I}
  + \tfrac12 \log \frac{\det{\Sigma_2}}{\det{\Sigma_1}}
  + \tfrac12 \tr{\bigl( \Sigma_2^{-1} \Sigma_1 \bigr)}
  + \tfrac12 (\mu_1 - \mu_2)^\top \Sigma_2^{-1} (\mu_1 - \mu_2)
  \,. $$

Somewhat counterintuitively, in empirical Bayes we optimize the hyperparameters of the
prior on the observed dataset. Thus the lower bound \eqref{eq:elbo} is optimized w.r.t.
$\tau$ of each weight's prior. \eqref{eq:emp-bayes-kl-div} is optimized (minimized) at
$\tau^\ast = (\sigma^2 + \lvert \mu \rvert^2)^{-1}$, yielding the following penalty in
ELBO:
\begin{equation}  \label{eq:emp-bayes-opt-kl}
KL(q \| \pi_{\tau^\ast})
  % = - 1 - \log{({\tau^\ast} \sigma^2)}
  %   + {\tau^\ast} \sigma^2 + {\tau^\ast} \lvert \mu \rvert^2
  % = \log{((\sigma^2 + \lvert \mu \rvert^2) \tfrac1{\sigma^2})}
  = \log{\bigl(1 + \tfrac{\lvert \mu \rvert^2}{\sigma^2}\bigr)}
  % = \log{\bigl(1 + \tfrac1\alpha \bigr)}
  \,,
\end{equation}
which exactly replicates the term, derived in \cite{kharitonov_variational_2018}.

% subsubsection empirical_bayes (end)

% subsection kulback_leibler_divergence (end)

\subsection{Local reparameterization trick} % (fold)
\label{sub:local_reparameterization_trick}

Consider the mean field complex gaussian variational approximation $q(W)$ for the weight
parameters of the linear layer: $W \in \cplx^{n\times m}$ is a random $\cplx$-gaussian
matrix with independent elements $
  w_{ij} \sim \mathcal{N}^{\cplx}\bigl(
    w_{ij}
    \big \vert
    \mu_{ij}, \sigma^2_{ij}, C_{ij}
  \bigr)
$ with $\mu_{ij}\in cplx$, $\sigma^2_{ij} \geq 0$, $C_{ij}\in \cplx$ and $
  \lvert C_{ij} \rvert^2 \leq \sigma^2_{ij}
$.

We begin by observing that the variational approximation of $q(W)$ is a mutlivariate
$\cplx$-gaussian distribution on $
  \vec{W}
    \sim \mathcal{N}^{\cplx}_{[n\times m]} \bigl(
      \vec{\mu}, \diag{\vec{\Sigma}}, \diag{\vec{C}}
    \bigr)
$, where $[n\times m]$ denotes lexicographically flattened dimension and $\diag{v}$
is a diagonal square matrix with $v_i$ on the main diagonal. Now, for any $x \in \cplx^m$
and $b\in \cplx^n$ we have
$$
W x + b
  % = I_n W x + b
  = (I_n \otimes x^\top) \vec{W} + b
  \,, $$
where $\vec{\cdot}$ denotes flattening in lexicographic order of indices (row-major),
and $\otimes$ denotes the Kronecker product, for which we have the identity $
  \vec{P Q R} = (P \otimes R^\top) \vec{Q}
$, \cite{petersen_matrix_2012}.
% It follows from the definition of the Kronecker product
% of $P$ and $R^\top$ and the row-major order vectorization.
%
Structure on the invloved matrices and the identiy $
  (P \otimes R) (C \otimes S) = P Q \otimes R S
$, \cite{petersen_matrix_2012}, imply
\begin{align}
(I \otimes x^\top) \diag{\vec{\Sigma}} (I \otimes x^\top)^\hop
  % &= \sum_{ij} (I \otimes x^\top) e_i \otimes e_j
  %   \sigma^2_{ij} (e_i \otimes e_j)^\top (I \otimes x^\top)^\hop
  % \\
  &
  = \sum_{ij} (e_i \otimes x^\top e_j)
    \sigma^2_{ij} (e_i \otimes x^\top e_j)^\hop
  \notag \\
  % &= \sum_{ij} (e_i \otimes x_j) \sigma^2_{ij} (e_i^\top \otimes \conj{x}_j)
  % \\
  &
  = \sum_i (e_i e_i^\top) \sum_j \sigma^2_{ij} x_j \conj{x}_j
  \,,
\end{align}
where $e_i$ is the $i$-th real unit vector of dimensionality conforming to the
expression it is involved with. In a similar fashion we obtainin the expression
for the relation matrix of the transformation:
\begin{equation}
(I \otimes x^\top) \diag{\vec{C}} (I \otimes x^\top)^\top
  % = \sum_{ij} (e_i \otimes x_j) C_{ij} (e_i \otimes x_j)^\top
  = \sum_i e_i e_i^\top \sum_j C_{ij} x_j^2
  \,.
\end{equation}
The term $
  \sum_i e_i e_i^\top (\ldots)
$ means that the covariance and relation matrices of $y = Wx + b$ are diagonal.

These observations coupled with the closure under affine transformation imply
that the components of $y = W x + b$ are independent univariate $\cplx$-gaussain
random variables with distributions
$$
y_i
  \sim \mathcal{N}^{\cplx}
    \bigl(
      e_i^\top \mu x + b,
      \, \sum_j \sigma^2_{ij} \lvert x_j \rvert^2,
      \, \sum_j C_{ij} x_j^2
    \bigr)
  \,. $$

Gist of the LRP (kingmaetal2015)
translating uncertainty (randomness) from parameters to activations
  adds noise -- helps regularization
  emulates one draw per batch element -- decorrelates gradient within the batch
  reduces computations (memory), leverages batched matmuls (convos)
  decreases variance of the gradient

additive noise reparameterization (molchanov\_variational\_2017) $(\mu, \alpha \mu^2)$:
  less noisy gradient wrt $\mu$, $\alpha$ recomputed on-the-fly

% subsection local_reparameterization_trick (end)

\subsection{Alternative VI} % (fold)
\label{sub:alternative_vi}

In this section we will consider alternative variational approximations $q(W)$ for
the posterior distribution.

\subsubsection{Real scaling dropout} % (fold)
\label{ssub:real_scaling_dropout}

Consider the following parameterization of the parameters $W$: $
  W = \mu \odot \xi
$ where $
  \xi_{ij} \sim \mathcal{N}(1, \alpha_{ij})
$, yet $\mu \in \cplx^{n \times m}$. This case corresponds to variational inference for
the mutliplicative mask $\xi$, distribution of which is approximated using independent
$\real$ univariate gaussians (oridinary dropout posteriors). Here we are actually
interested in iference regaridn the values of $\xi$ after observing the data (and
fitting other model parameters such as $\mu$). Essentially, in \eqref{eq:elbo}
we have
$$
\mathbb{E}_{\xi \sim q(\xi)}
  \log p_\theta(z \vert \xi)
  - KL(q(\xi) \| \pi(\xi))
  \approx
    N \hat{\mathbb{E}}_{(z, \xi) \sim B}
      \log p_\theta(z \vert \xi)
    - KL(q(\xi) \| \pi(\xi))
  \,, $$
and by running SGVB, i.e. maximizing stochastic batched ELBO, we minimize the KL-divergence
of the variational approximation $q(\xi)$ from its true posterior $p(\xi\vert D)$. Thus
we run inference on the relevance factors of the parameters, rather than parameters
themselves.

The KL-divergence penalty term in this case follows form the penalty of the real-gaussian
case with an improper prior $\pi(\xi)\propto \lvert \xi \rvert^{-\beta}$:
$$
KL(q(\xi)\| \pi(\xi))
  % = - \tfrac12 \log 2\pi e \alpha
  %   + \tfrac\beta2 \log \alpha
  %   + \tfrac\beta2 \mathbb{E}_{z\sim \mathcal{N}(0, 1)}
  %     \log \lvert z + \tfrac1\sqrt{\alpha} \rvert^2
  \propto \tfrac{\beta-2}2 \log \alpha
    + \tfrac\beta2 \mathbb{E}_{z\sim \mathcal{N}(0, 1)}
      \log \lvert z + \tfrac1{\sqrt{\alpha}} \rvert^2
  \,. $$
Under this parameterization \verify{we have},
$$
W_{ij} = \mu_{ij} \odot \xi_{ij}
  \sim \mathcal{N}^{\cplx} \bigl(
    \mu_{ij},
    \, \alpha_{ij} \lvert \mu_{ij} \rvert^2,
    \, \alpha_{ij} \mu_{ij}^2
  \bigr)
  \,, $$
which is a degenerate $\cplx$-gaussian. The local reparameterization trick for this
distribution implies
$$
y_i
  \sim \mathcal{N}^{\cplx}\bigl(
    e_i^\top \mu x + b,
    \, \sum_j \alpha_{ij} \lvert x_{ij} \mu_{ij}\rvert^2,
    \, \sum_j \alpha_{ij} (x_{ij} \mu_{ij})^2,
    \,
  \bigr)
  \,. $$
This means that it is a little bit more expensive to translate the local parameter
noise to the immediate activation, since we need to take into consideration the covariance
between complex outut and its conjugate. Due to this, additive noise reparameterization
trick, \cite{molchanov_variational_2017}, cannot provide its variance reducing benefits,
since it is \verify{not possible} to do away with dependence of the complex relation
of the key parameter $\mu_{ij}$.

% subsubsection real_scaling_dropout (end)

% subsection alternative_vi (end)

% section trunk (end)

\clearpage

\bibliographystyle{amsplain}
\bibliography{references}
\nocite{*}

\end{document}