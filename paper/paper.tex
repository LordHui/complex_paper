\documentclass[a4paper,10pt]{article}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{grffile}

\usepackage{subcaption}

\usepackage{booktabs}

% \usepackage{lipsum}

\newcommand{\important}[1]{\textbf{\color{red} #1}}
\newcommand{\attn}[2]{\textbf{\color{red} #2~\textsuperscript{\textit{[#1]}}}}
\newcommand{\verify}[1]{\attn{verify}{#1}}
\newcommand{\rewrite}[1]{\attn{rewrite}{#1}}
\newcommand{\todo}[1]{{\color{blue} [TODO]} \important{#1}}

\newcommand{\real}{\mathbb{R}}
\newcommand{\hop}{\ast}
\newcommand{\cplx}{\mathbb{C}}

\renewcommand{\vec}{\overrightarrow}

\title{Sparsifying $\cplx$-valued networks}
\author{Ivan Nazarov}

\begin{document}
\maketitle

% {\color{red} \lipsum[1-3]}

\section{Introduction} % (fold)
\label{sec:introduction}

Why sparsity matters, why $\cplx$ networks?

Where are $\cplx$-networks used? and do these applications warrant sparsity?

% section introduction (end)

\section{$\cplx$-valued networks} % (fold)
\label{sec:c_valued_networks}

In this section i remind the reader what the complex-valued networks are, how
they are implemented and how much of $\cplx$-ness they have. Mention careful
design.

I will describe the linear layers (dense, masked, and convolutions) and
activations borrowed straight from the $\real$-values networks. Then i shall
mention that this has been tried before and list papers that one way or another
address $\cplx$-data processing with neural networks.

\cite{trabelsi_deep_2017} provide a set of building blocks for deep $\cplx$-valued
networks. The core of their work is structured framework for complex-valued operations
including representation and linear layers (dense and convolutional) as well
as $\cplx$-valued activations and complex batch-normalization and weight initialization.

Following \cite{trabelsi_deep_2017} we identify $\cplx$ with $\real^2$ via $
  z_\mathrm{r} + j z_\mathrm{i} \mapsto (z_\mathrm{r}, z_\mathrm{i})
$, storing the real and imaginary parts in interleaved format, and emulate $\cplx$-algebra
using $\real$-valued arithmetic. Thus a $\cplx$-network is, essentially, a $\real$-network
with \textit{double} the layers' widths to accommodate real and imaginary parts and
with constraints on layers' parameters that make them respect $\cplx$-algebra. Thus
$\cplx$-valued networks can be retrofitted into or built upon the existing $\real$-valued
autograd libraries.

In particular, common linear layers, such as dense layers and convolutions,
$
  L \colon \cplx^{\mathrm{[in]}}
    \to \cplx^{\mathrm{[out]}}
$ act upon their inputs thus:
$$
\real^{\mathrm{[in]} \times 2}
  \to \real^{\mathrm{[out]} \times 2}
  \colon (x_r, x_i)
    \mapsto \Bigl(
      L_r x_r - L_i x_i,
      L_r x_i + L_i x_r
    \Bigr)
  \,, $$
where $
  L_r, L_i
    \colon \real^{\mathrm{[in]}}
      \to \real^{\mathrm{[out]}}
$ are the unique $\real$-linear operators that make up $L = L_r + j L_i$ the overall
effect of the layer. For instance, to respect $\cplx$-arithmetic a linear layer acting
on $x$ should have $W_{ri} = - W_{ir}$ and $W_{rr} = W_{ii}$ in
$$
  \begin{pmatrix}
    y_r \\ y_i
  \end{pmatrix}
    = \begin{pmatrix}
      {\color{red} W_{rr}} & {\color{blue} W_{ri}} \\ 
      {\color{blue} W_{ir}} & {\color{red} W_{ii}}
    \end{pmatrix}
    \begin{pmatrix}
      x_r \\ x_i
    \end{pmatrix}
    + \begin{pmatrix}
      b_r \\ b_i
    \end{pmatrix}
  \,. $$
Other algebraic operations that might be used in a networks are also designed in such a way as
to respect $\cplx$-operations. It is worth pointing out that the key advantage of the $\cplx$-%
constraints is that they permit use of Strassen's $3$M multiplication algorithm, potentially
saving up to $25\%$ on floating point multiplications.

% proof that such operators exist and are unique (well this is an obvious statement)
Consider $L\colon \cplx^m \to \cplx^n$ -- linear in $\cplx$. Then $
  L(u + jv) = L u + j L v
$ for any $u, v \in \real^m$, which implies that the effect of $L$ as $\cplx$-linear
operator is determined by its restriction onto $\real^n$. Let $
  F = L\vert_{\real^m}
  \colon \real^m \to \cplx^n
$ and observe that $F_r = \Re \circ F$ and $F_i = \Im \circ F$ are $\real$-linear operators
such that $F = F_r + j F_i$ (pointwise). Indeed,
$$
  F_r(a + \lambda b)
  % = \Re F(a + \lambda b)
  = \Re L(a + \lambda b)
  % = \Re \bigl( L a + \lambda L b \bigr)
  = \Re L a + \lambda \Re L b
  % = \Re F a + \lambda \Re F b
  = F_r a + \lambda F_r b
  \,. $$
Therefore for any $\cplx$-linear operator $L$ there are $\real$-linear operators $U, V$
such that
$$
L z 
  = (U + j V) (\Re z + j \Im z)
  % = (U + j V) \Re z + j (U + j V) \Im z
  % = U \Re z + j V \Re z + j \bigl(U \Im z + j V \Im z \bigr)
  = U \Re z - V \Im z + j \bigl( V \Re z + U \Im z \bigr)
  % = (U + j V) z
  \,. $$
Uniqueness of these operators follows, if this decomposition is applied to any $z$ with
$\Im z = 0$.

As far as non-linearities are concerned, \cite{trabelsi_deep_2017} consider a few truly
complex-valued ones, that are nevertheless not $\cplx$-differentiable: the most frequently
used activations are the common $\real$-valued non-linearities applied to real and
imaginary components independently.

\todo{something about activations}
modRelu -- relu on the modulus of a complex value (preserves the angle)
$
\mathrm{modReLU}
  \colon z \mapsto z \bigl(
    1 - \tfrac\theta{\lvert z \rvert}
  \bigr)_+
$
$
\mathrm{modReLU}
  \colon r e^{i \phi} \mapsto e^{i \phi} (r - \theta)_+
$

\todo{finish activations}

\subsection{Backprop through $\cplx$-networks} % (fold)
\label{sub:backprop_through_c_networks}

The question of differentiability of complex-valued networks is tricky, since they usually
use non-differentiable non-linearities. However, a more severe problem stems from the loss
objective being real-valued. Indeed, if $f\colon \cplx \to \cplx$ were $\cplx$-differentiable
(holomorphic), the composition of $f$ with a non-trivial $h \colon \cplx\to \real$ makes it
not $\cplx$-differentiable with respect parameters, since $h$ violates Cauchy-Riemann conditions.
This problem is commonly dealt with by using Wirtinger, or $\cplx\real$ calculus, which subsumes
$\cplx$-calculus, but is also applicable to non-holomorphic functions of $\cplx$-argument,
\cite{adali_complex-valued_2011} and \cite{trabelsi_deep_2017}. In essence, this approach
views $f$ as a function of two independent variables $z$ and its conjugate $z^\hop$ and
symbolically redefines the derivatives appropriately. This approach naturally extends the
chain rule to non-holomorphic functions and can be directly reformulated in terms of derivatives
with respect to real and imaginary parts, making it possible to use and apply the rules of
$\real$-calculus to the networks represented in double-$\real$ fashion and use existing
$\real$-valued backpropagation in autograd software.

% essential intro into Wirtinger calculus (CR)
Take $f\colon \cplx \to \cplx$ and identify it with $F\colon \real^2 \to \cplx$ function
via $F(u, v) = f(z)\vert_{z=u + j v}$. Wirtinger calculus introduces two formal derivative
operators
$
  \tfrac{\partial}{\partial z}
    = \tfrac12 \bigl(
      \tfrac{\partial}{\partial u}
      - j \tfrac{\partial}{\partial v}
    \bigr)
$ and $
  \tfrac{\partial}{\partial z^\hop}
    = \tfrac12 \bigl(
      \tfrac{\partial}{\partial u}
      + j \tfrac{\partial}{\partial v}
    \bigr)
$, defines $dz = du + j dv$, $dz^\hop = du - j dv$, and the differential of $f$ as
$$
df = \tfrac{\partial f}{\partial z} dz
    + \tfrac{\partial f}{\partial z^\hop} dz^\hop
   % = \tfrac12\bigl(
   %    \tfrac{\partial}{\partial u}
   %    - j \tfrac{\partial}{\partial v}
   % \bigr) F (du - j dv)
   % + \tfrac12\bigl(
   %    \tfrac{\partial}{\partial u}
   %    + j \tfrac{\partial}{\partial v}
   % \bigr) F (du - j dv)
   % = \tfrac12 \bigl(
   %    \tfrac{\partial F}{\partial u} du - j \tfrac{\partial F}{\partial v} du
   %    + j \tfrac{\partial F}{\partial u} dv + \tfrac{\partial F}{\partial v} dv
   % \bigr)
   % + \tfrac12 \bigl(
   %    \tfrac{\partial F}{\partial u} du + j \tfrac{\partial F}{\partial v} du
   %    - j \tfrac{\partial F}{\partial u} dv + \tfrac{\partial F}{\partial v} dv
   % \bigr)
   = \tfrac{\partial F}{\partial u} du
     + \tfrac{\partial F}{\partial v} dv
   = dF
  \,. $$
Thus the complex value and its conjugate are treated as independent variables. In this notation
Cauchy-Riemann conditions are expressed as $
  \tfrac{\partial f}{\partial z^\hop} = 0
$, or $
  -j\tfrac{\partial F}{\partial v} = \tfrac{\partial F}{\partial v}
$. Cauchy-Riemann impose a rigid structure on real and imaginary parts of $F(u, v)$, namely $
  \tfrac{\partial F_{\Re }}{\partial u} = \tfrac{\partial F_{\Im }}{\partial v}
$ and $
  \tfrac{\partial F_{\Re }}{\partial v} = - \tfrac{\partial F_{\Im }}{\partial u}
$. Thus Wirtinger calculus subsumes the usual $\cplx$-calculus, when $
  \tfrac{\partial f}{\partial z^\hop} = 0
$, i.e. $
  f(z)
    % = g(z, z^\hop)
    = F(\tfrac12 (z + z^\hop), \tfrac1{j 2} (z - z^\hop))
$ does not depend on $z^\hop$.

For a real-valued $f\colon \cplx \to \real$ we have $f^\hop = f$, whence $
  \tfrac{\partial f}{\partial z^\hop}
    = \tfrac{\partial f^\hop}{\partial z^\hop}
    = \bigl(\tfrac{\partial f}{\partial z} \bigr)^\hop
$. Hence
$$
df
  = \tfrac{\partial f}{\partial z} dz
    + \tfrac{\partial f}{\partial z^\hop} dz^\hop
  % = \bigl(
  %   \tfrac{\partial f^\hop}{\partial z^\hop}
  % \bigr)^\hop dz
  % + \bigl(
  %   \bigl( \tfrac{\partial f}{\partial z^\hop} \bigr)^\hop dz
  % \bigr)^\hop
  = 2 \Re \Bigl(
    \bigl( \tfrac{\partial f}{\partial z^\hop} \bigr)^\hop dz
  \Bigr)
  = 2 \Re \bigl(
    \tfrac{\partial f}{\partial z} dz
  \bigr)
  \,. $$
Hence the gradient, being the direction of \rewrite{maximal growth} of $f$ at $z$ is given by $
  \tfrac{\partial f}{\partial z^\hop}
    = \tfrac{\partial F}{\partial u}
      + j \tfrac{\partial F}{\partial v}
$. Thus under Wirtinger calculus it is possible to reuse $\real$-backpropagation.

% Show that $dh(z) = df(g(z)) dg(z)$.
% $h(z) = f(g(z)) = F(G_r(u, v), G_i(u, v)) = H(u, v)$
% $$
% dH
%   = \tfrac{\partial F}{\partial x} \tfrac{\partial G_r}{\partial u} du
%   + \tfrac{\partial F}{\partial x} \tfrac{\partial G_r}{\partial v} dv
%   + \tfrac{\partial F}{\partial y} \tfrac{\partial G_i}{\partial u} du
%   + \tfrac{\partial F}{\partial y} \tfrac{\partial G_i}{\partial v} dv
%   = \tfrac{\partial F}{\partial r} d\Re g(z)
%   + \tfrac{\partial F}{\partial i} d\Im g(z)
%   = df(\Re g(z) + j\Im g(z)) d g(z)
%   = df(g(z)) d g(z)
%   \,. $$
% subsection backprop_through_c_networks (end)

% section c_valued_networks (end)

\section{Related research} % (fold)
\label{sec:realted_research}

\subsection{On DPD} % (fold)
\label{sub:on_dpd}

Radio Frequency power amplifiers, being physical electronic devices, have imperfections such
as parasitic inductance and capacitance. This coupled with them being operated in saturation,
make their gain respond non-linearly to the input signal, adding undesirable interference and
spectral spreading. Although these effects can be minimized by careful circuit design, they
cannot be eliminated entirely \cite{citation_needed}. Digital signal predistortion aims at
linearizing the amplifier, by introducing signal-dependent compensating component to the input.
At the same time the non-linearities should be corrected on-device efficiently and with low
latency in order to reduce cross-channel interference and enable higher bandwidth utilization
in wireless transmitters.

\cite{traverso2019} uses \rewrite{some group delay estimators of something};
\cite{schoukens2016} develops an iterative approach to inference of the predistortion for a given
signal \todo{unclear};

\cite{traver2019} models the predistorter as a paired real dense shallow neural network and
demonstrates viability and competitiveness of this approach against polynomial based DPD on an
FPGA implementation.

% About rfWEbLab
The ``RFWebLab'' platform is remote RF power amplifier setup with web-based API, that feeds
the user-submitted quadrature signal through a class AB GaN amplifier and sends-back the measured
output \cite{dpdcompetiton2018}. It is maintained by the GHz Centre at Chalmers University of
Technolgy and National Instruments \cite{landin_rfweblab_2015}.

We use a custom-written python interface to the WebLab setup that is compatible with the MATLAB
interface form the setup owner.


% subsection on_dpd (end)

% section realted_research (end)

\section{Complex Variational Dropout} % (fold)
\label{sec:complex_varaitional_dropout}

In this section we outline and derive the variational dropout technique for complex-%
valued networks, which re-traces the evolution of dropout technique for real-valued
weights.

Variational dropout proposed by \cite{kingma_variational_2015} enables automatic parameter
relevance detection by learning individual per-parameter dropout rates in Gaussian dropout,
\cite{srivastava_dropout_2014,wang_fast_2013}, which generalizes Binary (Bernoulli) Dropout
\cite{hinton_improving_2012}.

\begin{quote}
% exceprts from kingmaetal2015
Bernoulli dropout alleviates the problem of overfitting
Gaussian dropout is in fact a variational approximation with fixed rate and thus optimizes ELBO
Allowing per-parameter rates enables flexible posterior approximations.

Bayesian posterior inference over the parameters of a network allows some control of overfitting;
``exact inference is intractable, but efficient approximation schemes can be designed''

Explore a trick that translates the uncertainty about global parameters into local noise, that
is independent across the elements of the minibatch.

Effectively regularizes by adding random noise.

bayesian inference: update prior belief using model likelihood of iid data into posterior belief
Exact psoterior -- intractable, approximations needed
variational inference: cast posterior inference as optimization over the approximation family

%sgvb
the gradient in sgvb is unbiased estimate of the $\log$-likelihood part of the elbo

in practice the performance of stochastic gradient methods depends on the variance of the gradient
field. The variance of (the value of) the minibatch estiamte of the likelihood term has a
non-negligible intrabatch covariance term. To ensure zero covariance we can sample a separate
$\omega\sim q_\omega(\theta)$ for each elemnt in the minibatch, 

%lrt
whenever source of uncertainty can be translated to local noise in the intermediate states
since the expected likelihood depends on the parameters through the activations of the layer.

sampling the activations directly, without the parameter randomness gives a low cost efficient
MC estimator (statistical efficiency).

Factorized Gaussian -- closure under linear transformations -- sample the local activation noise
from an appropriate Gaussian 

% on dropout
variational dropout: a reinterpretation of dropout with continuous noise as a variational inference
method. allow adaptive,data dependent dropout rates.

\end{quote}


% a comment on Bernoulli dropout
The key idea of Bernoulli dropout is to randomly mask input features of a layer during
training with in order to learn decorrelated feature maps. In a recent study of network
capacity for multitask learning, \cite{multitask2019}, it was shown, that associating
each dataset (task) with a context, that masks a subset of the parameters, allows non-%
destructive `packing' of the tasks in orthogonal on average `subsets' of a single set
of weights, without much interference between tasks. This lends support for beneficial
effects of Bernulli Dropout on test performance, since, in effect, the network learns
to perform well on identical copies of the same task with different (binary) contexts
with a single parameter set.

% exposition
To simplify exposition of the proposed technique we shall consider the linear dense layer,
and afterwards outline the caveats for convolutional layers. We begin with $\real$-valued
layer and later focus on $\cplx$-valued layer.

, that helps with model overfitting.
% plan

A linear layer is parameterized by $W \in\real^{n \times m}$ and bias $b\in \real^n$ and
acts on its input $x\in \real^m$ via $y = W x + b$ with $y\in \real^n$. The key idea of
Bernoulli dropout is to mask elements in the matrix $W$ of the layer
with some given probability $p$: $W = \theta \odot \xi$ with $\theta$ being the learnt
weight matrix, and $
  \xi \sim \mathcal{B}(\{0, \tfrac1{1-p}\}, 1-p)
$ -- an iid Bernoulli mask. During training, a dropout mask $\xi$ is used to compute $W$,
and during evaluation the weights are fixed to the expectation of $W$ which is $\theta$.
The discussion and analysis in \cite{kingma_variational_2015}, suggest that drawing one
common dropout mask for all data in the minibatch reduces statistical efficiency the estimator
of the weights' gradient in the stochastic approximation of the evidence lower bound. 

Gaussian dropout works in a similar fashion, except the dropout mask $\xi$ is iid $
  \mathcal{N}(1, \alpha=\tfrac{p}{1-p})
$, i.e. a multiplicative Gaussian noise is injected into the network.

The key idea of Variational Dropout is to assume a Gaussian Mean field variational
approximation of the posterior distribution of linear layer's weights (or convolutional
kernels, with a caveat\footnote).  \footnotetext{
  at each spatial patch of input data the kernel is independently redrawn form the
  VI distribution. This again helps with gradient variance reduction.
}
Weight $W$ are assumed to have independently distributed elements with $
  w_{ij} \sim \mathcal{N}(\theta_{ij}, \sigma^2_{ij})
$.

In fact multiplicative Gaussian dropout on weights $W$ is in fact itself a variational
approximation with variance $\sigma^2_{ij} = \theta_{ij}^2 \tfrac{p}{1-p}$ determined
by the dropout rate.

% set by step we retell the story of kingmaetal2015

Gaussian variational approximation enabled \cite{kingma_variational_2015} propose a
\textit{local reparameterization trick}, which reduces the variance of the gradients
in SGVB and does not resample $W$ per element of a minibatch. It relies the closure
of Gaussian distribution under linear transformations. Indeed, if $W$ is a Gaussian
matrix with independently distributed $
  W_{ij} \sim \mathcal{N}(\theta_{ij}, \sigma^2_{ij})
$, then we have
$$
y 
  = W x + b
  = I W x + b
  = (I \otimes x^\top) \vec{W} + b
  \,, $$
where $\vec{\cdot}$ denotes flattening in lexicographic order of indices (row-major),
for which we have the identity $\vec{A B C} = (A \otimes C^\top) \vec{B}$, \cite{cookbook2012}.
% It follows from the definition of the Kronecker product of $A$ and $C^\top$ and the
% row-major order vectorization.
From $\vec{W} \sim \mathcal{N}_{[n\times m]}(\vec{\theta}, \mathop{diag}\Sigma)$ we get
the following distribution
$$
y \sim \mathcal{N}_{n}
  \bigl(
    \theta x + b,
    \sum_{i} e_i e_i^\top \sum_j \sigma^2_{ij} x_j^2
  \bigr)
  \,, $$
where the variance follows from $(A\otimes B) (C\otimes D) = AC\otimes BD$, \cite{cookbook2012}:
$$
\ldots
  = (I \otimes x^\top) \mathop{diag}{\vec{\Sigma}} (I \otimes x^\top)^\top
  % = \sum_{ij} (I \otimes x^\top) e_i \otimes e_j
  %   \sigma^2_{ij} (e_i \otimes e_j)^\top (I \otimes x^\top)^\top
  = \sum_{ij} e_i (x^\top e_j) \sigma^2_{ij} (x^\top e_j) e_i^\top
  = \sum_{i} e_i e_i^\top \sum_j \sigma^2_{ij} x_j^2
  \,, $$
with $e_i$ being the $i$-th unit vector of dimensionality conforming to the expression
is is involved with.


and instead
translating the uncertainty from the weights into local output noise of each linear layer.
Without this it would have been necessary to draw new set of random weights per each element
in a mini-batch.

$$
  \mathcal{N}(\theta_{ij}, \alpha \theta^2_{ij})
  \overset{\mathcal{D}}{\sim}
  \theta_{ij} \mathcal{N}(1, \alpha)
  \,, $$

\cite{molchanov_variational_2017} propose additional step in the local reparametrization,
which further reduces the variance of the gradient estimator for each dropped out
weight. Their simple formal change of variables $(\theta, \alpha) \to (\theta, \sigma^2)$
decouples the stochastic component from the weights \rewrite:
$$
  w_{ij} = \theta_{ij} + \theta_{ij} \sqrt{\alpha} \varepsilon_{ij}
  \,\to\,
  w_{ij} = \theta_{ij} + \sigma^2_{ij} \varepsilon_{ij}
  \,, $$
with the appropriate change of variables in the Kullback-Leibler divergence.
($\alpha_{ij} = \tfrac{\sigma_{ij}^2}{\lvert \theta_{ij}\rvert^2}$).

Criticism of \cite{gale_state_2019} implies that $\ell_0$-variational dropout,
proposed by \cite{louizos_learning_2017} and based on the concrete binary distribution
of \cite{maddison_concrete_2016}, works consistently better than the variational
Gaussian dropout.

Maths and related results \cite{pav_moments_2015,taubock_complex-valued_2012},
and \cite{karseras_caution:_nodate}

% section complex_varaitional_dropout (end)

\section{Experiments} % (fold)
\label{sec:Experiments}

I this section we will compare the complex variational dropout techniques, discussed
above, on the tasks and datasets, that were studied in prior research on complex-%
valued networks.

\subsection{Tasks and datasets} % (fold)
\label{sub:tasks_and_datasets}

\cite{trabelsi_deep_2017} studies the applicability of complex-valued networks to
image classification (CIFAR-10, CIFAR-100, reduced train of SVHN), music-transcription
(MusicNet), and Speech Spectrum prediction (TIMIT).
\begin{itemize}
  \item 
\end{itemize}


\cite{monning_evaluation_2018}
\cite{jankowski_complex-valued_1996}
\cite{amin_complex-valued_nodate}
\cite{sarroff_complex_nodate}
\cite{lapidoth_capacity_2003}

% subsection tasks_and_datasets (end)

\subsection{Results and Discussion} % (fold)
\label{sub:results_and_discussion}

% subsection results_and_discussion (end)

% section Experiments (end)

\clearpage

\bibliographystyle{amsplain}
\bibliography{references}
\nocite{*}

\end{document}