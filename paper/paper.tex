\documentclass[a4paper,10pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{grffile}

\usepackage{subcaption}

\usepackage{booktabs}

\usepackage{lipsum}

\newcommand{\important}[1]{\textbf{\color{red} #1}}
\newcommand{\verify}{\important{verify}}
\newcommand{\rewrite}{\important{rewrite}}

\newcommand{\todo}{\important}  % alias

\newcommand{\real}{\mathbb{R}}
\newcommand{\cplx}{\mathbb{C}}

\renewcommand{\vec}{\overrightarrow}

\title{Sparsifying $\cplx$-valued networks}
\author{Ivan Nazarov}

\begin{document}
\maketitle

% {\color{red} \lipsum[1-3]}

\section{Introduction} % (fold)
\label{sec:introduction}

Why sparsity matters, why $\cplx$ networks?

Where are $\cplx$-networks used? and do these applications warrant sparsity?

% section introduction (end)

\section{$\cplx$-valued networks} % (fold)
\label{sec:c_valued_networks}

In this section i remind the reader what the complex-valued networks are, how
they are implemented and how much of $\cplx$-ness they have. Mention careful
design.

I will describe the linear layers (dense, masked, and convolutions) and
activations borrowed straight from the $\real$-values networks. Then i shall
mention that this has been tried before and list papers that one way or another
address $\cplx$-data processing with neural networks.

\cite{trabelsi_deep_2017} provide a set of building blocks for deep $\cplx$-valued
networks. The core of their work is structured framework for complex-valued operations
including representation and linear layers (dense and convolutional) as well
as $\cplx$-valued activations and complex batch-normalization and weight initialization.

Following \cite{trabelsi_deep_2017} we identify $\cplx$ with $\real^2$ via $
  z_\mathrm{r} + i z_\mathrm{i} \mapsto (z_\mathrm{r}, z_\mathrm{i})
$, storing the real and imaginary parts in interleaved format, and emulate $\cplx$-algebra
using $\real$-valued arithmetic. Thus a $\cplx$-network is, essentially, a $\real$-network
with \textit{double} the layers' widths to accommodate real and imaginary parts and
with constraints on layers' parameters that make them respect $\cplx$-algebra. Thus
$\cplx$-valued networks can be retrofitted into or built upon the existing $\real$-valued
autograd libraries.

In particular, common linear layers, such as dense layers and convolutions,
$
L \colon \cplx^{\mathrm{[in]}}
  \to \cplx^{\mathrm{[out]}}
$ act upon their inputs thus:
$$
\real^{\mathrm{[in]} \times 2}
  \to \real^{\mathrm{[out]} \times 2}
  \colon (x_r, x_i)
    \mapsto \Bigl(
      L_r x_r - L_i x_i,
      L_r x_i + L_i x_r
    \Bigr)
  \,, $$
where $
L_r, L_i
  \colon \real^{\mathrm{[in]}}
    \to \real^{\mathrm{[out]}}
$ are the unique $\real$-linear operators that make up $L = L_r + j L_i$ the overall
effect of the layer. For instance, to respect $\cplx$-arithmetic a linear layer acting
on $x$ should have $W_{ri} = - W_{ir}$ and $W_{rr} = W_{ii}$ in
$$
  \begin{pmatrix}
    y_r \\ y_i
  \end{pmatrix}
    = \begin{pmatrix}
      {\color{red} W_{rr}} & {\color{blue} W_{ri}} \\ 
      {\color{blue} W_{ir}} & {\color{red} W_{ii}}
    \end{pmatrix}
    \begin{pmatrix}
      x_r \\ x_i
    \end{pmatrix}
    + \begin{pmatrix}
      b_r \\ b_i
    \end{pmatrix}
  \,. $$

As far as non-linearities are concerned, \cite{trabelsi_deep_2017} consider a few truly
complex-valued ones, that are nevertheless not $\cplx$-differentiable: the most frequently
used activations are the common $\real$-valued non-linearities applied to real and
imaginary components independently.

\todo{something about activations}
modRelu -- relu on the modulus of a complex value (preserves the angle)
$
\mathrm{modReLU}
  \colon z \mapsto z \bigl(
    1 - \tfrac\theta{\lvert z \rvert}
  \bigr)_+
$
$
\mathrm{modReLU}
  \colon r e^{i \phi} \mapsto e^{i \phi} (r - \theta)_+
$

Other operations that might be used in a networks are also designed in such a way as to obey
$\cplx$-field algebra, e.g. summing or multiplying outputs of layers is carried out .

The question of differentiability of complex-valued networks is tricky. Since they include
non-differentiable non-linearities, they are strictly speaking, not differentiable at all.
Moreover, since we typically consider real-valued losses in practical problems, even if the
network itself were $\cplx$-differentiable (holomorphic), the fact that the loss is purely
$\real$ and non-constant violates the Cauchy-Riemann conditions, making it not $\cplx$-%
differentiable. However as \cite{trabelsi_deep_2017} suggests, it is possible to use and
apply the rules of $\real$-calculus to the network represented in double-$\real$ fashion
and use $\real$-valued backpropagation.

Such non-holomorphic approach to differentiation in $\cplx$-valued networks is also known
as Wirtinger calculus, \cite{adali_complex-valued_2011}. Essentially, derivatives are taken
with respect to real and imaginary parts as if they were independent variables.
\todo{recap Wirtinger calculus here in three sentences}

Cauchy-Riemann impose a rigid structure on real and imaginary parts of the function $
F(u, v) = f(z)\vert_{z=u+iv}
$, namely $
\tfrac{\partial F_{\Re }}{\partial u} = \tfrac{\partial F_{\Im }}{\partial v}
$ and $
\tfrac{\partial F_{\Re }}{\partial v} = - \tfrac{\partial F_{\Im }}{\partial u}
$.

% section c_valued_networks (end)

\section{Related research} % (fold)
\label{sec:realted_research}


% section realted_research (end)

\section{Complex Variational Dropout} % (fold)
\label{sec:complex_varaitional_dropout}

In this section we outline and derive the variational dropout technique for complex-%
valued networks, which re-traces the evolution of dropout technique for real-valued
weights.

Variational dropout, which enables automatic relevance detection by learning individual
per-weight dropout rates, was proposed by \cite{kingma_variational_2015} as a natural
generalization of the Gaussian Dropout \cite{srivastava_dropout_2014,wang_fast_2013},
itself -- a generalization of the Binary (Bernoulli) Dropout technique \cite{hinton_improving_2012}.

Let $W \in\real^{n\times m}$, and $b\in \real^n$ and consider the action of a linear
layer on its input $x$: $y = W x + b$ for $y\in \real^n$, $x\in \real^m$. The key idea
of Bernoulli dropout is to mask elements in the matrix $W$ of the layer with some given
probability $p$: $W = \theta \odot \xi$, there $\theta$ is the learnt weight matrix,
and is an iid Bernoulli mask $\xi \sim \mathcal{B}(\{0, \tfrac1{1-p}\}, 1-p)$. During
training, a dropout mask $\xi$ is used to compute $W$, and during evaluation the weights
are fixed to the expectation of $W$ which is $\theta$. The discussion and analysis in
\cite{kingma_variational_2015}, suggest that drawing one common dropout mask for all
data in the minibatch reduces statistical efficiency the estimator of the weights'
gradient.

Gaussian dropout works in a similar fashion, except the dropout mask $\xi$ is iid
$\mathcal{N}(1, \alpha=\tfrac{p}{1-p})$, i.e. a multiplicative Gaussian noise is
injected into the network.

% set by step we retell the story of kingmaetal2015

\cite{kingma_variational_2015} also propose the \textit{local reparameterization trick},
which improves efficiency stochastic gradient variational inference by translating
the uncertainty from the weights into local output noise of each linear layer. Without
this it would have been necessary to draw new set of random weights per each element
in a mini-batch.

$$
  \mathcal{N}(\theta_{ij}, \alpha \theta^2_{ij})
  \overset{\mathcal{D}}{\sim}
  \theta_{ij} \mathcal{N}(1, \alpha)
  \,, $$

\cite{molchanov_variational_2017} propose additional step in the local reparametrization,
which further reduces the variance of the gradient estimator for each dropped out
weight. Their simple formal change of variables $(\theta, \alpha) \to (\theta, \sigma^2)$
decouples the stochastic component from the weights \rewrite:
$$
  w_{ij} = \theta_{ij} + \theta_{ij} \sqrt{\alpha} \varepsilon_{ij}
  \,\to\,
  w_{ij} = \theta_{ij} + \sigma^2_{ij} \varepsilon_{ij}
  \,, $$
with the appropriate change of variables in the Kullback-Leibler divergence.
($\alpha_{ij} = \tfrac{\sigma_{ij}^2}{\lvert \theta_{ij}\rvert^2}$).

To recap the derivation in \cite{kingma_variational_2015}, let $W$ be a random martrix
with independently distributed elements, each according to $\mathcal{N}(\theta_{ij}, \sigma^2_{ij})$.
Then the action of a linear layer on $x$ is described as 
$$
  y = W x + b = I W x + b
    = (I \otimes x^\top) \vec{W} + b
  \,, $$
where $\vec{\cdot}$ denotes flattening in lexicographic order of indices (row-major),
for which we have the identity $\vec{A B C} = (A \otimes C^\top) \vec{B}$, \cite{cookbook2012}.
% It follows from the definition of the Kronecker product of $A$ and $C^\top$ and the
% row-major order vectorization.
Next, from $\vec{W}\sim \mathcal{N}_{[n\times m]}(\vec{\theta}, \mathop{diag}\Sigma)$
we get the following distribution for the output:
$$
  y \sim \mathcal{N}_{n}
    \bigl(
      \theta x + b,
      \sum_{i} e_i e_i^\top \sum_j \sigma^2_{ij} x_j^2
    \bigr)
  \,, $$
where the variance follows from the identity $(A\otimes B)(C\otimes D) = AC\otimes BD$,
\cite{cookbook2012}:
$$
  % (I \otimes x^\top) \mathop{diag}{\vec{\Sigma}} (I \otimes x^\top)^\top
  \ldots
    % = \sum_{ij} (I \otimes x^\top) e_i \otimes e_j
    %   \sigma^2_{ij} (e_i \otimes e_j)^\top (I \otimes x^\top)^\top
    = \sum_{ij} e_i (x^\top e_j) \sigma^2_{ij} (x^\top e_j) e_i^\top
    = \sum_{i} e_i e_i^\top \sum_j \sigma^2_{ij} x_j^2
  \,, $$
with $e_i$ being the $i$-th unit vector of dimensionality conforming to the expression
is is involved with.

Criticism of \cite{gale_state_2019} implies that $\ell_0$-variational dropout,
proposed by \cite{louizos_learning_2017} and based on the concrete binary distribution
of \cite{maddison_concrete_2016}, works consistently better than the variational
Gaussian dropout.

Maths and related results \cite{pav_moments_2015,taubock_complex-valued_2012},
and \cite{karseras_caution:_nodate}

% section complex_varaitional_dropout (end)

\section{Experiments} % (fold)
\label{sec:Experiments}

I this section we will compare the complex variational dropout techniques, discussed
above, on the tasks and datasets, that were studied in prior research on complex-%
valued networks.

\subsection{Tasks and datasets} % (fold)
\label{sub:tasks_and_datasets}

\cite{trabelsi_deep_2017} studies the applicability of complex-valued networks to
image classification (CIFAR-10, CIFAR-100, reduced train of SVHN), music-transcription
(MusicNet), and Speech Spectrum prediction (TIMIT).
\begin{itemize}
  \item 
\end{itemize}


\cite{monning_evaluation_2018}
\cite{jankowski_complex-valued_1996}
\cite{amin_complex-valued_nodate}
\cite{sarroff_complex_nodate}
\cite{lapidoth_capacity_2003}

% subsection tasks_and_datasets (end)

\subsection{Results and Discussion} % (fold)
\label{sub:results_and_discussion}

% subsection results_and_discussion (end)

% section Experiments (end)

\clearpage

\bibliographystyle{amsplain}
\bibliography{references}
\nocite{*}

\end{document}