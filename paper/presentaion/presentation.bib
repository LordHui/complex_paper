@inproceedings{trabelsi_deep_2018,
    title = {Deep {Complex} {Networks}},
    url = {https://openreview.net/forum?id=H1T2hmZAb},
    booktitle = {International {Conference} on {Learning} {Representations}},
    author = {Trabelsi, Chiheb and Bilaniuk, Olexa and Zhang, Ying and Serdyuk, Dmitriy and Subramanian, Sandeep and Santos, Joao Felipe and Mehri, Soroush and Rostamzadeh, Negar and Bengio, Yoshua and Pal, Christopher J.},
    year = {2018}
}

@article{yang_complex_2019,
    title = {Complex {Transformer}: {A} {Framework} for {Modeling} {Complex}-{Valued} {Sequence}},
    shorttitle = {Complex {Transformer}},
    url = {http://arxiv.org/abs/1910.10202},
    abstract = {While deep learning has received a surge of interest in a variety of fields in recent years, major deep learning models barely use complex numbers. However, speech, signal and audio data are naturally complex-valued after Fourier Transform, and studies have shown a potentially richer representation of complex nets. In this paper, we propose a Complex Transformer, which incorporates the transformer model as a backbone for sequence modeling; we also develop attention and encoder-decoder network operating for complex input. The model achieves state-of-the-art performance on the MusicNet dataset and an In-phase Quadrature (IQ) signal dataset.},
    urldate = {2019-11-04},
    journal = {arXiv:1910.10202 [cs, eess, stat]},
    author = {Yang, Muqiao and Ma, Martin Q. and Li, Dongyu and Tsai, Yao-Hung Hubert and Salakhutdinov, Ruslan},
    month = oct,
    year = {2019},
    note = {arXiv: 1910.10202},
    keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning}
}

@inproceedings{hirose_complex-valued_2009,
    title = {Complex-valued neural networks: {The} merits and their origins},
    shorttitle = {Complex-valued neural networks},
    doi = {10.1109/IJCNN.2009.5178754},
    abstract = {This paper discusses what the merits of complex-valued neural networks (CVNNs) arise from. First we look back the mathematical history to elucidate the features of complex numbers, in particular to confirm the importance of the phase-and-amplitude viewpoint for designing and constructing CVNNs to enhance the features. The viewpoint is essential in general to deal with waves such as electromagnetic-wave and lightwave. Then we point out that, although we represent a complex number as an ordered pair of real numbers for example, we can reduce ineffective degree of freedom in learning or self-organization in CVNNs to achieve better generalization characteristics. This wave-oriented merit is useful widely for general signal processing with Fourier synthesis or in frequency-domain treatment through Fourier transform.},
    booktitle = {2009 {International} {Joint} {Conference} on {Neural} {Networks}},
    author = {Hirose, Akira},
    month = jun,
    year = {2009},
    note = {ISSN: 2161-4407},
    keywords = {signal processing, Neural networks, neural nets, complex numbers, complex-valued neural networks, electromagnetic-wave, Fourier synthesis, Fourier transform, Fourier transforms, frequency-domain analysis, frequency-domain treatment, general signal processing},
    pages = {1237--1244},
    file = {IEEE Xplore Abstract Record:/Users/ivannazarov/Zotero/storage/RC5XSTFG/5178754.html:text/html;IEEE Xplore Full Text PDF:/Users/ivannazarov/Zotero/storage/YYDZ4VQR/Hirose - 2009 - Complex-valued neural networks The merits and the.pdf:application/pdf}
}

@inproceedings{wolter_complex_2018,
    address = {USA},
    series = {{NIPS}'18},
    title = {Complex {Gated} {Recurrent} {Neural} {Networks}},
    url = {http://dl.acm.org/citation.cfm?id=3327546.3327714},
    abstract = {Complex numbers have long been favoured for digital signal processing, yet complex representations rarely appear in deep learning architectures. RNNs, widely used to process time series and sequence information, could greatly benefit from complex representations. We present a novel complex gated recurrent cell, which is a hybrid cell combining complex-valued and norm-preserving state transitions with a gating mechanism. The resulting RNN exhibits excellent stability and convergence properties and performs competitively on the synthetic memory and adding task, as well as on the real-world tasks of human motion prediction.},
    urldate = {2019-11-22},
    booktitle = {Proceedings of the {32Nd} {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
    publisher = {Curran Associates Inc.},
    author = {Wolter, Moritz and Yao, Angela},
    year = {2018},
    note = {event-place: Montr√©al, Canada},
    pages = {10557--10567}
}

@article{tarver_design_2019,
    title = {Design and {Implementation} of a {Neural} {Network} {Based} {Predistorter} for {Enhanced} {Mobile} {Broadband}},
    url = {http://arxiv.org/abs/1907.00766},
    abstract = {Digital predistortion is the process of correcting for nonlinearities in the analog RF front-end of a wireless transmitter. These nonlinearities contribute to adjacent channel leakage, degrade the error vector magnitude of transmitted signals, and often force the transmitter to reduce its transmission power into a more linear but less power-efficient region of the device. Most predistortion techniques are based on polynomial models with an indirect learning architecture which have been shown to be overly sensitive to noise. In this work, we use neural network based predistortion with a novel neural network training method that avoids the indirect learning architecture and that shows significant improvements in both the adjacent channel leakage ratio and error vector magnitude. Moreover, we show that, by using a neural network based predistorter, we are able to achieve a 42\% reduction in latency and 9.6\% increase in throughput on an FPGA accelerator with 15\% fewer multiplications per sample when compared to a similarly performing memory-polynomial implementation.},
    urldate = {2020-04-18},
    journal = {arXiv:1907.00766 [eess]},
    author = {Tarver, Chance and Balatsoukas-Stimming, Alexios and Cavallaro, Joseph R.},
    month = jul,
    year = {2019},
    note = {arXiv: 1907.00766},
    keywords = {Electrical Engineering and Systems Science - Signal Processing}
}

@inproceedings{arjovsky_unitary_2016,
    title = {Unitary {Evolution} {Recurrent} {Neural} {Networks}},
    url = {http://proceedings.mlr.press/v48/arjovsky16.html},
    abstract = {Recurrent neural networks (RNNs) are notoriously difficult to train. When the eigenvalues of the hidden to hidden weight matrix deviate from absolute value 1, optimization becomes difficult due to ...},
    language = {en},
    urldate = {2019-11-22},
    booktitle = {International {Conference} on {Machine} {Learning}},
    author = {Arjovsky, Martin and Shah, Amar and Bengio, Yoshua},
    month = jun,
    year = {2016},
    pages = {1120--1128},
    file = {Full Text PDF:/Users/ivannazarov/Zotero/storage/AVWSC8VQ/Arjovsky et al. - 2016 - Unitary Evolution Recurrent Neural Networks.pdf:application/pdf;Snapshot:/Users/ivannazarov/Zotero/storage/S6AIYFXD/arjovsky16.html:text/html}
}

@incollection{wisdom_full-capacity_2016,
    title = {Full-{Capacity} {Unitary} {Recurrent} {Neural} {Networks}},
    url = {http://papers.nips.cc/paper/6327-full-capacity-unitary-recurrent-neural-networks.pdf},
    urldate = {2019-11-22},
    booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 29},
    publisher = {Curran Associates, Inc.},
    author = {Wisdom, Scott and Powers, Thomas and Hershey, John and Le Roux, Jonathan and Atlas, Les},
    editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
    year = {2016},
    pages = {4880--4888},
    file = {NIPS Full Text PDF:/Users/ivannazarov/Zotero/storage/84WDH4SR/Wisdom et al. - 2016 - Full-Capacity Unitary Recurrent Neural Networks.pdf:application/pdf;NIPS Snapshot:/Users/ivannazarov/Zotero/storage/AHALCX4B/6327-full-capacity-unitary-recurrent-neural-networks.html:text/html}
}

@article{uhlich_differentiable_2019,
    title = {Differentiable {Quantization} of {Deep} {Neural} {Networks}},
    url = {http://arxiv.org/abs/1905.11452},
    abstract = {We propose differentiable quantization (DQ) for efficient deep neural network (DNN) inference where gradient descent is used to learn the quantizer's step size, dynamic range and bitwidth. Training with differentiable quantizers brings two main benefits: first, DQ does not introduce hyperparameters; second, we can learn for each layer a different step size, dynamic range and bitwidth. Our experiments show that DNNs with heterogeneous and learned bitwidth yield better performance than DNNs with a homogeneous one. Further, we show that there is one natural DQ parametrization especially well suited for training. We confirm our findings with experiments on CIFAR-10 and ImageNet and we obtain quantized DNNs with learned quantization parameters achieving state-of-the-art performance.},
    urldate = {2020-01-17},
    journal = {arXiv:1905.11452 [cs, stat]},
    author = {Uhlich, Stefan and Mauch, Lukas and Yoshiyama, Kazuki and Cardinaux, Fabien and Garcia, Javier Alonso and Tiedemann, Stephen and Kemp, Thomas and Nakamura, Akira},
    month = may,
    year = {2019},
    note = {arXiv: 1905.11452},
    keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
    file = {arXiv Fulltext PDF:/Users/ivannazarov/Zotero/storage/G6KB63EM/Uhlich et al. - 2019 - Differentiable Quantization of Deep Neural Network.pdf:application/pdf;arXiv.org Snapshot:/Users/ivannazarov/Zotero/storage/TSTRRHDE/1905.html:text/html}
}

@inproceedings{lin_fixed_2016,
    title = {Fixed {Point} {Quantization} of {Deep} {Convolutional} {Networks}},
    url = {http://proceedings.mlr.press/v48/linb16.html},
    abstract = {In recent years increasingly complex architectures for deep convolution networks (DCNs) have been proposed to boost the performance on image recognition tasks. However, the gains in performance hav...},
    language = {en},
    urldate = {2020-01-17},
    booktitle = {International {Conference} on {Machine} {Learning}},
    author = {Lin, Darryl and Talathi, Sachin and Annapureddy, Sreekanth},
    month = jun,
    year = {2016},
    pages = {2849--2858},
    file = {Full Text PDF:/Users/ivannazarov/Zotero/storage/HXX5HPBD/Lin et al. - 2016 - Fixed Point Quantization of Deep Convolutional Net.pdf:application/pdf;Snapshot:/Users/ivannazarov/Zotero/storage/TX4FJ2ZG/linb16.html:text/html}
}

@inproceedings{chen_fxpnet_2017,
    title = {{FxpNet}: {Training} a deep convolutional neural network in fixed-point representation},
    shorttitle = {{FxpNet}},
    doi = {10.1109/IJCNN.2017.7966159},
    abstract = {We introduce FxpNet, a framework to train deep convolutional neural networks with low bit-width arithmetics in both forward pass and backward pass. During training FxpNet further reduces the bit-width of stored parameters (also known as primal parameters) by adaptively updating their fixed-point formats. These primal parameters are usually represented in the full resolution of floating-point values in previous binarized and quantized neural networks. In FxpNet, during forward pass fixed-point primal weights and activations are first binarized before computation, while in backward pass all gradients are represented as low resolution fixed-point values and then accumulated to corresponding fixed-point primal parameters. To have highly efficient implementations in FPGAs, ASICs and other dedicated devices, FxpNet introduces Integer Batch Normalization (IBN) and Fixed-point ADAM (FxpADAM) methods to further reduce the required floating-point operations, which will save considerable power and chip area. The evaluation on CIFAR-10 dataset indicates the effectiveness that FxpNet with 12-bit primal parameters and 12-bit gradients achieves comparable prediction accuracy with state-of-the-art binarized and quantized neural networks.},
    booktitle = {2017 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
    author = {Chen, Xi and Hu, Xiaolin and Zhou, Hucheng and Xu, Ningyi},
    month = may,
    year = {2017},
    note = {ISSN: 2161-4407},
    keywords = {12-bit gradients, 12-bit primal parameters, Acceleration, application specific integrated circuits, ASICs, backward pass, binarized neural networks, bit-width arithmetics, CIFAR-10 dataset, convolution, Convolution, deep convolutional neural network, field programmable gate arrays, Field programmable gate arrays, fixed point arithmetic, fixed-point ADAM, fixed-point primal parameters, fixed-point primal weights, fixed-point representation, floating point arithmetic, floating-point values, forward pass, FPGAs, FxpADAM, FxpNet, IBN, integer batch normalization, Kernel, low resolution fixed-point values, neural nets, Neural networks, Quantization (signal), quantized neural networks, Training},
    pages = {2494--2501},
    file = {IEEE Xplore Abstract Record:/Users/ivannazarov/Zotero/storage/TZJ7NK59/7966159.html:text/html;IEEE Xplore Full Text PDF:/Users/ivannazarov/Zotero/storage/FKJRRC3C/Chen et al. - 2017 - FxpNet Training a deep convolutional neural netwo.pdf:application/pdf}
}

@article{hinton_distilling_2015,
    title = {Distilling the {Knowledge} in a {Neural} {Network}},
    url = {http://arxiv.org/abs/1503.02531},
    abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
    urldate = {2020-01-17},
    journal = {arXiv:1503.02531 [cs, stat]},
    author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
    month = mar,
    year = {2015},
    note = {arXiv: 1503.02531},
    keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
    file = {arXiv Fulltext PDF:/Users/ivannazarov/Zotero/storage/LQEPQVKJ/Hinton et al. - 2015 - Distilling the Knowledge in a Neural Network.pdf:application/pdf;arXiv.org Snapshot:/Users/ivannazarov/Zotero/storage/R2MDZY9Z/1503.html:text/html}
}

@article{zhu_prune_2018,
    title = {To {Prune}, or {Not} to {Prune}: {Exploring} the {Efficacy} of {Pruning} for {Model} {Compression}},
    shorttitle = {To {Prune}, or {Not} to {Prune}},
    url = {https://openreview.net/forum?id=Sy1iIDkPM},
    abstract = {Model pruning seeks to induce sparsity in a deep neural network's various connection matrices, thereby reducing the number of nonzero-valued parameters in the model. Recent reports (Han et al., 2015; Narang et al., 2017) prune deep networks at the cost of only a marginal loss in accuracy and achieve a sizable reduction in model size. This hints at the possibility that the baseline models in these experiments are perhaps severely over-parameterized at the outset and a viable alternative for model compression might be to simply reduce the number of hidden units while maintaining the model's dense connection structure, exposing a similar trade-off in model size and accuracy. We investigate these two distinct paths for model compression within the context of energy-efficient inference in resource-constrained environments and propose a new gradual pruning technique that is simple and straightforward to apply across a variety of models/datasets with minimal tuning and can be seamlessly incorporated within the training process. We compare the accuracy of large, but pruned models (large-sparse) and their smaller, but dense (small-dense) counterparts with identical memory footprint. Across a broad range of neural network architectures (deep CNNs, stacked LSTM, and seq2seq LSTM models), we find large-sparse models to consistently outperform small-dense models and achieve up to 10x reduction in number of non-zero parameters with minimal loss in accuracy.},
    urldate = {2020-04-19},
    author = {Zhu, Michael H. and Gupta, Suyog},
    month = feb,
    year = {2018}
}

@inproceedings{molchanov_variational_2017,
    title = {Variational {Dropout} {Sparsifies} {Deep} {Neural} {Networks}},
    url = {http://proceedings.mlr.press/v70/molchanov17a.html},
    abstract = {We explore a recently proposed Variational Dropout technique that provided an elegant Bayesian interpretation to Gaussian Dropout. We extend Variational Dropout to the case when dropout rates are unbounded, propose a way to reduce the variance of the gradient estimator and report first experimental results with individual dropout rates per weight. Interestingly, it leads to extremely sparse solutions both in fully-connected and convolutional layers. This effect is similar to automatic relevance determination effect in empirical Bayes but has a number of advantages. We reduce the number of parameters up to 280 times on LeNet architectures and up to 68 times on VGG-like networks with a negligible decrease of accuracy.},
    language = {en},
    urldate = {2020-04-19},
    booktitle = {International {Conference} on {Machine} {Learning}},
    author = {Molchanov, Dmitry and Ashukha, Arsenii and Vetrov, Dmitry},
    month = jul,
    year = {2017},
    note = {ISSN: 1938-7228
Section: Machine Learning},
    keywords = {printed},
    pages = {2498--2507}
}

@incollection{kingma_variational_2015,
    title = {Variational {Dropout} and the {Local} {Reparameterization} {Trick}},
    url = {http://papers.nips.cc/paper/5666-variational-dropout-and-the-local-reparameterization-trick.pdf},
    urldate = {2019-06-03},
    booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 28},
    publisher = {Curran Associates, Inc.},
    author = {Kingma, Durk P and Salimans, Tim and Welling, Max},
    editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
    year = {2015},
    keywords = {printed},
    pages = {2575--2583},
    file = {NIPS Full Text PDF:/Users/ivannazarov/Zotero/storage/9X5Y6NEQ/Kingma et al. - 2015 - Variational Dropout and the Local Reparameterizati.pdf:application/pdf;NIPS Snapshot:/Users/ivannazarov/Zotero/storage/D3863CBQ/5666-variational-dropout-and-the-local-reparameterization-trick.html:text/html}
}

@inproceedings{thickstun_invariances_2018,
    title = {Invariances and {Data} {Augmentation} for {Supervised} {Music} {Transcription}},
    doi = {10.1109/ICASSP.2018.8461686},
    abstract = {This paper explores a variety of models for frame-based music transcription, with an emphasis on the methods needed to reach state-of-the-art on human recordings. The translation-invariant network discussed in this paper, which combines a traditional filterbank with a convolutional neural network, was the top-performing model in the 2017 MIREX Multiple Fundamental Frequency Estimation evaluation. This class of models shares parameters in the log-frequency domain, which exploits the frequency invariance of music to reduce the number of model parameters and avoid overfitting to the training data. All models in this paper were trained with supervision by labeled data from the MusicNet dataset, augmented by random label-preserving pitch-shift transformations.},
    booktitle = {2018 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
    author = {Thickstun, John and Harchaoui, Zaid and Foster, Dean P. and Kakade, Sham M.},
    month = apr,
    year = {2018},
    note = {ISSN: 2379-190X},
    keywords = {2017 MIREX Multiple Fundamental Frequency Estimation evaluation, audio signal processing, Computational modeling, Computer architecture, convolution, Convolution, convolutional neural network, convolutional neural networks, data augmentation, Data models, feedforward neural nets, filterbank, frame-based music transcription, frequency estimation, Frequency-domain analysis, human recordings, information retrieval, invariances, learning, learning (artificial intelligence), log-frequency domain, model parameters, music, Music, music information retrieval, MusicNet dataset, random label-preserving pitch-shift transformations, supervised music transcription, Task analysis, translation-invariant network},
    pages = {2241--2245}
}


@article{kharitonov_variational_2018,
    title = {Variational {Dropout} via {Empirical} {Bayes}},
    url = {http://arxiv.org/abs/1811.00596},
    abstract = {We study the Automatic Relevance Determination procedure applied to deep neural networks. We show that ARD applied to Bayesian DNNs with Gaussian approximate posterior distributions leads to a variational bound similar to that of variational dropout, and in the case of a fixed dropout rate, objectives are exactly the same. Experimental results show that the two approaches yield comparable results in practice even when the dropout rates are trained. This leads to an alternative Bayesian interpretation of dropout and mitigates some of the theoretical issues that arise with the use of improper priors in the variational dropout model. Additionally, we explore the use of the hierarchical priors in ARD and show that it helps achieve higher sparsity for the same accuracy.},
    urldate = {2019-09-18},
    journal = {arXiv:1811.00596 [cs, stat]},
    author = {Kharitonov, Valery and Molchanov, Dmitry and Vetrov, Dmitry},
    month = nov,
    year = {2018},
    note = {arXiv: 1811.00596},
    keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
    file = {arXiv\:1811.00596 PDF:/Users/ivannazarov/Zotero/storage/VPQCYVWU/Kharitonov et al. - 2018 - Variational Dropout via Empirical Bayes.pdf:application/pdf;arXiv.org Snapshot:/Users/ivannazarov/Zotero/storage/HY3FB5HQ/1811.html:text/html}
}

