@inproceedings{trabelsi_deep_2018,
    title = {Deep {Complex} {Networks}},
    url = {https://openreview.net/forum?id=H1T2hmZAb},
    booktitle = {International {Conference} on {Learning} {Representations}},
    author = {Trabelsi, Chiheb and Bilaniuk, Olexa and Zhang, Ying and Serdyuk, Dmitriy and Subramanian, Sandeep and Santos, Joao Felipe and Mehri, Soroush and Rostamzadeh, Negar and Bengio, Yoshua and Pal, Christopher J.},
    year = {2018}
}

@article{yang_complex_2019,
    title = {Complex {Transformer}: {A} {Framework} for {Modeling} {Complex}-{Valued} {Sequence}},
    shorttitle = {Complex {Transformer}},
    url = {http://arxiv.org/abs/1910.10202},
    abstract = {While deep learning has received a surge of interest in a variety of fields in recent years, major deep learning models barely use complex numbers. However, speech, signal and audio data are naturally complex-valued after Fourier Transform, and studies have shown a potentially richer representation of complex nets. In this paper, we propose a Complex Transformer, which incorporates the transformer model as a backbone for sequence modeling; we also develop attention and encoder-decoder network operating for complex input. The model achieves state-of-the-art performance on the MusicNet dataset and an In-phase Quadrature (IQ) signal dataset.},
    urldate = {2019-11-04},
    journal = {arXiv:1910.10202 [cs, eess, stat]},
    author = {Yang, Muqiao and Ma, Martin Q. and Li, Dongyu and Tsai, Yao-Hung Hubert and Salakhutdinov, Ruslan},
    month = oct,
    year = {2019},
    note = {arXiv: 1910.10202},
    keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning}
}

@inproceedings{hirose_complex-valued_2009,
    title = {Complex-valued neural networks: {The} merits and their origins},
    shorttitle = {Complex-valued neural networks},
    doi = {10.1109/IJCNN.2009.5178754},
    abstract = {This paper discusses what the merits of complex-valued neural networks (CVNNs) arise from. First we look back the mathematical history to elucidate the features of complex numbers, in particular to confirm the importance of the phase-and-amplitude viewpoint for designing and constructing CVNNs to enhance the features. The viewpoint is essential in general to deal with waves such as electromagnetic-wave and lightwave. Then we point out that, although we represent a complex number as an ordered pair of real numbers for example, we can reduce ineffective degree of freedom in learning or self-organization in CVNNs to achieve better generalization characteristics. This wave-oriented merit is useful widely for general signal processing with Fourier synthesis or in frequency-domain treatment through Fourier transform.},
    booktitle = {2009 {International} {Joint} {Conference} on {Neural} {Networks}},
    author = {Hirose, Akira},
    month = jun,
    year = {2009},
    note = {ISSN: 2161-4407},
    keywords = {signal processing, Neural networks, neural nets, complex numbers, complex-valued neural networks, electromagnetic-wave, Fourier synthesis, Fourier transform, Fourier transforms, frequency-domain analysis, frequency-domain treatment, general signal processing},
    pages = {1237--1244},
    file = {IEEE Xplore Abstract Record:/Users/ivannazarov/Zotero/storage/RC5XSTFG/5178754.html:text/html;IEEE Xplore Full Text PDF:/Users/ivannazarov/Zotero/storage/YYDZ4VQR/Hirose - 2009 - Complex-valued neural networks The merits and the.pdf:application/pdf}
}

@inproceedings{wolter_complex_2018,
    address = {USA},
    series = {{NIPS}'18},
    title = {Complex {Gated} {Recurrent} {Neural} {Networks}},
    url = {http://dl.acm.org/citation.cfm?id=3327546.3327714},
    abstract = {Complex numbers have long been favoured for digital signal processing, yet complex representations rarely appear in deep learning architectures. RNNs, widely used to process time series and sequence information, could greatly benefit from complex representations. We present a novel complex gated recurrent cell, which is a hybrid cell combining complex-valued and norm-preserving state transitions with a gating mechanism. The resulting RNN exhibits excellent stability and convergence properties and performs competitively on the synthetic memory and adding task, as well as on the real-world tasks of human motion prediction.},
    urldate = {2019-11-22},
    booktitle = {Proceedings of the {32Nd} {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
    publisher = {Curran Associates Inc.},
    author = {Wolter, Moritz and Yao, Angela},
    year = {2018},
    note = {event-place: Montr√©al, Canada},
    pages = {10557--10567}
}

@article{tarver_design_2019,
    title = {Design and {Implementation} of a {Neural} {Network} {Based} {Predistorter} for {Enhanced} {Mobile} {Broadband}},
    url = {http://arxiv.org/abs/1907.00766},
    abstract = {Digital predistortion is the process of correcting for nonlinearities in the analog RF front-end of a wireless transmitter. These nonlinearities contribute to adjacent channel leakage, degrade the error vector magnitude of transmitted signals, and often force the transmitter to reduce its transmission power into a more linear but less power-efficient region of the device. Most predistortion techniques are based on polynomial models with an indirect learning architecture which have been shown to be overly sensitive to noise. In this work, we use neural network based predistortion with a novel neural network training method that avoids the indirect learning architecture and that shows significant improvements in both the adjacent channel leakage ratio and error vector magnitude. Moreover, we show that, by using a neural network based predistorter, we are able to achieve a 42\% reduction in latency and 9.6\% increase in throughput on an FPGA accelerator with 15\% fewer multiplications per sample when compared to a similarly performing memory-polynomial implementation.},
    urldate = {2020-04-18},
    journal = {arXiv:1907.00766 [eess]},
    author = {Tarver, Chance and Balatsoukas-Stimming, Alexios and Cavallaro, Joseph R.},
    month = jul,
    year = {2019},
    note = {arXiv: 1907.00766},
    keywords = {Electrical Engineering and Systems Science - Signal Processing}
}

@inproceedings{arjovsky_unitary_2016,
    title = {Unitary {Evolution} {Recurrent} {Neural} {Networks}},
    url = {http://proceedings.mlr.press/v48/arjovsky16.html},
    abstract = {Recurrent neural networks (RNNs) are notoriously difficult to train. When the eigenvalues of the hidden to hidden weight matrix deviate from absolute value 1, optimization becomes difficult due to ...},
    language = {en},
    urldate = {2019-11-22},
    booktitle = {International {Conference} on {Machine} {Learning}},
    author = {Arjovsky, Martin and Shah, Amar and Bengio, Yoshua},
    month = jun,
    year = {2016},
    pages = {1120--1128},
    file = {Full Text PDF:/Users/ivannazarov/Zotero/storage/AVWSC8VQ/Arjovsky et al. - 2016 - Unitary Evolution Recurrent Neural Networks.pdf:application/pdf;Snapshot:/Users/ivannazarov/Zotero/storage/S6AIYFXD/arjovsky16.html:text/html}
}

@incollection{wisdom_full-capacity_2016,
    title = {Full-{Capacity} {Unitary} {Recurrent} {Neural} {Networks}},
    url = {http://papers.nips.cc/paper/6327-full-capacity-unitary-recurrent-neural-networks.pdf},
    urldate = {2019-11-22},
    booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 29},
    publisher = {Curran Associates, Inc.},
    author = {Wisdom, Scott and Powers, Thomas and Hershey, John and Le Roux, Jonathan and Atlas, Les},
    editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
    year = {2016},
    pages = {4880--4888},
    file = {NIPS Full Text PDF:/Users/ivannazarov/Zotero/storage/84WDH4SR/Wisdom et al. - 2016 - Full-Capacity Unitary Recurrent Neural Networks.pdf:application/pdf;NIPS Snapshot:/Users/ivannazarov/Zotero/storage/AHALCX4B/6327-full-capacity-unitary-recurrent-neural-networks.html:text/html}
}

@article{uhlich_differentiable_2019,
    title = {Differentiable {Quantization} of {Deep} {Neural} {Networks}},
    url = {http://arxiv.org/abs/1905.11452},
    abstract = {We propose differentiable quantization (DQ) for efficient deep neural network (DNN) inference where gradient descent is used to learn the quantizer's step size, dynamic range and bitwidth. Training with differentiable quantizers brings two main benefits: first, DQ does not introduce hyperparameters; second, we can learn for each layer a different step size, dynamic range and bitwidth. Our experiments show that DNNs with heterogeneous and learned bitwidth yield better performance than DNNs with a homogeneous one. Further, we show that there is one natural DQ parametrization especially well suited for training. We confirm our findings with experiments on CIFAR-10 and ImageNet and we obtain quantized DNNs with learned quantization parameters achieving state-of-the-art performance.},
    urldate = {2020-01-17},
    journal = {arXiv:1905.11452 [cs, stat]},
    author = {Uhlich, Stefan and Mauch, Lukas and Yoshiyama, Kazuki and Cardinaux, Fabien and Garcia, Javier Alonso and Tiedemann, Stephen and Kemp, Thomas and Nakamura, Akira},
    month = may,
    year = {2019},
    note = {arXiv: 1905.11452},
    keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
    file = {arXiv Fulltext PDF:/Users/ivannazarov/Zotero/storage/G6KB63EM/Uhlich et al. - 2019 - Differentiable Quantization of Deep Neural Network.pdf:application/pdf;arXiv.org Snapshot:/Users/ivannazarov/Zotero/storage/TSTRRHDE/1905.html:text/html}
}

@inproceedings{lin_fixed_2016,
    title = {Fixed {Point} {Quantization} of {Deep} {Convolutional} {Networks}},
    url = {http://proceedings.mlr.press/v48/linb16.html},
    abstract = {In recent years increasingly complex architectures for deep convolution networks (DCNs) have been proposed to boost the performance on image recognition tasks. However, the gains in performance hav...},
    language = {en},
    urldate = {2020-01-17},
    booktitle = {International {Conference} on {Machine} {Learning}},
    author = {Lin, Darryl and Talathi, Sachin and Annapureddy, Sreekanth},
    month = jun,
    year = {2016},
    pages = {2849--2858},
    file = {Full Text PDF:/Users/ivannazarov/Zotero/storage/HXX5HPBD/Lin et al. - 2016 - Fixed Point Quantization of Deep Convolutional Net.pdf:application/pdf;Snapshot:/Users/ivannazarov/Zotero/storage/TX4FJ2ZG/linb16.html:text/html}
}

@inproceedings{chen_fxpnet_2017,
    title = {{FxpNet}: {Training} a deep convolutional neural network in fixed-point representation},
    shorttitle = {{FxpNet}},
    doi = {10.1109/IJCNN.2017.7966159},
    abstract = {We introduce FxpNet, a framework to train deep convolutional neural networks with low bit-width arithmetics in both forward pass and backward pass. During training FxpNet further reduces the bit-width of stored parameters (also known as primal parameters) by adaptively updating their fixed-point formats. These primal parameters are usually represented in the full resolution of floating-point values in previous binarized and quantized neural networks. In FxpNet, during forward pass fixed-point primal weights and activations are first binarized before computation, while in backward pass all gradients are represented as low resolution fixed-point values and then accumulated to corresponding fixed-point primal parameters. To have highly efficient implementations in FPGAs, ASICs and other dedicated devices, FxpNet introduces Integer Batch Normalization (IBN) and Fixed-point ADAM (FxpADAM) methods to further reduce the required floating-point operations, which will save considerable power and chip area. The evaluation on CIFAR-10 dataset indicates the effectiveness that FxpNet with 12-bit primal parameters and 12-bit gradients achieves comparable prediction accuracy with state-of-the-art binarized and quantized neural networks.},
    booktitle = {2017 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
    author = {Chen, Xi and Hu, Xiaolin and Zhou, Hucheng and Xu, Ningyi},
    month = may,
    year = {2017},
    note = {ISSN: 2161-4407},
    keywords = {12-bit gradients, 12-bit primal parameters, Acceleration, application specific integrated circuits, ASICs, backward pass, binarized neural networks, bit-width arithmetics, CIFAR-10 dataset, convolution, Convolution, deep convolutional neural network, field programmable gate arrays, Field programmable gate arrays, fixed point arithmetic, fixed-point ADAM, fixed-point primal parameters, fixed-point primal weights, fixed-point representation, floating point arithmetic, floating-point values, forward pass, FPGAs, FxpADAM, FxpNet, IBN, integer batch normalization, Kernel, low resolution fixed-point values, neural nets, Neural networks, Quantization (signal), quantized neural networks, Training},
    pages = {2494--2501},
    file = {IEEE Xplore Abstract Record:/Users/ivannazarov/Zotero/storage/TZJ7NK59/7966159.html:text/html;IEEE Xplore Full Text PDF:/Users/ivannazarov/Zotero/storage/FKJRRC3C/Chen et al. - 2017 - FxpNet Training a deep convolutional neural netwo.pdf:application/pdf}
}

@article{hinton_distilling_2015,
    title = {Distilling the {Knowledge} in a {Neural} {Network}},
    url = {http://arxiv.org/abs/1503.02531},
    abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
    urldate = {2020-01-17},
    journal = {arXiv:1503.02531 [cs, stat]},
    author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
    month = mar,
    year = {2015},
    note = {arXiv: 1503.02531},
    keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
    file = {arXiv Fulltext PDF:/Users/ivannazarov/Zotero/storage/LQEPQVKJ/Hinton et al. - 2015 - Distilling the Knowledge in a Neural Network.pdf:application/pdf;arXiv.org Snapshot:/Users/ivannazarov/Zotero/storage/R2MDZY9Z/1503.html:text/html}
}

@article{zhu_prune_2018,
    title = {To {Prune}, or {Not} to {Prune}: {Exploring} the {Efficacy} of {Pruning} for {Model} {Compression}},
    shorttitle = {To {Prune}, or {Not} to {Prune}},
    url = {https://openreview.net/forum?id=Sy1iIDkPM},
    abstract = {Model pruning seeks to induce sparsity in a deep neural network's various connection matrices, thereby reducing the number of nonzero-valued parameters in the model. Recent reports (Han et al., 2015; Narang et al., 2017) prune deep networks at the cost of only a marginal loss in accuracy and achieve a sizable reduction in model size. This hints at the possibility that the baseline models in these experiments are perhaps severely over-parameterized at the outset and a viable alternative for model compression might be to simply reduce the number of hidden units while maintaining the model's dense connection structure, exposing a similar trade-off in model size and accuracy. We investigate these two distinct paths for model compression within the context of energy-efficient inference in resource-constrained environments and propose a new gradual pruning technique that is simple and straightforward to apply across a variety of models/datasets with minimal tuning and can be seamlessly incorporated within the training process. We compare the accuracy of large, but pruned models (large-sparse) and their smaller, but dense (small-dense) counterparts with identical memory footprint. Across a broad range of neural network architectures (deep CNNs, stacked LSTM, and seq2seq LSTM models), we find large-sparse models to consistently outperform small-dense models and achieve up to 10x reduction in number of non-zero parameters with minimal loss in accuracy.},
    urldate = {2020-04-19},
    author = {Zhu, Michael H. and Gupta, Suyog},
    month = feb,
    year = {2018}
}

@inproceedings{molchanov_variational_2017,
    title = {Variational {Dropout} {Sparsifies} {Deep} {Neural} {Networks}},
    url = {http://proceedings.mlr.press/v70/molchanov17a.html},
    abstract = {We explore a recently proposed Variational Dropout technique that provided an elegant Bayesian interpretation to Gaussian Dropout. We extend Variational Dropout to the case when dropout rates are unbounded, propose a way to reduce the variance of the gradient estimator and report first experimental results with individual dropout rates per weight. Interestingly, it leads to extremely sparse solutions both in fully-connected and convolutional layers. This effect is similar to automatic relevance determination effect in empirical Bayes but has a number of advantages. We reduce the number of parameters up to 280 times on LeNet architectures and up to 68 times on VGG-like networks with a negligible decrease of accuracy.},
    language = {en},
    urldate = {2020-04-19},
    booktitle = {International {Conference} on {Machine} {Learning}},
    author = {Molchanov, Dmitry and Ashukha, Arsenii and Vetrov, Dmitry},
    month = jul,
    year = {2017},
    note = {ISSN: 1938-7228
Section: Machine Learning},
    keywords = {printed},
    pages = {2498--2507}
}

@incollection{kingma_variational_2015,
    title = {Variational {Dropout} and the {Local} {Reparameterization} {Trick}},
    url = {http://papers.nips.cc/paper/5666-variational-dropout-and-the-local-reparameterization-trick.pdf},
    urldate = {2019-06-03},
    booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 28},
    publisher = {Curran Associates, Inc.},
    author = {Kingma, Durk P and Salimans, Tim and Welling, Max},
    editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
    year = {2015},
    keywords = {printed},
    pages = {2575--2583},
    file = {NIPS Full Text PDF:/Users/ivannazarov/Zotero/storage/9X5Y6NEQ/Kingma et al. - 2015 - Variational Dropout and the Local Reparameterizati.pdf:application/pdf;NIPS Snapshot:/Users/ivannazarov/Zotero/storage/D3863CBQ/5666-variational-dropout-and-the-local-reparameterization-trick.html:text/html}
}

@inproceedings{thickstun_invariances_2018,
    title = {Invariances and {Data} {Augmentation} for {Supervised} {Music} {Transcription}},
    doi = {10.1109/ICASSP.2018.8461686},
    abstract = {This paper explores a variety of models for frame-based music transcription, with an emphasis on the methods needed to reach state-of-the-art on human recordings. The translation-invariant network discussed in this paper, which combines a traditional filterbank with a convolutional neural network, was the top-performing model in the 2017 MIREX Multiple Fundamental Frequency Estimation evaluation. This class of models shares parameters in the log-frequency domain, which exploits the frequency invariance of music to reduce the number of model parameters and avoid overfitting to the training data. All models in this paper were trained with supervision by labeled data from the MusicNet dataset, augmented by random label-preserving pitch-shift transformations.},
    booktitle = {2018 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
    author = {Thickstun, John and Harchaoui, Zaid and Foster, Dean P. and Kakade, Sham M.},
    month = apr,
    year = {2018},
    note = {ISSN: 2379-190X},
    keywords = {2017 MIREX Multiple Fundamental Frequency Estimation evaluation, audio signal processing, Computational modeling, Computer architecture, convolution, Convolution, convolutional neural network, convolutional neural networks, data augmentation, Data models, feedforward neural nets, filterbank, frame-based music transcription, frequency estimation, Frequency-domain analysis, human recordings, information retrieval, invariances, learning, learning (artificial intelligence), log-frequency domain, model parameters, music, Music, music information retrieval, MusicNet dataset, random label-preserving pitch-shift transformations, supervised music transcription, Task analysis, translation-invariant network},
    pages = {2241--2245}
}


@article{kharitonov_variational_2018,
    title = {Variational {Dropout} via {Empirical} {Bayes}},
    url = {http://arxiv.org/abs/1811.00596},
    abstract = {We study the Automatic Relevance Determination procedure applied to deep neural networks. We show that ARD applied to Bayesian DNNs with Gaussian approximate posterior distributions leads to a variational bound similar to that of variational dropout, and in the case of a fixed dropout rate, objectives are exactly the same. Experimental results show that the two approaches yield comparable results in practice even when the dropout rates are trained. This leads to an alternative Bayesian interpretation of dropout and mitigates some of the theoretical issues that arise with the use of improper priors in the variational dropout model. Additionally, we explore the use of the hierarchical priors in ARD and show that it helps achieve higher sparsity for the same accuracy.},
    urldate = {2019-09-18},
    journal = {arXiv:1811.00596 [cs, stat]},
    author = {Kharitonov, Valery and Molchanov, Dmitry and Vetrov, Dmitry},
    month = nov,
    year = {2018},
    note = {arXiv: 1811.00596},
    keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
    file = {arXiv\:1811.00596 PDF:/Users/ivannazarov/Zotero/storage/VPQCYVWU/Kharitonov et al. - 2018 - Variational Dropout via Empirical Bayes.pdf:application/pdf;arXiv.org Snapshot:/Users/ivannazarov/Zotero/storage/HY3FB5HQ/1811.html:text/html}
}

@article{wu_compressing_2019,
    title = {Compressing complex convolutional neural network based on an improved deep compression algorithm},
    url = {http://arxiv.org/abs/1903.02358},
    abstract = {Although convolutional neural network (CNN) has made great progress, large redundant parameters restrict its deployment on embedded devices, especially mobile devices. The recent compression works are focused on real-value convolutional neural network (Real CNN), however, to our knowledge, there is no attempt for the compression of complex-value convolutional neural network (Complex CNN). Compared with the real-valued network, the complex-value neural network is easier to optimize, generalize, and has better learning potential. This paper extends the commonly used deep compression algorithm from real domain to complex domain and proposes an improved deep compression algorithm for the compression of Complex CNN. The proposed algorithm compresses the network about 8 times on CIFAR-10 dataset with less than 3\% accuracy loss. On the ImageNet dataset, our method compresses the model about 16 times and the accuracy loss is about 2\% without retraining.},
    urldate = {2020-04-18},
    journal = {arXiv:1903.02358 [cs]},
    author = {Wu, Jiasong and Ren, Hongshan and Kong, Youyong and Yang, Chunfeng and Senhadji, Lotfi and Shu, Huazhong},
    month = mar,
    year = {2019},
    note = {arXiv: 1903.02358},
    keywords = {Computer Science - Computer Vision and Pattern Recognition},
    annote = {Comment: 5 pages, 2 figures, 4 tables},
    file = {arXiv Fulltext PDF:/Users/ivannazarov/Zotero/storage/4BTP4CUV/Wu et al. - 2019 - Compressing complex convolutional neural network b.pdf:application/pdf;arXiv.org Snapshot:/Users/ivannazarov/Zotero/storage/PFNTNPPU/1903.html:text/html}
}

@article{vecchi_compressing_2020,
    title = {Compressing deep quaternion neural networks with targeted regularization},
    url = {http://arxiv.org/abs/1907.11546},
    abstract = {In recent years, hyper-complex deep networks (e.g., quaternion-based) have received increasing interest with applications ranging from image reconstruction to 3D audio processing. Similarly to their real-valued counterparts, quaternion neural networks might require custom regularization strategies to avoid overfitting. In addition, for many real-world applications and embedded implementations there is the need of designing sufficiently compact networks, with as few weights and units as possible. However, the problem of how to regularize and/or sparsify quaternion-valued networks has not been properly addressed in the literature as of now. In this paper we show how to address both problems by designing targeted regularization strategies, able to minimize the number of connections and neurons of the network during training. To this end, we investigate two extensions of \${\textbackslash}ell\_1\$ and structured regularization to the quaternion domain. In our experimental evaluation, we show that these tailored strategies significantly outperform classical (real-valued) regularization strategies, resulting in small networks especially suitable for low-power and real-time applications.},
    urldate = {2020-04-18},
    journal = {arXiv:1907.11546 [cs, stat]},
    author = {Vecchi, Riccardo and Scardapane, Simone and Comminiello, Danilo and Uncini, Aurelio},
    month = jan,
    year = {2020},
    note = {arXiv: 1907.11546},
    keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
    annote = {Comment: Submitted to CAAI Transactions on Intelligence Technology},
    file = {arXiv Fulltext PDF:/Users/ivannazarov/Zotero/storage/3DMH2LYB/Vecchi et al. - 2020 - Compressing deep quaternion neural networks with t.pdf:application/pdf;arXiv.org Snapshot:/Users/ivannazarov/Zotero/storage/IERBLSIX/1907.html:text/html}
}

@inproceedings{han_deep_2016,
    title = {Deep {Compression}: {Compressing} {Deep} {Neural} {Network} with {Pruning}, {Trained} {Quantization} and {Huffman} {Coding}},
    url = {http://arxiv.org/abs/1510.00149},
    abstract = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce "deep compression", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9x to 13x; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy efficiency.},
    booktitle = {4th {International} {Conference} on {Learning} {Representations}, {ICLR} 2016, {San} {Juan}, {Puerto} {Rico}, {May} 2-4, 2016, {Conference} {Track} {Proceedings}},
    author = {Han, Song and Mao, Huizi and Dally, William J.},
    editor = {Bengio, Yoshua and LeCun, Yann},
    year = {2016},
    annote = {Comment: Published as a conference paper at ICLR 2016 (oral)},
    file = {arXiv Fulltext PDF:/Users/ivannazarov/Zotero/storage/V247BBUH/Han et al. - 2016 - Deep Compression Compressing Deep Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/ivannazarov/Zotero/storage/8TVXP7HU/1510.html:text/html}
}

@incollection{novikov_tensorizing_2015,
    title = {Tensorizing {Neural} {Networks}},
    url = {http://papers.nips.cc/paper/5787-tensorizing-neural-networks.pdf},
    urldate = {2019-11-02},
    booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 28},
    publisher = {Curran Associates, Inc.},
    author = {Novikov, Alexander and Podoprikhin, Dmitrii and Osokin, Anton and Vetrov, Dmitry P},
    editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
    year = {2015},
    pages = {442--450},
    file = {NIPS Full Text PDF:/Users/ivannazarov/Zotero/storage/WJIPECL7/Novikov et al. - 2015 - Tensorizing Neural Networks.pdf:application/pdf;NIPS Snapshot:/Users/ivannazarov/Zotero/storage/FP9U3J8U/5787-tensorizing-neural-networks.html:text/html}
}

@article{balasubramanian_deep_2016,
    title = {Deep {Model} {Compression}: {Distilling} {Knowledge} from {Noisy} {Teachers}},
    shorttitle = {Deep {Model} {Compression}},
    url = {http://arxiv.org/abs/1610.09650},
    abstract = {The remarkable successes of deep learning models across various applications have resulted in the design of deeper networks that can solve complex problems. However, the increasing depth of such models also results in a higher storage and runtime complexity, which restricts the deployability of such very deep models on mobile and portable devices, which have limited storage and battery capacity. While many methods have been proposed for deep model compression in recent years, almost all of them have focused on reducing storage complexity. In this work, we extend the teacher-student framework for deep model compression, since it has the potential to address runtime and train time complexity too. We propose a simple methodology to include a noise-based regularizer while training the student from the teacher, which provides a healthy improvement in the performance of the student network. Our experiments on the CIFAR-10, SVHN and MNIST datasets show promising improvement, with the best performance on the CIFAR-10 dataset. We also conduct a comprehensive empirical evaluation of the proposed method under related settings on the CIFAR-10 dataset to show the promise of the proposed approach},
    urldate = {2020-06-07},
    author = {Balasubramanian, Vineeth N.},
    year = {2016},
    file = {Full Text PDF:/Users/ivannazarov/Zotero/storage/847VLLUG/Balasubramanian - Deep Model Compression Distilling Knowledge from .pdf:application/pdf;Snapshot:/Users/ivannazarov/Zotero/storage/8ZBUKQAA/80948240.html:text/html}
}

@article{higgins_beta-vae_2017,
    title = {beta-{VAE}: {Learning} {Basic} {Visual} {Concepts} with a {Constrained} {Variational} {Framework}},
    shorttitle = {beta-{VAE}},
    url = {https://openreview.net/forum?id=Sy2fzU9gl},
    abstract = {Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artificial intelligence that is able to learn and reason in the same way that humans do. We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. Our approach is a modification of the variational autoencoder (VAE) framework. We introduce an adjustable hyperparameter beta that balances latent channel capacity and independence constraints with reconstruction accuracy. We demonstrate that beta-VAE with appropriately tuned  beta {\textgreater} 1 qualitatively outperforms VAE (beta = 1), as well as state of the art unsupervised (InfoGAN) and semi-supervised (DC-IGN) approaches to disentangled factor learning on a variety of datasets (celebA, faces and chairs). Furthermore, we devise a protocol to quantitatively compare the degree of disentanglement learnt by different models, and show that our approach also significantly outperforms all baselines quantitatively. Unlike InfoGAN, beta-VAE is stable to train, makes few assumptions about the data and relies on tuning a single hyperparameter, which can be directly optimised through a hyper parameter search using weakly labelled data or through heuristic visual inspection for purely unsupervised data.},
    urldate = {2020-04-18},
    author = {Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
    month = apr,
    year = {2017},
    file = {Full Text PDF:/Users/ivannazarov/Zotero/storage/2NLNBC5B/Higgins et al. - 2016 - beta-VAE Learning Basic Visual Concepts with a Co.pdf:application/pdf;Snapshot:/Users/ivannazarov/Zotero/storage/3NIJG8CG/forum.html:text/html}
}
