
@inproceedings{trabelsi_deep_2018,
	title = {Deep {Complex} {Networks}},
	url = {https://openreview.net/forum?id=H1T2hmZAb},
	abstract = {At present, the vast majority of building blocks, techniques, and architectures for deep learning are based on real-valued operations and representations. However, recent work on recurrent neural networks and older fundamental theoretical analysis suggests that complex numbers could have a richer representational capacity and could also facilitate noise-robust memory retrieval mechanisms. Despite their attractive properties and potential for opening up entirely new neural architectures, complex-valued deep neural networks have been marginalized due to the absence of the building blocks required to design such models. In this work, we provide the key atomic components for complex-valued deep neural networks and apply them to convolutional feed-forward networks and convolutional LSTMs. More precisely, we rely on complex convolutions and present algorithms for complex batch-normalization, complex weight initialization strategies for complex-valued neural nets and we use them in experiments with end-to-end training schemes. We demonstrate that such complex-valued models are competitive with their real-valued counterparts. We test deep complex models on several computer vision tasks, on music transcription using the MusicNet dataset and on Speech Spectrum Prediction using the TIMIT dataset. We achieve state-of-the-art performance on these audio-related tasks.},
	urldate = {2019-06-03},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Trabelsi, Chiheb and Bilaniuk, Olexa and Zhang, Ying and Serdyuk, Dmitriy and Subramanian, Sandeep and Santos, João Felipe and Mehri, Soroush and Rostamzadeh, Negar and Bengio, Yoshua and Pal, Christopher J.},
	year = {2018},
	note = {arXiv: 1705.09792},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, printed}
}

@article{gale_state_2019,
	title = {The {State} of {Sparsity} in {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1902.09574},
	abstract = {We rigorously evaluate three state-of-the-art techniques for inducing sparsity in deep neural networks on two large-scale learning tasks: Transformer trained on WMT 2014 English-to-German, and ResNet-50 trained on ImageNet. Across thousands of experiments, we demonstrate that complex techniques (Molchanov et al., 2017; Louizos et al., 2017b) shown to yield high compression rates on smaller datasets perform inconsistently, and that simple magnitude pruning approaches achieve comparable or better results. Additionally, we replicate the experiments performed by (Frankle \& Carbin, 2018) and (Liu et al., 2018) at scale and show that unstructured sparse architectures learned through pruning cannot be trained from scratch to the same test set performance as a model trained with joint sparsification and optimization. Together, these results highlight the need for large-scale benchmarks in the field of model compression. We open-source our code, top performing model checkpoints, and results of all hyperparameter configurations to establish rigorous baselines for future work on compression and sparsification.},
	urldate = {2019-06-03},
	journal = {arXiv:1902.09574 [cs, stat]},
	author = {Gale, Trevor and Elsen, Erich and Hooker, Sara},
	month = feb,
	year = {2019},
	note = {arXiv: 1902.09574},
	keywords = {Computer Science - Machine Learning, printed, Statistics - Machine Learning}
}

@article{kharitonov_variational_2018,
	title = {Variational {Dropout} via {Empirical} {Bayes}},
	url = {http://arxiv.org/abs/1811.00596},
	abstract = {We study the Automatic Relevance Determination procedure applied to deep neural networks. We show that ARD applied to Bayesian DNNs with Gaussian approximate posterior distributions leads to a variational bound similar to that of variational dropout, and in the case of a fixed dropout rate, objectives are exactly the same. Experimental results show that the two approaches yield comparable results in practice even when the dropout rates are trained. This leads to an alternative Bayesian interpretation of dropout and mitigates some of the theoretical issues that arise with the use of improper priors in the variational dropout model. Additionally, we explore the use of the hierarchical priors in ARD and show that it helps achieve higher sparsity for the same accuracy.},
	urldate = {2019-09-18},
	journal = {arXiv:1811.00596 [cs, stat]},
	author = {Kharitonov, Valery and Molchanov, Dmitry and Vetrov, Dmitry},
	month = nov,
	year = {2018},
	note = {arXiv: 1811.00596},
	keywords = {Computer Science - Machine Learning, printed, Statistics - Machine Learning}
}

@incollection{novikov_tensorizing_2015,
	title = {Tensorizing {Neural} {Networks}},
	url = {http://papers.nips.cc/paper/5787-tensorizing-neural-networks.pdf},
	urldate = {2019-11-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 28},
	publisher = {Curran Associates, Inc.},
	author = {Novikov, Alexander and Podoprikhin, Dmitrii and Osokin, Anton and Vetrov, Dmitry P},
	editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
	year = {2015},
	pages = {442--450}
}

@article{yang_complex_2019,
	title = {Complex {Transformer}: {A} {Framework} for {Modeling} {Complex}-{Valued} {Sequence}},
	shorttitle = {Complex {Transformer}},
	url = {http://arxiv.org/abs/1910.10202},
	abstract = {While deep learning has received a surge of interest in a variety of fields in recent years, major deep learning models barely use complex numbers. However, speech, signal and audio data are naturally complex-valued after Fourier Transform, and studies have shown a potentially richer representation of complex nets. In this paper, we propose a Complex Transformer, which incorporates the transformer model as a backbone for sequence modeling; we also develop attention and encoder-decoder network operating for complex input. The model achieves state-of-the-art performance on the MusicNet dataset and an In-phase Quadrature (IQ) signal dataset.},
	urldate = {2019-11-04},
	journal = {arXiv:1910.10202 [cs, eess, stat]},
	author = {Yang, Muqiao and Ma, Martin Q. and Li, Dongyu and Tsai, Yao-Hung Hubert and Salakhutdinov, Ruslan},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.10202},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning}
}

@article{thickstun_learning_2017,
	title = {Learning {Features} of {Music} from {Scratch}},
	url = {http://arxiv.org/abs/1611.09827},
	abstract = {This paper introduces a new large-scale music dataset, MusicNet, to serve as a source of supervision and evaluation of machine learning methods for music research. MusicNet consists of hundreds of freely-licensed classical music recordings by 10 composers, written for 11 instruments, together with instrument/note annotations resulting in over 1 million temporal labels on 34 hours of chamber music performances under various studio and microphone conditions. The paper defines a multi-label classification task to predict notes in musical recordings, along with an evaluation protocol, and benchmarks several machine learning architectures for this task: i) learning from spectrogram features; ii) end-to-end learning with a neural net; iii) end-to-end learning with a convolutional neural net. These experiments show that end-to-end models trained for note prediction learn frequency selective filters as a low-level representation of audio.},
	urldate = {2019-11-24},
	journal = {arXiv:1611.09827 [cs, stat]},
	author = {Thickstun, John and Harchaoui, Zaid and Kakade, Sham},
	month = apr,
	year = {2017},
	note = {arXiv: 1611.09827},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Statistics - Machine Learning},
	annote = {Comment: 14 pages; camera-ready version; updated experiments and related works; additional MIR metrics (Appendix C)}
}

@inproceedings{wolter_complex_2018,
	address = {USA},
	series = {{NIPS}'18},
	title = {Complex {Gated} {Recurrent} {Neural} {Networks}},
	url = {http://papers.nips.cc/paper/8253-complex-gated-recurrent-neural-networks.pdf},
	abstract = {Complex numbers have long been favoured for digital signal processing, yet complex representations rarely appear in deep learning architectures. RNNs, widely used to process time series and sequence information, could greatly benefit from complex representations. We present a novel complex gated recurrent cell, which is a hybrid cell combining complex-valued and norm-preserving state transitions with a gating mechanism. The resulting RNN exhibits excellent stability and convergence properties and performs competitively on the synthetic memory and adding task, as well as on the real-world tasks of human motion prediction.},
	urldate = {2019-11-22},
	booktitle = {Proceedings of the {32Nd} {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Wolter, Moritz and Yao, Angela},
	year = {2018},
	note = {event-place: Montréal, Canada},
	pages = {10557--10567}
}

@inproceedings{hirose_complex-valued_2009,
	title = {Complex-valued neural networks: {The} merits and their origins},
	shorttitle = {Complex-valued neural networks},
	doi = {10.1109/IJCNN.2009.5178754},
	abstract = {This paper discusses what the merits of complex-valued neural networks (CVNNs) arise from. First we look back the mathematical history to elucidate the features of complex numbers, in particular to confirm the importance of the phase-and-amplitude viewpoint for designing and constructing CVNNs to enhance the features. The viewpoint is essential in general to deal with waves such as electromagnetic-wave and lightwave. Then we point out that, although we represent a complex number as an ordered pair of real numbers for example, we can reduce ineffective degree of freedom in learning or self-organization in CVNNs to achieve better generalization characteristics. This wave-oriented merit is useful widely for general signal processing with Fourier synthesis or in frequency-domain treatment through Fourier transform.},
	booktitle = {2009 {International} {Joint} {Conference} on {Neural} {Networks}},
	author = {Hirose, Akira},
	month = jun,
	year = {2009},
	note = {ISSN: 2161-4407},
	keywords = {complex numbers, complex-valued neural networks, electromagnetic-wave, Fourier synthesis, Fourier transform, Fourier transforms, frequency-domain analysis, frequency-domain treatment, general signal processing, neural nets, Neural networks, signal processing},
	pages = {1237--1244}
}

@inproceedings{arjovsky_unitary_2016,
	title = {Unitary {Evolution} {Recurrent} {Neural} {Networks}},
	url = {http://proceedings.mlr.press/v48/arjovsky16.html},
	abstract = {Recurrent neural networks (RNNs) are notoriously difficult to train. When the eigenvalues of the hidden to hidden weight matrix deviate from absolute value 1, optimization becomes difficult due to ...},
	language = {en},
	urldate = {2019-11-22},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Arjovsky, Martin and Shah, Amar and Bengio, Yoshua},
	month = jun,
	year = {2016},
	pages = {1120--1128}
}

@incollection{wisdom_full-capacity_2016,
	title = {Full-{Capacity} {Unitary} {Recurrent} {Neural} {Networks}},
	url = {http://papers.nips.cc/paper/6327-full-capacity-unitary-recurrent-neural-networks.pdf},
	urldate = {2019-11-22},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 29},
	publisher = {Curran Associates, Inc.},
	author = {Wisdom, Scott and Powers, Thomas and Hershey, John and Le Roux, Jonathan and Atlas, Les},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	year = {2016},
	pages = {4880--4888}
}

@article{uhlich_differentiable_2019,
	title = {Differentiable {Quantization} of {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1905.11452},
	abstract = {We propose differentiable quantization (DQ) for efficient deep neural network (DNN) inference where gradient descent is used to learn the quantizer's step size, dynamic range and bitwidth. Training with differentiable quantizers brings two main benefits: first, DQ does not introduce hyperparameters; second, we can learn for each layer a different step size, dynamic range and bitwidth. Our experiments show that DNNs with heterogeneous and learned bitwidth yield better performance than DNNs with a homogeneous one. Further, we show that there is one natural DQ parametrization especially well suited for training. We confirm our findings with experiments on CIFAR-10 and ImageNet and we obtain quantized DNNs with learned quantization parameters achieving state-of-the-art performance.},
	urldate = {2020-01-17},
	journal = {arXiv:1905.11452 [cs, stat]},
	author = {Uhlich, Stefan and Mauch, Lukas and Yoshiyama, Kazuki and Cardinaux, Fabien and Garcia, Javier Alonso and Tiedemann, Stephen and Kemp, Thomas and Nakamura, Akira},
	month = may,
	year = {2019},
	note = {arXiv: 1905.11452},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{hinton_distilling_2015,
	title = {Distilling the {Knowledge} in a {Neural} {Network}},
	url = {http://arxiv.org/abs/1503.02531},
	abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
	urldate = {2020-01-17},
	journal = {arXiv:1503.02531 [cs, stat]},
	author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	month = mar,
	year = {2015},
	note = {arXiv: 1503.02531},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	annote = {Comment: NIPS 2014 Deep Learning Workshop}
}

@inproceedings{chen_fxpnet_2017,
	title = {{FxpNet}: {Training} a deep convolutional neural network in fixed-point representation},
	shorttitle = {{FxpNet}},
	doi = {10.1109/IJCNN.2017.7966159},
	abstract = {We introduce FxpNet, a framework to train deep convolutional neural networks with low bit-width arithmetics in both forward pass and backward pass. During training FxpNet further reduces the bit-width of stored parameters (also known as primal parameters) by adaptively updating their fixed-point formats. These primal parameters are usually represented in the full resolution of floating-point values in previous binarized and quantized neural networks. In FxpNet, during forward pass fixed-point primal weights and activations are first binarized before computation, while in backward pass all gradients are represented as low resolution fixed-point values and then accumulated to corresponding fixed-point primal parameters. To have highly efficient implementations in FPGAs, ASICs and other dedicated devices, FxpNet introduces Integer Batch Normalization (IBN) and Fixed-point ADAM (FxpADAM) methods to further reduce the required floating-point operations, which will save considerable power and chip area. The evaluation on CIFAR-10 dataset indicates the effectiveness that FxpNet with 12-bit primal parameters and 12-bit gradients achieves comparable prediction accuracy with state-of-the-art binarized and quantized neural networks.},
	booktitle = {2017 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Chen, Xi and Hu, Xiaolin and Zhou, Hucheng and Xu, Ningyi},
	month = may,
	year = {2017},
	note = {ISSN: 2161-4407},
	keywords = {12-bit gradients, 12-bit primal parameters, Acceleration, application specific integrated circuits, ASICs, backward pass, binarized neural networks, bit-width arithmetics, CIFAR-10 dataset, convolution, Convolution, deep convolutional neural network, field programmable gate arrays, Field programmable gate arrays, fixed point arithmetic, fixed-point ADAM, fixed-point primal parameters, fixed-point primal weights, fixed-point representation, floating point arithmetic, floating-point values, forward pass, FPGAs, FxpADAM, FxpNet, IBN, integer batch normalization, Kernel, low resolution fixed-point values, neural nets, Neural networks, Quantization (signal), quantized neural networks, Training},
	pages = {2494--2501}
}

@inproceedings{lin_fixed_2016,
	title = {Fixed {Point} {Quantization} of {Deep} {Convolutional} {Networks}},
	url = {http://proceedings.mlr.press/v48/linb16.html},
	abstract = {In recent years increasingly complex architectures for deep convolution networks (DCNs) have been proposed to boost the performance on image recognition tasks. However, the gains in performance hav...},
	language = {en},
	urldate = {2020-01-17},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Lin, Darryl and Talathi, Sachin and Annapureddy, Sreekanth},
	month = jun,
	year = {2016},
	pages = {2849--2858}
}

@article{simonyan_very_2015,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	url = {http://arxiv.org/abs/1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	urldate = {2020-02-04},
	journal = {arXiv:1409.1556 [cs]},
	author = {Simonyan, Karen and Zisserman, Andrew},
	month = apr,
	year = {2015},
	note = {arXiv: 1409.1556},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@inproceedings{gaudet_deep_2018,
	title = {Deep {Quaternion} {Networks}},
	doi = {10.1109/IJCNN.2018.8489651},
	abstract = {The field of deep learning has seen significant advancement in recent years. However, much of the existing work has been focused on real-valued numbers. Recent work has shown that a deep learning system using the complex numbers can be deeper for a fixed parameter budget compared to its real-valued counterpart. In this work, we explore the benefits of generalizing one step further into the hyper-complex numbers, quaternions specifically, and provide the architecture components needed to build deep quaternion networks. We develop the theoretical basis by reviewing quaternion convolutions, developing a novel quaternion weight initialization scheme, and developing novel algorithms for quaternion batch-normalization. These pieces are tested in a classification model by end-to-end training on the CIFAR -10 and CIFAR -100 data sets and a segmentation model by end-to-end training on the KITTI Road Segmentation data set. These quaternion networks show improved convergence compared to real-valued and complex-valued networks, especially on the segmentation task, while having fewer parameters.},
	booktitle = {2018 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Gaudet, Chase J. and Maida, Anthony S.},
	month = jul,
	year = {2018},
	note = {ISSN: 2161-4407},
	keywords = {architecture components, CIFAR -10 data sets, CIFAR -100 data sets, classification model, complex, complex-valued networks, Computer architecture, Covariance matrices, deep learning, deep learning system, deep quaternion networks, end-to-end training, hyper-complex numbers, Image color analysis, Kernel, learning (artificial intelligence), neural nets, neural networks, Neural networks, number theory, pattern classification, quaternion, quaternion batch-normalization, quaternion convolutions, quaternion weight initialization scheme, Quaternions, real-valued numbers, Training},
	pages = {1--8}
}

@article{wu_compressing_2019,
	title = {Compressing complex convolutional neural network based on an improved deep compression algorithm},
	url = {http://arxiv.org/abs/1903.02358},
	abstract = {Although convolutional neural network (CNN) has made great progress, large redundant parameters restrict its deployment on embedded devices, especially mobile devices. The recent compression works are focused on real-value convolutional neural network (Real CNN), however, to our knowledge, there is no attempt for the compression of complex-value convolutional neural network (Complex CNN). Compared with the real-valued network, the complex-value neural network is easier to optimize, generalize, and has better learning potential. This paper extends the commonly used deep compression algorithm from real domain to complex domain and proposes an improved deep compression algorithm for the compression of Complex CNN. The proposed algorithm compresses the network about 8 times on CIFAR-10 dataset with less than 3\% accuracy loss. On the ImageNet dataset, our method compresses the model about 16 times and the accuracy loss is about 2\% without retraining.},
	urldate = {2020-04-18},
	journal = {arXiv:1903.02358 [cs]},
	author = {Wu, Jiasong and Ren, Hongshan and Kong, Youyong and Yang, Chunfeng and Senhadji, Lotfi and Shu, Huazhong},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.02358},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 5 pages, 2 figures, 4 tables}
}

@article{vecchi_compressing_2020,
	title = {Compressing deep quaternion neural networks with targeted regularization},
	url = {http://arxiv.org/abs/1907.11546},
	abstract = {In recent years, hyper-complex deep networks (e.g., quaternion-based) have received increasing interest with applications ranging from image reconstruction to 3D audio processing. Similarly to their real-valued counterparts, quaternion neural networks might require custom regularization strategies to avoid overfitting. In addition, for many real-world applications and embedded implementations there is the need of designing sufficiently compact networks, with as few weights and units as possible. However, the problem of how to regularize and/or sparsify quaternion-valued networks has not been properly addressed in the literature as of now. In this paper we show how to address both problems by designing targeted regularization strategies, able to minimize the number of connections and neurons of the network during training. To this end, we investigate two extensions of \${\textbackslash}ell\_1\$ and structured regularization to the quaternion domain. In our experimental evaluation, we show that these tailored strategies significantly outperform classical (real-valued) regularization strategies, resulting in small networks especially suitable for low-power and real-time applications.},
	urldate = {2020-04-18},
	journal = {arXiv:1907.11546 [cs, stat]},
	author = {Vecchi, Riccardo and Scardapane, Simone and Comminiello, Danilo and Uncini, Aurelio},
	month = jan,
	year = {2020},
	note = {arXiv: 1907.11546},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Submitted to CAAI Transactions on Intelligence Technology},
	annote = {Comment: Submitted to CAAI Transactions on Intelligence Technology}
}

@inproceedings{thickstun_invariances_2018,
	title = {Invariances and {Data} {Augmentation} for {Supervised} {Music} {Transcription}},
	doi = {10.1109/ICASSP.2018.8461686},
	abstract = {This paper explores a variety of models for frame-based music transcription, with an emphasis on the methods needed to reach state-of-the-art on human recordings. The translation-invariant network discussed in this paper, which combines a traditional filterbank with a convolutional neural network, was the top-performing model in the 2017 MIREX Multiple Fundamental Frequency Estimation evaluation. This class of models shares parameters in the log-frequency domain, which exploits the frequency invariance of music to reduce the number of model parameters and avoid overfitting to the training data. All models in this paper were trained with supervision by labeled data from the MusicNet dataset, augmented by random label-preserving pitch-shift transformations.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Thickstun, John and Harchaoui, Zaid and Foster, Dean P. and Kakade, Sham M.},
	month = apr,
	year = {2018},
	note = {ISSN: 2379-190X},
	keywords = {2017 MIREX Multiple Fundamental Frequency Estimation evaluation, audio signal processing, Computational modeling, Computer architecture, convolution, Convolution, convolutional neural network, convolutional neural networks, data augmentation, Data models, feedforward neural nets, filterbank, frame-based music transcription, frequency estimation, Frequency-domain analysis, human recordings, information retrieval, invariances, learning, learning (artificial intelligence), log-frequency domain, model parameters, music, Music, music information retrieval, MusicNet dataset, random label-preserving pitch-shift transformations, supervised music transcription, Task analysis, translation-invariant network},
	pages = {2241--2245}
}

@article{tarver_design_2019,
	title = {Design and {Implementation} of a {Neural} {Network} {Based} {Predistorter} for {Enhanced} {Mobile} {Broadband}},
	url = {http://arxiv.org/abs/1907.00766},
	abstract = {Digital predistortion is the process of correcting for nonlinearities in the analog RF front-end of a wireless transmitter. These nonlinearities contribute to adjacent channel leakage, degrade the error vector magnitude of transmitted signals, and often force the transmitter to reduce its transmission power into a more linear but less power-efficient region of the device. Most predistortion techniques are based on polynomial models with an indirect learning architecture which have been shown to be overly sensitive to noise. In this work, we use neural network based predistortion with a novel neural network training method that avoids the indirect learning architecture and that shows significant improvements in both the adjacent channel leakage ratio and error vector magnitude. Moreover, we show that, by using a neural network based predistorter, we are able to achieve a 42\% reduction in latency and 9.6\% increase in throughput on an FPGA accelerator with 15\% fewer multiplications per sample when compared to a similarly performing memory-polynomial implementation.},
	urldate = {2020-04-18},
	journal = {arXiv:1907.00766 [eess]},
	author = {Tarver, Chance and Balatsoukas-Stimming, Alexios and Cavallaro, Joseph R.},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.00766},
	keywords = {Electrical Engineering and Systems Science - Signal Processing}
}

@inproceedings{sivachitra_planning_2015,
	title = {Planning and relaxed state {EEG} signal classification using complex valued neural classifier for brain computer interface},
	doi = {10.1109/CCIP.2015.7100718},
	abstract = {Most of the Brain Computer Interface (BCI) techniques use EEG signals as a main source. Any BCI system consists of three modules and they are signal recorder, signal preprocessor and classifier. Development /Selection of efficient classifiers are a challenging task in this domain. The key work addressed in this paper is the classification of EEG signals measured under planning and relaxed state using advanced machine learning classifiers. Planning relax dataset is a benchmark data and it is obtained from UCI (University of California Irvine) machine learning repository. FC-FLC is a recently developed fast learning complex valued classifier and it is used for the EEG signal classification task. Complex valued classifier (FC-FLC) performs better than all the real valued classifiers as well as few fuzzy classifiers taken for comparison from the literature. The improvement is due to the use of Gd (gudermannian) activation function in the hidden layer of the network and the tuning free algorithm.},
	booktitle = {2015 {International} {Conference} on {Cognitive} {Computing} and {Information} {Processing}({CCIP})},
	author = {Sivachitra, M. and Vijayachitra, S.},
	month = mar,
	year = {2015},
	keywords = {advanced machine learning classifiers, BCI techniques, Biological neural networks, brain computer interface, brain-computer interfaces, Brain-computer interfaces, Classification algorithms, complex valued neural classifier, Complex valued neural network, Diseases, electroencephalography, Electroencephalography, Fast learning classifier, fast learning complex valued classifier, FC-FLC, fuzzy classifiers, fuzzy set theory, Gd activation function, learning (artificial intelligence), medical signal processing, neural nets, Planning, planning (artificial intelligence), Planning and relaxed dataset and Brain Computer Interface, planning EEG signal classification, planning relax dataset, Radial basis function networks, relaxed state EEG signal classification, signal classification, signal recorder, tuning free algorithm, UCI machine learning repository, University of California Irvine machine learning repository},
	pages = {1--4}
}

@article{higgins_beta-vae_2017,
	title = {beta-{VAE}: {Learning} {Basic} {Visual} {Concepts} with a {Constrained} {Variational} {Framework}},
	shorttitle = {beta-{VAE}},
	url = {https://openreview.net/forum?id=Sy2fzU9gl},
	abstract = {Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artificial intelligence that is able to learn and reason in the same way that humans do. We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. Our approach is a modification of the variational autoencoder (VAE) framework. We introduce an adjustable hyperparameter beta that balances latent channel capacity and independence constraints with reconstruction accuracy. We demonstrate that beta-VAE with appropriately tuned  beta {\textgreater} 1 qualitatively outperforms VAE (beta = 1), as well as state of the art unsupervised (InfoGAN) and semi-supervised (DC-IGN) approaches to disentangled factor learning on a variety of datasets (celebA, faces and chairs). Furthermore, we devise a protocol to quantitatively compare the degree of disentanglement learnt by different models, and show that our approach also significantly outperforms all baselines quantitatively. Unlike InfoGAN, beta-VAE is stable to train, makes few assumptions about the data and relies on tuning a single hyperparameter, which can be directly optimised through a hyper parameter search using weakly labelled data or through heuristic visual inspection for purely unsupervised data.},
	urldate = {2020-04-18},
	author = {Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
	month = apr,
	year = {2017}
}

@inproceedings{louizos_learning_2018,
	title = {Learning {Sparse} {Neural} {Networks} through {L}\_0 {Regularization}},
	url = {https://openreview.net/forum?id=H1Y8hhg0b},
	abstract = {We propose a practical method for \$L\_0\$ norm regularization for neural networks: pruning the network during training by encouraging weights to become exactly zero. Such regularization is interesting since (1) it can greatly speed up training and inference, and (2) it can improve generalization. AIC and BIC, well-known model selection criteria, are special cases of \$L\_0\$ regularization. However, since the \$L\_0\$ norm of weights is non-differentiable, we cannot incorporate it directly as a regularization term in the objective function. We propose a solution through the inclusion of a collection of non-negative stochastic gates, which collectively determine which weights to set to zero. We show that, somewhat surprisingly, for certain distributions over the gates, the expected \$L\_0\$ norm of the resulting gated weights is differentiable with respect to the distribution parameters. We further propose the {\textbackslash}emph\{hard concrete\} distribution for the gates, which is obtained by "stretching" a binary concrete distribution and then transforming its samples with a hard-sigmoid. The parameters of the distribution over the gates can then be jointly optimized with the original network parameters. As a result our method allows for straightforward and efficient learning of model structures with stochastic gradient descent and allows for conditional computation in a principled way. We perform various experiments to demonstrate the effectiveness of the resulting approach and regularizer.},
	urldate = {2020-04-19},
	author = {Louizos, Christos and Welling, Max and Kingma, Diederik P.},
	month = feb,
	year = {2018}
}

@article{zhu_prune_2018,
	title = {To {Prune}, or {Not} to {Prune}: {Exploring} the {Efficacy} of {Pruning} for {Model} {Compression}},
	shorttitle = {To {Prune}, or {Not} to {Prune}},
	url = {https://openreview.net/forum?id=Sy1iIDkPM},
	abstract = {Model pruning seeks to induce sparsity in a deep neural network's various connection matrices, thereby reducing the number of nonzero-valued parameters in the model. Recent reports (Han et al., 2015; Narang et al., 2017) prune deep networks at the cost of only a marginal loss in accuracy and achieve a sizable reduction in model size. This hints at the possibility that the baseline models in these experiments are perhaps severely over-parameterized at the outset and a viable alternative for model compression might be to simply reduce the number of hidden units while maintaining the model's dense connection structure, exposing a similar trade-off in model size and accuracy. We investigate these two distinct paths for model compression within the context of energy-efficient inference in resource-constrained environments and propose a new gradual pruning technique that is simple and straightforward to apply across a variety of models/datasets with minimal tuning and can be seamlessly incorporated within the training process. We compare the accuracy of large, but pruned models (large-sparse) and their smaller, but dense (small-dense) counterparts with identical memory footprint. Across a broad range of neural network architectures (deep CNNs, stacked LSTM, and seq2seq LSTM models), we find large-sparse models to consistently outperform small-dense models and achieve up to 10x reduction in number of non-zero parameters with minimal loss in accuracy.},
	urldate = {2020-04-19},
	author = {Zhu, Michael H. and Gupta, Suyog},
	month = feb,
	year = {2018}
}

@inproceedings{molchanov_variational_2017,
	title = {Variational {Dropout} {Sparsifies} {Deep} {Neural} {Networks}},
	url = {http://proceedings.mlr.press/v70/molchanov17a.html},
	abstract = {We explore a recently proposed Variational Dropout technique that provided an elegant Bayesian interpretation to Gaussian Dropout. We extend Variational Dropout to the case when dropout rates are unbounded, propose a way to reduce the variance of the gradient estimator and report first experimental results with individual dropout rates per weight. Interestingly, it leads to extremely sparse solutions both in fully-connected and convolutional layers. This effect is similar to automatic relevance determination effect in empirical Bayes but has a number of advantages. We reduce the number of parameters up to 280 times on LeNet architectures and up to 68 times on VGG-like networks with a negligible decrease of accuracy.},
	language = {en},
	urldate = {2020-04-19},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Molchanov, Dmitry and Ashukha, Arsenii and Vetrov, Dmitry},
	month = jul,
	year = {2017},
	note = {ISSN: 1938-7228
Section: Machine Learning},
	keywords = {printed},
	pages = {2498--2507}
}

@inproceedings{han_deep_2016,
	title = {Deep {Compression}: {Compressing} {Deep} {Neural} {Network} with {Pruning}, {Trained} {Quantization} and {Huffman} {Coding}},
	url = {http://arxiv.org/abs/1510.00149},
	abstract = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce "deep compression", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9x to 13x; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy efficiency.},
	booktitle = {4th {International} {Conference} on {Learning} {Representations}, {ICLR} 2016, {San} {Juan}, {Puerto} {Rico}, {May} 2-4, 2016, {Conference} {Track} {Proceedings}},
	author = {Han, Song and Mao, Huizi and Dally, William J.},
	editor = {Bengio, Yoshua and LeCun, Yann},
	year = {2016},
	annote = {Comment: Published as a conference paper at ICLR 2016 (oral)}
}

@article{balasubramanian_deep_2016,
	title = {Deep {Model} {Compression}: {Distilling} {Knowledge} from {Noisy} {Teachers}},
	shorttitle = {Deep {Model} {Compression}},
	url = {http://arxiv.org/abs/1610.09650},
	abstract = {The remarkable successes of deep learning models across various applications have resulted in the design of deeper networks that can solve complex problems. However, the increasing depth of such models also results in a higher storage and runtime complexity, which restricts the deployability of such very deep models on mobile and portable devices, which have limited storage and battery capacity. While many methods have been proposed for deep model compression in recent years, almost all of them have focused on reducing storage complexity. In this work, we extend the teacher-student framework for deep model compression, since it has the potential to address runtime and train time complexity too. We propose a simple methodology to include a noise-based regularizer while training the student from the teacher, which provides a healthy improvement in the performance of the student network. Our experiments on the CIFAR-10, SVHN and MNIST datasets show promising improvement, with the best performance on the CIFAR-10 dataset. We also conduct a comprehensive empirical evaluation of the proposed method under related settings on the CIFAR-10 dataset to show the promise of the proposed approach},
	urldate = {2020-06-07},
	author = {Balasubramanian, Vineeth N.},
	year = {2016}
}

@article{zhang_complex-valued_2017,
	title = {Complex-{Valued} {Convolutional} {Neural} {Network} and {Its} {Application} in {Polarimetric} {SAR} {Image} {Classification}},
	volume = {55},
	issn = {1558-0644},
	doi = {10.1109/TGRS.2017.2743222},
	abstract = {Following the great success of deep convolutional neural networks (CNNs) in computer vision, this paper proposes a complex-valued CNN (CV-CNN) specifically for synthetic aperture radar (SAR) image interpretation. It utilizes both amplitude and phase information of complex SAR imagery. All elements of CNN including input-output layer, convolution layer, activation function, and pooling layer are extended to the complex domain. Moreover, a complex backpropagation algorithm based on stochastic gradient descent is derived for CV-CNN training. The proposed CV-CNN is then tested on the typical polarimetric SAR image classification task which classifies each pixel into known terrain types via supervised training. Experiments with the benchmark data sets of Flevoland and Oberpfaffenhofen show that the classification error can be further reduced if employing CV-CNN instead of conventional real-valued CNN with the same degrees of freedom. The performance of CV-CNN is comparable to that of existing state-of-the-art methods in terms of overall classification accuracy.},
	number = {12},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Zhang, Zhimian and Wang, Haipeng and Xu, Feng and Jin, Ya-Qiu},
	month = dec,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Geoscience and Remote Sensing},
	keywords = {activation function, classification error, complex backpropagation algorithm, complex SAR imagery, complex-valued CNN, Complex-valued convolutional neural network (CV-CNN), computer vision, Computer vision, convolutional neural network, Convolutional neural networks, CV-CNN training, deep convolutional neural networks, deep learning, Feature extraction, gradient methods, image classification, learning (artificial intelligence), Machine learning, neural nets, Neural networks, phase information, pooling layer, radar computing, radar imaging, radar polarimetry, stochastic gradient descent, supervised training, synthetic aperture radar, Synthetic aperture radar, synthetic aperture radar (SAR), synthetic aperture radar image interpretation, terrain classification, Training data, typical polarimetric SAR image classification task},
	pages = {7177--7188}
}

@inproceedings{popa_complex-valued_2017,
	title = {Complex-valued convolutional neural networks for real-valued image classification},
	doi = {10.1109/IJCNN.2017.7965936},
	abstract = {In this paper, complex-valued convolutional neural networks are presented, by giving the full deduction of the gradient descent algorithm for training this type of networks. The performances of convolutional neural networks in the real-valued domain for image classification gave rise to the idea of extending them to the complex-valued domain, also. Real-valued image classification experiments done using the MNIST and CIFAR-10 datasets have shown an improvement in performance of complex-valued convolutional neural networks over their real-valued counterparts.},
	booktitle = {2017 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Popa, Călin-Adrian},
	month = may,
	year = {2017},
	note = {ISSN: 2161-4407},
	keywords = {Biological neural networks, CIFAR-10 dataset, complex-valued convolutional neural network training, Convolution, feedforward neural nets, Feedforward neural networks, gradient descent algorithm, image classification, Image recognition, learning (artificial intelligence), Machine learning, MNIST dataset, performance improvement, real-valued image classification, visual databases, Zirconium},
	pages = {816--822}
}

@article{reichert_neuronal_2014,
	title = {Neuronal {Synchrony} in {Complex}-{Valued} {Deep} {Networks}},
	url = {http://arxiv.org/abs/1312.6115},
	abstract = {Deep learning has recently led to great successes in tasks such as image recognition (e.g Krizhevsky et al., 2012). However, deep networks are still outmatched by the power and versatility of the brain, perhaps in part due to the richer neuronal computations available to cortical circuits. The challenge is to identify which neuronal mechanisms are relevant, and to find suitable abstractions to model them. Here, we show how aspects of spike timing, long hypothesized to play a crucial role in cortical information processing, could be incorporated into deep networks to build richer, versatile representations. We introduce a neural network formulation based on complex-valued neuronal units that is not only biologically meaningful but also amenable to a variety of deep learning frameworks. Here, units are attributed both a firing rate and a phase, the latter indicating properties of spike timing. We show how this formulation qualitatively captures several aspects thought to be related to neuronal synchrony, including gating of information processing and dynamic binding of distributed object representations. Focusing on the latter, we demonstrate the potential of the approach in several simple experiments. Thus, neuronal synchrony could be a flexible mechanism that fulfills multiple functional roles in deep networks.},
	urldate = {2020-06-11},
	journal = {arXiv:1312.6115 [cs, q-bio, stat]},
	author = {Reichert, David P. and Serre, Thomas},
	month = mar,
	year = {2014},
	note = {arXiv: 1312.6115},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
	annote = {Comment: ICLR 2014, accepted to conference track. This version: added proceedings note, minor additions}
}

@article{danihelka_associative_2016,
	title = {Associative {Long} {Short}-{Term} {Memory}},
	url = {http://arxiv.org/abs/1602.03032},
	abstract = {We investigate a new method to augment recurrent neural networks with extra memory without increasing the number of network parameters. The system has an associative memory based on complex-valued vectors and is closely related to Holographic Reduced Representations and Long Short-Term Memory networks. Holographic Reduced Representations have limited capacity: as they store more information, each retrieval becomes noisier due to interference. Our system in contrast creates redundant copies of stored information, which enables retrieval with reduced noise. Experiments demonstrate faster learning on multiple memorization tasks.},
	urldate = {2020-06-11},
	journal = {arXiv:1602.03032 [cs]},
	author = {Danihelka, Ivo and Wayne, Greg and Uria, Benigno and Kalchbrenner, Nal and Graves, Alex},
	month = may,
	year = {2016},
	note = {arXiv: 1602.03032},
	keywords = {Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: ICML-2016}
}

@article{bruna_mathematical_2015,
	title = {A mathematical motivation for complex-valued convolutional networks},
	url = {http://arxiv.org/abs/1503.03438},
	abstract = {A complex-valued convolutional network (convnet) implements the repeated application of the following composition of three operations, recursively applying the composition to an input vector of nonnegative real numbers: (1) convolution with complex-valued vectors followed by (2) taking the absolute value of every entry of the resulting vectors followed by (3) local averaging. For processing real-valued random vectors, complex-valued convnets can be viewed as "data-driven multiscale windowed power spectra," "data-driven multiscale windowed absolute spectra," "data-driven multiwavelet absolute values," or (in their most general configuration) "data-driven nonlinear multiwavelet packets." Indeed, complex-valued convnets can calculate multiscale windowed spectra when the convnet filters are windowed complex-valued exponentials. Standard real-valued convnets, using rectified linear units (ReLUs), sigmoidal (for example, logistic or tanh) nonlinearities, max. pooling, etc., do not obviously exhibit the same exact correspondence with data-driven wavelets (whereas for complex-valued convnets, the correspondence is much more than just a vague analogy). Courtesy of the exact correspondence, the remarkably rich and rigorous body of mathematical analysis for wavelets applies directly to (complex-valued) convnets.},
	urldate = {2020-06-11},
	journal = {arXiv:1503.03438 [cs, stat]},
	author = {Bruna, Joan and Chintala, Soumith and LeCun, Yann and Piantino, Serkan and Szlam, Arthur and Tygert, Mark},
	month = dec,
	year = {2015},
	note = {arXiv: 1503.03438},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	annote = {Comment: 11 pages, 3 figures; this is the retitled version submitted to the journal, "Neural Computation"}
}

@article{hirose_generalization_2012,
	title = {Generalization {Characteristics} of {Complex}-{Valued} {Feedforward} {Neural} {Networks} in {Relation} to {Signal} {Coherence}},
	volume = {23},
	issn = {2162-2388},
	doi = {10.1109/TNNLS.2012.2183613},
	abstract = {Applications of complex-valued neural networks (CVNNs) have expanded widely in recent years-in particular in radar and coherent imaging systems. In general, the most important merit of neural networks lies in their generalization ability. This paper compares the generalization characteristics of complex-valued and real-valued feedforward neural networks in terms of the coherence of the signals to be dealt with. We assume a task of function approximation such as interpolation of temporal signals. Simulation and real-world experiments demonstrate that CVNNs with amplitude-phase-type activation function show smaller generalization error than real-valued networks, such as bivariate and dual-univariate real-valued neural networks. Based on the results, we discuss how the generalization characteristics are influenced by the coherence of the signals depending on the degree of freedom in the learning and on the circularity in neural dynamics.},
	number = {4},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Hirose, Akira and Yoshida, Shotaro},
	month = apr,
	year = {2012},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Algorithms, amplitude-phase-type activation function, Biological neural networks, bivariate real-valued neural networks, Coherence, coherent imaging systems, complex-valued feedforward neural networks, Complex-valued neural network, Computer Simulation, dual-univariate real-valued neural networks, Feedback, Feedforward neural networks, function approximation, generalisation (artificial intelligence), generalization, generalization characteristics, Image Interpretation, Computer-Assisted, learning, learning (artificial intelligence), Models, Statistical, neural dynamics, neural nets, Neural Networks (Computer), Neurons, Pattern Recognition, Automated, radar systems, real-valued feedforward neural networks, signal coherence, signal processing, Signal to noise ratio, supervised learning, temporal signal interpolation, Tomography, Optical Coherence, Vectors},
	pages = {541--551}
}

@inproceedings{zimmermann_comparison_2011,
	title = {Comparison of the {Complex} {Valued} and {Real} {Valued} {Neural} {Networks} {Trained} with {Gradient} {Descent} and {Random} {Search} {Algorithms}},
	abstract = {Complex Valued Neural Network is one of the open topics in the machine learning society. In this paper we will try to go through the problems of the complex valued neural networks gradients computations by combining the global and local optimization algorithms. The outcome of the current research is the combined global-local algorithm for training the complex valued feed forward neural network which is appropriate for the considered chaotic problem.},
	booktitle = {{ESANN}},
	author = {Zimmermann, Hans-Georg and Minin, Alexey and Kusherbaeva, Victoria},
	year = {2011},
	pages = {6}
}

@inproceedings{savitha_fast_2011,
	title = {A {Fast} {Learning} {Complex}-valued {Neural} {Classifier} for real-valued classification problems},
	doi = {10.1109/IJCNN.2011.6033508},
	abstract = {This paper presents a fast learning fully complex-valued classifier to solve real-valued classification problems, called the `Fast Learning Complex-valued Neural Classifier' (FLCNC). The FLCNC is a single hidden layer network with a non-linear, real to complex transformed input layer, a hidden layer with a fully complex activation function and a linear output layer. The neurons in the input layer convert the real-valued input features to the Complex domain using an unique non-linear transformation. At the hidden layer, the complex-valued transformed input features are mapped onto a higher dimensional Complex plane using a fully complex-valued activation function of the type of `sech'. The parameters of the input and hidden neurons of the FLCNC are chosen randomly and the output parameters are estimated analytically which makes the FLCNC to perform fast classification. Moreover, the unique nonlinear input transformation and the orthogonal decision boundaries of the complex-valued neural network help the FLCNC to perform accurate classification. Performance of the FLCNC is demonstrated using a set of multi-category and binary real valued classification problems with both balanced and unbalanced data sets from the UCI machine learning repository. Performance comparison with existing complex-valued and real-valued classifiers show the superior classification performance of the FLCNC.},
	booktitle = {The 2011 {International} {Joint} {Conference} on {Neural} {Networks}},
	author = {Savitha, R. and Suresh, S. and Sundararajan, N.},
	month = jul,
	year = {2011},
	note = {ISSN: 2161-4407},
	keywords = {Accuracy, artificial neural networks, Benchmark testing, Biological neural networks, complex plane, fast learning complex-valued neural classifier, fast learning fully complex-valued classifier, FLCNC, fully complex activation function, learning (artificial intelligence), Machine learning, neural nets, Neurons, nonlinear input transformation, orthogonal decision boundaries, output parameter estimation, parameter estimation, pattern classification, real-valued classification problems, sech, Support vector machines, Training, UCI machine learning repository, unbalanced data sets},
	pages = {2243--2249}
}

@inproceedings{hui_mri_1995,
	title = {{MRI} reconstruction from truncated data using a complex domain backpropagation neural network},
	doi = {10.1109/PACRIM.1995.519582},
	abstract = {We propose a new data (extrapolation) modeling approach for reconstructing truncated magnetic resonance (MR) data. In our method, available low-frequency MR data are used to train a complex domain backpropagation neural network. This network is used to extrapolate the MR data and recover the missing high-frequency components. The performance of the proposed approach is demonstrated with a comparison to an existing real-valued neural network based method. Better results are obtained with the new approach because the complex-valued network makes use of the correlated information in the complex data instead of treating the data as separate real and imaginary parts.},
	booktitle = {{IEEE} {Pacific} {Rim} {Conference} on {Communications}, {Computers}, and {Signal} {Processing}. {Proceedings}},
	author = {Hui, Y. and Smith, M.R.},
	month = may,
	year = {1995},
	keywords = {backpropagation, Backpropagation, Biomedical imaging, biomedical NMR, complex domain backpropagation neural network, correlated information, correlation methods, Data engineering, extrapolation, Extrapolation, feed-forward neural network, feedforward neural nets, Frequency, high-frequency components recovery, image reconstruction, Image reconstruction, low-frequency MR data, Magnetic resonance, Magnetic resonance imaging, medical image processing, MRI images, MRI reconstruction, multilayer perceptrons, Neural networks, Neurons, performance, real-valued neural network based method, truncated magnetic resonance data},
	pages = {513--516}
}

@article{wang_deepcomplexmri_2020,
	title = {{DeepcomplexMRI}: {Exploiting} deep residual network for fast parallel {MR} imaging with complex convolution},
	volume = {68},
	issn = {0730-725X},
	shorttitle = {{DeepcomplexMRI}},
	url = {http://www.sciencedirect.com/science/article/pii/S0730725X19305338},
	doi = {10.1016/j.mri.2020.02.002},
	abstract = {This paper proposes a multi-channel image reconstruction method, named DeepcomplexMRI, to accelerate parallel MR imaging with residual complex convolutional neural network. Different from most existing works which rely on the utilization of the coil sensitivities or prior information of predefined transforms, DeepcomplexMRI takes advantage of the availability of a large number of existing multi-channel groudtruth images and uses them as target data to train the deep residual convolutional neural network offline. In particular, a complex convolutional network is proposed to take into account the correlation between the real and imaginary parts of MR images. In addition, the k-space data consistency is further enforced repeatedly in between layers of the network. The evaluations on in vivo datasets show that the proposed method has the capability to recover the desired multi-channel images. Its comparison with state-of-the-art methods also demonstrates that the proposed method can reconstruct the desired MR images more accurately.},
	language = {en},
	urldate = {2020-06-11},
	journal = {Magnetic Resonance Imaging},
	author = {Wang, Shanshan and Cheng, Huitao and Ying, Leslie and Xiao, Taohui and Ke, Ziwen and Zheng, Hairong and Liang, Dong},
	month = may,
	year = {2020},
	keywords = {Convolutional neural network, Deep learning, Fast MR imaging, Parallel imaging, Prior knowledge},
	pages = {136--147}
}

@inproceedings{hansch_complex-valued_2010,
	title = {Complex-valued convolutional neural networks for object detection in {PolSAR} data},
	booktitle = {8th {European} {Conference} on {Synthetic} {Aperture} {Radar}},
	publisher = {VDE},
	author = {Hänsch, Ronny and Hellwich, Olaf},
	year = {2010},
	pages = {1--4}
}

@article{scardapane_complex-valued_2018,
	title = {Complex-valued {Neural} {Networks} with {Non}-parametric {Activation} {Functions}},
	url = {http://arxiv.org/abs/1802.08026},
	abstract = {Complex-valued neural networks (CVNNs) are a powerful modeling tool for domains where data can be naturally interpreted in terms of complex numbers. However, several analytical properties of the complex domain (e.g., holomorphicity) make the design of CVNNs a more challenging task than their real counterpart. In this paper, we consider the problem of flexible activation functions (AFs) in the complex domain, i.e., AFs endowed with sufficient degrees of freedom to adapt their shape given the training data. While this problem has received considerable attention in the real case, a very limited literature exists for CVNNs, where most activation functions are generally developed in a split fashion (i.e., by considering the real and imaginary parts of the activation separately) or with simple phase-amplitude techniques. Leveraging over the recently proposed kernel activation functions (KAFs), and related advances in the design of complex-valued kernels, we propose the first fully complex, non-parametric activation function for CVNNs, which is based on a kernel expansion with a fixed dictionary that can be implemented efficiently on vectorized hardware. Several experiments on common use cases, including prediction and channel equalization, validate our proposal when compared to real-valued neural networks and CVNNs with fixed activation functions.},
	urldate = {2020-06-11},
	journal = {arXiv:1802.08026 [cs]},
	author = {Scardapane, Simone and Van Vaerenbergh, Steven and Hussain, Amir and Uncini, Aurelio},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.08026},
	keywords = {Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: Submitted to IEEE Transactions on Emerging Topics in Computational Intelligence}
}

@inproceedings{mandt_variational_2016,
	title = {Variational {Tempering}},
	url = {http://proceedings.mlr.press/v51/mandt16.html},
	abstract = {Variational inference (VI) combined with data subsampling enables approximate posterior inference with large data sets for otherwise intractable models, but suffers from poor local optima. We first...},
	language = {en},
	urldate = {2020-06-12},
	booktitle = {Artificial {Intelligence} and {Statistics}},
	author = {Mandt, Stephan and McInerney, James and Abrol, Farhan and Ranganath, Rajesh and Blei, David},
	month = may,
	year = {2016},
	note = {ISSN: 1938-7228
Section: Machine Learning},
	pages = {704--712}
}

@incollection{denil_predicting_2013,
	title = {Predicting {Parameters} in {Deep} {Learning}},
	url = {http://papers.nips.cc/paper/5025-predicting-parameters-in-deep-learning.pdf},
	urldate = {2020-06-12},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 26},
	publisher = {Curran Associates, Inc.},
	author = {Denil, Misha and Shakibi, Babak and Dinh, Laurent and Ranzato, Marc{\textbackslash}textquotesingle Aurelio and de Freitas, Nando},
	editor = {Burges, C. J. C. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
	year = {2013},
	pages = {2148--2156}
}

@inproceedings{nakkiran_compressing_2015,
	title = {Compressing {Deep} {Neural} {Networks} using a {Rank}-{Constrained} {Topology}},
	booktitle = {Proceedings of {Annual} {Conference} of the {International} {Speech} {Communication} {Association} ({Interspeech})},
	author = {Nakkiran, Preetum and Alvarez, Raziel and Prabhavalkar, Rohit and Parada, Carolina},
	year = {2015},
	pages = {1473--1477}
}

@incollection{denton_exploiting_2014,
	title = {Exploiting {Linear} {Structure} {Within} {Convolutional} {Networks} for {Efficient} {Evaluation}},
	url = {http://papers.nips.cc/paper/5544-exploiting-linear-structure-within-convolutional-networks-for-efficient-evaluation.pdf},
	urldate = {2020-06-12},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 27},
	publisher = {Curran Associates, Inc.},
	author = {Denton, Emily L and Zaremba, Wojciech and Bruna, Joan and LeCun, Yann and Fergus, Rob},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
	year = {2014},
	pages = {1269--1277}
}

@article{anwar_compact_2016,
	title = {Compact {Deep} {Convolutional} {Neural} {Networks} {With} {Coarse} {Pruning}},
	url = {http://arxiv.org/abs/1610.09639},
	abstract = {The learning capability of a neural network improves with increasing depth at higher computational costs. Wider layers with dense kernel connectivity patterns furhter increase this cost and may hinder real-time inference. We propose feature map and kernel level pruning for reducing the computational complexity of a deep convolutional neural network. Pruning feature maps reduces the width of a layer and hence does not need any sparse representation. Further, kernel pruning converts the dense connectivity pattern into a sparse one. Due to coarse nature, these pruning granularities can be exploited by GPUs and VLSI based implementations. We propose a simple and generic strategy to choose the least adversarial pruning masks for both granularities. The pruned networks are retrained which compensates the loss in accuracy. We obtain the best pruning ratios when we prune a network with both granularities. Experiments with the CIFAR-10 dataset show that more than 85\% sparsity can be induced in the convolution layers with less than 1\% increase in the missclassification rate of the baseline network.},
	urldate = {2020-06-12},
	journal = {arXiv:1610.09639 [cs]},
	author = {Anwar, Sajid and Sung, Wonyong},
	month = oct,
	year = {2016},
	note = {arXiv: 1610.09639},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing}
}

@incollection{han_learning_2015,
	title = {Learning both {Weights} and {Connections} for {Efficient} {Neural} {Network}},
	url = {http://papers.nips.cc/paper/5784-learning-both-weights-and-connections-for-efficient-neural-network.pdf},
	urldate = {2020-06-12},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 28},
	publisher = {Curran Associates, Inc.},
	author = {Han, Song and Pool, Jeff and Tran, John and Dally, William},
	editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
	year = {2015},
	pages = {1135--1143}
}

@article{seide_conversational_2011,
	title = {Conversational {Speech} {Transcription} {Using} {Context}-{Dependent} {Deep} {Neural} {Networks}},
	url = {https://www.microsoft.com/en-us/research/publication/conversational-speech-transcription-using-context-dependent-deep-neural-networks/},
	abstract = {We apply the recently proposed Context-Dependent Deep- Neural-Network HMMs, or CD-DNN-HMMs, to speech-to-text transcription. For single-pass speaker-independent recognition on the RT03S Fisher portion of phone-call transcription benchmark (Switchboard), the word-error rate is reduced from 27.4\%, obtained by discriminatively trained Gaussian-mixture HMMs, to 18.5\%?aa 33\% relative improvement. CD-DNN-HMMs combine classic artificial-neural-network HMMs with traditional tied-state triphones …},
	language = {en-US},
	urldate = {2020-06-12},
	author = {Seide, Frank and Li, Gang and Yu, Dong},
	month = aug,
	year = {2011}
}

@incollection{lecun_optimal_1990,
	title = {Optimal {Brain} {Damage}},
	url = {http://papers.nips.cc/paper/250-optimal-brain-damage.pdf},
	abstract = {We have used information-theoretic ideas to derive a class of practical and nearly optimal schemes for adapting the size of a neural network. By removing unimportant weights from a network, several improvements can be expected: better generalization, fewer training examples required, and improved speed of learning and/or classification. The basic idea is to use second-derivative information to make a tradeoff between network complexity and training set error. Experiments confirm the usefulness of the methods on a real-world application.},
	urldate = {2020-06-12},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 2},
	publisher = {Morgan-Kaufmann},
	author = {LeCun, Yann and Denker, John S. and Solla, Sara A.},
	editor = {Touretzky, D. S.},
	year = {1990},
	pages = {598--605}
}

@inproceedings{vanhoucke_improving_2011,
	title = {Improving the speed of neural networks on {CPUs}},
	booktitle = {Deep {Learning} and {Unsupervised} {Feature} {Learning} {Workshop}, {NIPS} 2011},
	author = {Vanhoucke, Vincent and Senior, Andrew and Mao, Mark Z.},
	year = {2011}
}

@inproceedings{chen_compressing_2015,
	title = {Compressing {Neural} {Networks} with the {Hashing} {Trick}},
	url = {http://proceedings.mlr.press/v37/chenc15.html},
	abstract = {As deep nets are increasingly used in applications suited for mobile devices, a fundamental dilemma becomes apparent: the trend in deep learning is to grow models to absorb ever-increasing data set...},
	language = {en},
	urldate = {2020-06-12},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Chen, Wenlin and Wilson, James and Tyree, Stephen and Weinberger, Kilian and Chen, Yixin},
	month = jun,
	year = {2015},
	note = {ISSN: 1938-7228
Section: Machine Learning},
	pages = {2285--2294}
}

@article{gong_compressing_2014,
	title = {Compressing {Deep} {Convolutional} {Networks} using {Vector} {Quantization}},
	url = {http://arxiv.org/abs/1412.6115},
	abstract = {Deep convolutional neural networks (CNN) has become the most promising method for object recognition, repeatedly demonstrating record breaking results for image classification and object detection in recent years. However, a very deep CNN generally involves many layers with millions of parameters, making the storage of the network model to be extremely large. This prohibits the usage of deep CNNs on resource limited hardware, especially cell phones or other embedded devices. In this paper, we tackle this model storage issue by investigating information theoretical vector quantization methods for compressing the parameters of CNNs. In particular, we have found in terms of compressing the most storage demanding dense connected layers, vector quantization methods have a clear gain over existing matrix factorization methods. Simply applying k-means clustering to the weights or conducting product quantization can lead to a very good balance between model size and recognition accuracy. For the 1000-category classification task in the ImageNet challenge, we are able to achieve 16-24 times compression of the network with only 1\% loss of classification accuracy using the state-of-the-art CNN.},
	urldate = {2020-06-12},
	journal = {arXiv:1412.6115 [cs]},
	author = {Gong, Yunchao and Liu, Liu and Yang, Ming and Bourdev, Lubomir},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.6115},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing}
}

@article{courbariaux_training_2015,
	title = {Training deep neural networks with low precision multiplications},
	url = {http://arxiv.org/abs/1412.7024},
	abstract = {Multipliers are the most space and power-hungry arithmetic operators of the digital implementation of deep neural networks. We train a set of state-of-the-art neural networks (Maxout networks) on three benchmark datasets: MNIST, CIFAR-10 and SVHN. They are trained with three distinct formats: floating point, fixed point and dynamic fixed point. For each of those datasets and for each of those formats, we assess the impact of the precision of the multiplications on the final error after training. We find that very low precision is sufficient not just for running trained networks but also for training them. For example, it is possible to train Maxout networks with 10 bits multiplications.},
	urldate = {2020-06-12},
	journal = {arXiv:1412.7024 [cs]},
	author = {Courbariaux, Matthieu and Bengio, Yoshua and David, Jean-Pierre},
	month = sep,
	year = {2015},
	note = {arXiv: 1412.7024},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: 10 pages, 5 figures, Accepted as a workshop contribution at ICLR 2015}
}

@incollection{courbariaux_binaryconnect_2015,
	title = {{BinaryConnect}: {Training} {Deep} {Neural} {Networks} with binary weights during propagations},
	shorttitle = {{BinaryConnect}},
	url = {http://papers.nips.cc/paper/5647-binaryconnect-training-deep-neural-networks-with-binary-weights-during-propagations.pdf},
	urldate = {2020-06-12},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 28},
	publisher = {Curran Associates, Inc.},
	author = {Courbariaux, Matthieu and Bengio, Yoshua and David, Jean-Pierre},
	editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
	year = {2015},
	pages = {3123--3131}
}

@incollection{kingma_variational_2015,
	title = {Variational {Dropout} and the {Local} {Reparameterization} {Trick}},
	url = {http://papers.nips.cc/paper/5666-variational-dropout-and-the-local-reparameterization-trick.pdf},
	urldate = {2020-06-12},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 28},
	publisher = {Curran Associates, Inc.},
	author = {Kingma, Durk P and Salimans, Tim and Welling, Max},
	editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
	year = {2015},
	keywords = {printed},
	pages = {2575--2583}
}

@article{sanh_movement_2020,
	title = {Movement {Pruning}: {Adaptive} {Sparsity} by {Fine}-{Tuning}},
	shorttitle = {Movement {Pruning}},
	url = {http://arxiv.org/abs/2005.07683},
	abstract = {Magnitude pruning is a widely used strategy for reducing model size in pure supervised learning; however, it is less effective in the transfer learning regime that has become standard for state-of-the-art natural language processing applications. We propose the use of movement pruning, a simple, deterministic first-order weight pruning method that is more adaptive to pretrained model fine-tuning. We give mathematical foundations to the method and compare it to existing zeroth- and first-order pruning methods. Experiments show that when pruning large pretrained language models, movement pruning shows significant improvements in high-sparsity regimes. When combined with distillation, the approach achieves minimal accuracy loss with down to only 3\% of the model parameters.},
	urldate = {2020-06-14},
	journal = {arXiv:2005.07683 [cs]},
	author = {Sanh, Victor and Wolf, Thomas and Rush, Alexander M.},
	month = may,
	year = {2020},
	note = {arXiv: 2005.07683},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 12 pages, 6 figures, 3 tables}
}

@inproceedings{hassibi_optimal_1993,
	title = {Optimal {Brain} {Surgeon} and general network pruning},
	doi = {10.1109/ICNN.1993.298572},
	abstract = {The use of information from all second-order derivatives of the error function to perform network pruning (i.e., removing unimportant weights from a trained network) in order to improve generalization, simplify networks, reduce hardware or storage requirements, increase the speed of further training, and, in some cases, enable rule extraction, is investigated. The method, Optimal Brain Surgeon (OBS), is significantly better than magnitude-based methods and Optimal Brain Damage, which often remove the wrong weights. OBS, permits pruning of more weights than other methods (for the same error on the training set), and thus yields better generalization on test data. Crucial to OBS is a recursion relation for calculating the inverse Hessian matrix H/sup -1/ from training data and structural information of the set. OBS deletes the correct weights from a trained XOR network in every case.{\textless}{\textgreater}},
	booktitle = {{IEEE} {International} {Conference} on {Neural} {Networks}},
	author = {Hassibi, B. and Stork, D.G. and Wolff, G.J.},
	month = mar,
	year = {1993},
	keywords = {Backpropagation, Benchmark testing, Biological neural networks, Data mining, error function, general network pruning, generalisation (artificial intelligence), generalization, Hardware, inverse Hessian matrix, learning (artificial intelligence), Machine learning, neural nets, Optimal Brain Surgeon, Pattern recognition, recursion relation, rule extraction, second-order derivatives, Statistics, storage requirements, structural information, Surges, trained XOR network, Training data},
	pages = {293--299 vol.1}
}

@incollection{wen_learning_2016,
	title = {Learning {Structured} {Sparsity} in {Deep} {Neural} {Networks}},
	url = {http://papers.nips.cc/paper/6504-learning-structured-sparsity-in-deep-neural-networks.pdf},
	urldate = {2020-06-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 29},
	publisher = {Curran Associates, Inc.},
	author = {Wen, Wei and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	year = {2016},
	pages = {2074--2082}
}
