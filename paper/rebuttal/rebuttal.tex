\documentclass[10pt,a4paper,draft]{article}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

\bibliographystyle{amsplain}

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{xcolor}
\usepackage{url}

\usepackage{dialogue}

\title{Rebuttal}
\author{Anonymous Authors}

\begin{document}
% \maketitle

Dear reviewers, we would like to thank you for your valuable comments, however space
limitations compelled us to respond to them altogether and not individually. We will
fix the mistakes you found, and shall make available the code for the experiments and
a library for C-valued Variational Dropout after the review process, regardless of its
outcome.
\medskip

% (rev 1, 5)
The fundamental motivation for C-valued networks is that there is evidence from signal
processing, which hints at them performing better both for R-valued and C-valued signals,
\cite{tarver_design_2019,sivachitra_planning_2015} and \cite{hu_initial_2016}. In terms
of arithmetic complexity, doubly-real networks require 4x more multiplications, while
C-valued require only 3x, due to the skew symmetric parameter constraint embedded in them
when viewed as a doubly-real to emulates C arithmetic. At the same time, higher redundancy
generally offers higher compression rates, and may enjoy better test performance for some
hyperparameters. Therefore, we conjecture that there would be no compressibility advantage
to C-networks. Thus the main reason to use C-valued networks is whether the domain knowledge
dictates that C-consistent transformations be used, i.g. with acoustic or radio
telecommunications' data, or MRI.

% (rev 3, 5)
Concerning the novelty of the study, it is the first application of Variational Inference
to C-valued networks, based on thorough analysis of the related work. The most closely
related study has been done by \cite{popa_complex-valued_2018}, who introduce C-valued Botlzmann
Machine and maximize ELBO over a fully-factorized posterior of Bernoulli distributions on
the hidden states of the machine. At the same time this is not the first attempt at compressing
C-valued networks. For instance, \cite{wu_compressing_2019} consider several compression
approaches, among which is fixed threshold-based pruning; and \cite{vecchi_compressing_2020}
apply sparsity-inducing regularization (LASSO) to quaternion-valued networks, which are
generalizations of complex numbers.

% (rev 3) on kl-scaling
For variational autoencoders scaling the KL term has been motivated by the need to control
the capacity and statistical independence of the latent information through a constraint on
the KL-divergence of the encoding variational posterior from the prior, \cite{higgins_beta-vae_2017}.
In Variational Dropout methods scaling is used to balance the gradient feedback of the loss
and the prior-matching terms of ELBO. In \cite{molchanov_variational_2017,kharitonov_variational_2018,gale_state_2019}
the coefficient C is linearly annealed from 0 to 1 during training. Unlike these studies,
we use constant C during `sparsify` stage in order to directly control the balance and explore
performance-compression trade-off by varying the weight of the complexity penalty of the
variational posterior distribution.

% (rev 3)
Variational Dropout permits non-uniform sparsity and as a Bayesian approach, allows estimation
of parameter uncertainty, which is useful for selecting sufficient bit precision level
\cite{louizos_bayesian_2017}. However, as a compression method, VD is more computationally
costly, than pruning, which has less arithmetic complexity and smaller memory footprint,
since it is not necessary to backpropagate through variational approximation's parameters.
We expect the conclusion of \cite{gale_state_2019} to carry over to C-valued models, since
pruning of \cite{zhu_prune_2018} masks parameters based on their ranking by absolute value,
$l_0$-based pruning \cite{louizos_learning_2018} learns a multiplicative mask, and there is
nothing fundamentally different in C-valued Variational Dropout.

% (rev 1)
Concerning the staged training procedure, the second step in
\cite{havasi_minimal_2018}
% [(Havasi et al. 2018)](https://openreview.net/forum?id=r1f0YiCctm)
is aimed at encoding parameters as indices in a shared lookup table, whereas our fine-tuning
stage trains the weights on the sparse pathways discovered by Variational Dropout methods.
Albeit their argument exhibits much more rigour, regrettably, it cannot be used to motivate
or `fine-tune` stage. However, this study is very relevant to Bayesian sparsification shall
be included in the text.

The recommended study \cite{havasi_minimal_2018} proposes KL-divergence targeting for reducing
communication complexity, motivated by the bits-back coding argument. The authors partition
parameters into groups and use KL-targeted $\beta$-VAE SGVB method split into pre-training
and random group-wise encoding phase. Targeting is done by adjusting the scaling coefficient
if the divergence deviates from the allotted `coding goal' for the group. The coding goal
specifies the amount of bits allocated to communicating the index in the shared procedurally
generated weight lookup table specific to that group, and therefore explicitly controls the
compression rate.

This method does not necessarily reduce arithmetic complexity, since a deterministic
non-zero parameter requires zero bits to be communicated given the assumed shared lookup
table, but cannot be excluded from the computation. The current study, on the other hand,
retains arithmetically significant parameters, those which are away from zero with high
confidence with respect to the variational posterior. Here compression is conflated with
sparsification, since implicit zeros reduce arithmetic and communication complexity.

% (rev 5)
Overall our experiment results are in line with empirical evidence that, other things being
equal, VD provides marginally stronger compression than ARD, but both show statistically similar
performance \cite[tab.1]{kharitonov_variational_2018}. Although there is no rigorous result
that would imply pareto-dominance of one method over the other, VD can be considered
theoretically flawed, since it uses the log-uniform prior which is improper and cannot be
normalized to a probability density. Both methods' KL-divergence terms provide similar
gradient feedback, which would imply certain uniformity in terms of convergence and stability,
but there has been no study addressing this directly.

% (rev 3, 5)
Concerning poor performance of undercompressed models on MusicNet, we strongly suspect
that the reason is overfitting. In our preliminary attempts at verbatim replication of
\cite{trabelsi_deep_2018}, we could not reach 72\% AP. However, we noticed that the validation
score had been consistently peaking at about the 15-th epoch and then steadily declining.
This lead us to believe that on this dataset (train-valid-test split) the undercompressed
models tends to overfit and for them the fine-tuning stage only exacerbates it. Analysis
of the compression rate and the number of epochs until early-stopping at the fine-tune
stage confirmed that undercompressed models start to overfit on validation very early.

Disregarding compression, our average best result of 72\% AP is modest compared to the current
state of the art on this task. First, \cite{trabelsi_deep_2018} report 72.9\% AP and show
that their deep architecture is superior to its R-valued variant (69.6\%). \cite{wolter_complex_2018}
use complex gated RNN but achieve 53\% AP. \cite{yang_complex_2019} use C-valued transformer
and achieve 74.2\% AP. Finally, \cite{thickstun_invariances_2018} use translation-invariant
R-valued network to achieve 77.3\% AP. Concerning compression, there seems no study concerning
compression of classifiers for MusicNet.

\bibliography{rebuttal}

\end{document}
