\documentclass[10pt,a4paper,draft]{article}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

\bibliographystyle{amsplain}

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{xcolor}
\usepackage{url}

\usepackage{dialogue}

\title{Rebuttal}
\author{Anonymous Authors}

\begin{document}
% \maketitle
% this is unabrdiged version of the rebuttal: the actual one uses more abbreviations and newspaper style 

Dear reviewers, we would like to thank you for your valuable comments, however space
limitations compel us to respond to them altogether and not individually. We shall make
available the code for the experiments and a library for C-valued Variational Dropout
after the review process, regardless of its outcome.
\medskip

% (rev 1, 5)
The fundamental motivation for C-valued networks is whether the domain knowledge
dictates that C-consistent transformations be used, e.g. with acoustic, radio, or MRI
data. In addition to discussion in the paper, there is scattered evidence from signal
processing, hinting at them performing better both for R-valued and C-valued signals,
\cite{tarver_design_2019,sivachitra_planning_2015} and \cite{hu_initial_2016}. In terms
of arithmetic complexity, doubly-real networks require 4x more multiplications, while
C-networks require only 3x, due to the skew symmetric constraint needed to emulate C
arithmetic with R2. Finally, higher redundancy generally offers higher compression
rates, and may enjoy better test performance for some hyperparameters, whence we
believe that there is no compressibility advantage to C-networks.

% (rev 3, 5)
This study is the first application of Variational Inference to C-valued networks,
based on thorough survey of the related work. The most closely related study has
been done by \cite{popa_complex-valued_2018}, who introduces C-valued Botlzmann Machine
and maximizes ELBO over a fully-factorized posterior of Bernoulli distributions on
the hidden states of the machine. At the same time this is not the first attempt at
compressing C-valued networks. For instance, \cite{wu_compressing_2019} consider
several compression approaches, among which is fixed threshold-based pruning; and
\cite{vecchi_compressing_2020} apply LASSO regularization to quaternion-valued networks,
which are generalizations of complex numbers.

% (rev 3) on kl-scaling
In Variational Autoencoders KL-divergence scaling allows to control the capacity and
statistical independence of the latent information through a constraint on the divergence
of the encoding variational posterior from the prior, \cite{higgins_beta-vae_2017}.
In Variational Dropout methods scaling balances the gradient feedback of the loss
and the prior-matching terms of ELBO. In contrast to \cite{molchanov_variational_2017},
\cite{kharitonov_variational_2018}, and \cite{gale_state_2019}, who linearly anneal
the scale from 0 to 1 during training, we use constant C during `sparsify` stage in
order to directly control the balance and explore performance-compression trade-off
by varying the weight of the complexity penalty of the variational posterior distribution.

% (rev 3)
Variational Dropout permits non-uniform sparsity and, as a Bayesian approach, can estimate
parameter uncertainty, which is useful for selecting bit precision sufficient for it,
\cite{louizos_bayesian_2017}. However, as a compression method, VD is more computationally
costly than pruning, which has less arithmetic complexity and smaller memory footprint,
since it is not necessary to backpropagate through variational approximation's parameters.
We expect the conclusion of \cite{gale_state_2019} to carry over to C-valued models, since
the pruning method of \cite{zhu_prune_2018} masks parameters based on their ranking by
absolute value, the probabilistic $l_0$ regularization of \cite{louizos_learning_2018}
learns a multiplicative mask, and there is nothing fundamentally different in C-valued
Variational Dropout.

% (rev 1)
Concerning the staged training procedure, regrettably, the study by \cite{havasi_minimal_2018}
% [(Havasi et al. 2018)](https://openreview.net/forum?id=r1f0YiCctm)
cannot motivate our `fine-tune` stage, albeit their argument exhibits much more rigour.
Their second step uses Bayesian compression to encode parameters as indices in a shared
lookup table, whereas our fine-tuning stage trains the weights on the sparse pathways
discovered by Variational Dropout. However, the suggested study is very relevant to Bayesian
sparsification and shall be covered in the text.

The recommended study \cite{havasi_minimal_2018} proposes KL-divergence targeting for reducing
communication complexity, motivated by the bits-back coding argument. The authors partition
parameters into groups and use KL-targeted $\beta$-VAE SGVB method split into pre-training
and random group-wise encoding phase. Targeting is done by adjusting the scaling coefficient
if the divergence deviates from the allotted `coding goal' for the group. The coding goal
specifies the amount of bits allocated to communicating the index in the shared procedurally
generated weight lookup table specific to that group, and therefore explicitly controls the
compression rate.

This method does not necessarily reduce arithmetic complexity, since a deterministic
non-zero parameter requires zero bits to be communicated given the assumed shared lookup
table, but cannot be excluded from the computation. The current study, on the other hand,
retains arithmetically significant parameters, those which are away from zero with high
confidence with respect to the variational posterior. Here compression is conflated with
sparsification, since implicit zeros reduce arithmetic and communication complexity.

% (rev 5)
Overall our experiment results are in line with empirical evidence that, other things
equal, VD provides marginally stronger compression than ARD, but both show statistically
similar performance \cite[tab.1]{kharitonov_variational_2018}. Both methods' KL-divergence
terms provide similar gradient feedback, which would imply certain uniformity in terms
of convergence and stability, but there has been no study addressing this directly.
Although there is no rigorous result that would imply Pareto-dominance of one method
over the other, VD can be considered theoretically flawed, since it uses the log-uniform
prior which is improper and cannot be normalized to a probability density.

% (rev 3, 5)
We suspect the reason for poor performance of under-compressed models on MusicNet is
overfitting. In our preliminary attempts at verbatim replication of \cite{trabelsi_deep_2018},
we could not surpass 72\% AP, and noticed that the validation score had been consistently
peaking around the 15-th epoch and then declining afterwards. This lead us to believe
that the under-compressed models tend to overfit and for them fine-tuning only exacerbates
it. Analysis of the compression rate and the number of epochs until early-stopping at
the final stage showed that undercompressed models start to overfit on validation very early.

Disregarding compression, our average best result of 72\% AP is modest compared to the current
state of the art on this task. \cite{trabelsi_deep_2018} report 72.9\% AP and show that their
deep C-network is superior to its R-valued variant (69.6\%). \cite{yang_complex_2019} use
C-valued transformer and achieve 74.2\% AP. Finally, \cite{thickstun_invariances_2018} use
translation-invariant R-valued network to achieve 77.3\% AP. Concerning compression, there
seems no study concerning compression of classifiers for MusicNet.

\bibliography{rebuttal}

\end{document}
