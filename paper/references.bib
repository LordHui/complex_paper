
@article{trabelsi_deep_2017,
	title = {Deep {Complex} {Networks}},
	url = {http://arxiv.org/abs/1705.09792},
	abstract = {At present, the vast majority of building blocks, techniques, and architectures for deep learning are based on real-valued operations and representations. However, recent work on recurrent neural networks and older fundamental theoretical analysis suggests that complex numbers could have a richer representational capacity and could also facilitate noise-robust memory retrieval mechanisms. Despite their attractive properties and potential for opening up entirely new neural architectures, complex-valued deep neural networks have been marginalized due to the absence of the building blocks required to design such models. In this work, we provide the key atomic components for complex-valued deep neural networks and apply them to convolutional feed-forward networks and convolutional LSTMs. More precisely, we rely on complex convolutions and present algorithms for complex batch-normalization, complex weight initialization strategies for complex-valued neural nets and we use them in experiments with end-to-end training schemes. We demonstrate that such complex-valued models are competitive with their real-valued counterparts. We test deep complex models on several computer vision tasks, on music transcription using the MusicNet dataset and on Speech Spectrum Prediction using the TIMIT dataset. We achieve state-of-the-art performance on these audio-related tasks.},
	urldate = {2019-06-03},
	journal = {arXiv:1705.09792 [cs]},
	author = {Trabelsi, Chiheb and Bilaniuk, Olexa and Zhang, Ying and Serdyuk, Dmitriy and Subramanian, Sandeep and Santos, João Felipe and Mehri, Soroush and Rostamzadeh, Negar and Bengio, Yoshua and Pal, Christopher J.},
	month = may,
	year = {2017},
	note = {arXiv: 1705.09792},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, printed},
	file = {arXiv\:1705.09792 PDF:/Users/ivannazarov/Zotero/storage/I6HPMFT6/Trabelsi et al. - 2017 - Deep Complex Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/ivannazarov/Zotero/storage/AJS6T5BF/1705.html:text/html}
}

@inproceedings{cohen_group_2016,
	title = {Group {Equivariant} {Convolutional} {Networks}},
	url = {http://proceedings.mlr.press/v48/cohenc16.html},
	abstract = {We introduce Group equivariant Convolutional Neural Networks (G-CNNs), a natural generalization of convolutional neural networks that reduces sample complexity by exploiting symmetries. G-CNNs use ...},
	language = {en},
	urldate = {2019-06-03},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Cohen, Taco and Welling, Max},
	month = jun,
	year = {2016},
	pages = {2990--2999},
	file = {Full Text PDF:/Users/ivannazarov/Zotero/storage/238EEAPT/Cohen and Welling - 2016 - Group Equivariant Convolutional Networks.pdf:application/pdf;Snapshot:/Users/ivannazarov/Zotero/storage/JI5RDJJY/cohenc16.html:text/html}
}

@article{hunger_introduction_nodate,
	title = {An {Introduction} to {Complex} {Differentials} and {Complex} {Differentiability}},
	language = {en},
	author = {Hunger, Raphael},
	pages = {20},
	file = {Hunger - An Introduction to Complex Differentials and Compl.pdf:/Users/ivannazarov/Zotero/storage/QDSY8L7K/Hunger - An Introduction to Complex Differentials and Compl.pdf:application/pdf}
}

@article{karseras_caution:_nodate,
	title = {Caution: {The} {Complex} {Normal} {Distribution} !},
	language = {en},
	author = {Karseras, Evripidis},
	pages = {15},
	file = {Karseras - Caution The Complex Normal Distribution !.pdf:/Users/ivannazarov/Zotero/storage/UMHXS9J7/Karseras - Caution The Complex Normal Distribution !.pdf:application/pdf}
}

@misc{noauthor_bandpower_nodate,
	title = {Bandpower of an {EEG} signal},
	url = {https://raphaelvallat.com/bandpower.html},
	urldate = {2019-06-03},
	file = {Bandpower of an EEG signal:/Users/ivannazarov/Zotero/storage/A98FDWYL/bandpower.html:text/html}
}

@article{chani-cahuana_digital_nodate,
	title = {Digital {Predistortion} for the {Linearization} of {Power} {Ampliﬁers}},
	language = {en},
	author = {Chani-Cahuana, Jessica},
	pages = {63},
	file = {Chani-Cahuana - Digital Predistortion for the Linearization of Pow.pdf:/Users/ivannazarov/Zotero/storage/5N9FB9E6/Chani-Cahuana - Digital Predistortion for the Linearization of Pow.pdf:application/pdf}
}

@article{moore_introduction_2006,
	title = {An {Introduction} to {Iterative} {Learning} {Control} {Theory}},
	language = {en},
	author = {Moore, Kevin L},
	year = {2006},
	pages = {47},
	file = {Moore - 2006 - An Introduction to Iterative Learning Control Theo.pdf:/Users/ivannazarov/Zotero/storage/GSCEACYQ/Moore - 2006 - An Introduction to Iterative Learning Control Theo.pdf:application/pdf}
}

@article{wang_study_nodate,
	title = {Study on complexity reduction of digital predistortion for power amplifier linearization},
	language = {en},
	author = {Wang, Siqi},
	pages = {157},
	file = {Wang - Study on complexity reduction of digital predistor.pdf:/Users/ivannazarov/Zotero/storage/8UG9NXWV/Wang - Study on complexity reduction of digital predistor.pdf:application/pdf}
}

@article{molchanov_variational_2017,
	title = {Variational {Dropout} {Sparsifies} {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1701.05369},
	abstract = {We explore a recently proposed Variational Dropout technique that provided an elegant Bayesian interpretation to Gaussian Dropout. We extend Variational Dropout to the case when dropout rates are unbounded, propose a way to reduce the variance of the gradient estimator and report first experimental results with individual dropout rates per weight. Interestingly, it leads to extremely sparse solutions both in fully-connected and convolutional layers. This effect is similar to automatic relevance determination effect in empirical Bayes but has a number of advantages. We reduce the number of parameters up to 280 times on LeNet architectures and up to 68 times on VGG-like networks with a negligible decrease of accuracy.},
	urldate = {2019-06-03},
	journal = {arXiv:1701.05369 [cs, stat]},
	author = {Molchanov, Dmitry and Ashukha, Arsenii and Vetrov, Dmitry},
	month = jan,
	year = {2017},
	note = {arXiv: 1701.05369},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, printed},
	file = {arXiv\:1701.05369 PDF:/Users/ivannazarov/Zotero/storage/8SCBTQQH/Molchanov et al. - 2017 - Variational Dropout Sparsifies Deep Neural Network.pdf:application/pdf;arXiv.org Snapshot:/Users/ivannazarov/Zotero/storage/J6DEYHHG/1701.html:text/html}
}

@article{pav_moments_2015,
	title = {Moments of the log non-central chi-square distribution},
	url = {http://arxiv.org/abs/1503.06266},
	abstract = {The cumulants and moments of the log of the non-central chi-square distribution are derived. For example, the expected log of a chi-square random variable with v degrees of freedom is log(2) + psi(v/2). Applications to modeling probability distributions are discussed.},
	urldate = {2019-06-03},
	journal = {arXiv:1503.06266 [math, stat]},
	author = {Pav, Steven E.},
	month = mar,
	year = {2015},
	note = {arXiv: 1503.06266},
	keywords = {Statistics - Applications, Mathematics - Probability, 60E07},
	file = {arXiv\:1503.06266 PDF:/Users/ivannazarov/Zotero/storage/B38HCG2W/Pav - 2015 - Moments of the log non-central chi-square distribu.pdf:application/pdf;arXiv.org Snapshot:/Users/ivannazarov/Zotero/storage/HHNRLD9X/1503.html:text/html}
}

@article{monning_evaluation_2018,
	title = {Evaluation of {Complex}-{Valued} {Neural} {Networks} on {Real}-{Valued} {Classification} {Tasks}},
	url = {http://arxiv.org/abs/1811.12351},
	abstract = {Complex-valued neural networks are not a new concept, however, the use of real-valued models has often been favoured over complex-valued models due to difficulties in training and performance. When comparing real-valued versus complex-valued neural networks, existing literature often ignores the number of parameters, resulting in comparisons of neural networks with vastly different sizes. We find that when real and complex neural networks of similar capacity are compared, complex models perform equal to or slightly worse than real-valued models for a range of real-valued classification tasks. The use of complex numbers allows neural networks to handle noise on the complex plane. When classifying real-valued data with a complex-valued neural network, the imaginary parts of the weights follow their real parts. This behaviour is indicative for a task that does not require a complex-valued model. We further investigated this in a synthetic classification task. We can transfer many activation functions from the real to the complex domain using different strategies. The weight initialisation of complex neural networks, however, remains a significant problem.},
	urldate = {2019-06-03},
	journal = {arXiv:1811.12351 [cs, stat]},
	author = {Mönning, Nils and Manandhar, Suresh},
	month = nov,
	year = {2018},
	note = {arXiv: 1811.12351},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1811.12351 PDF:/Users/ivannazarov/Zotero/storage/HBAVPNXV/Mönning and Manandhar - 2018 - Evaluation of Complex-Valued Neural Networks on Re.pdf:application/pdf;arXiv.org Snapshot:/Users/ivannazarov/Zotero/storage/TEGL2NBS/1811.html:text/html}
}

@article{jankowski_complex-valued_1996,
	title = {Complex-valued multistate neural associative memory},
	volume = {7},
	issn = {1045-9227},
	doi = {10.1109/72.548176},
	abstract = {A model of a multivalued associative memory is presented. This memory has the form of a fully connected attractor neural network composed of multistate complex-valued neurons. Such a network is able to perform the task of storing and recalling gray-scale images. It is also shown that the complex-valued fully connected neural network may be considered as a generalization of a Hopfield network containing real-valued neurons. A computational energy function is introduced and evaluated in order to prove network stability for asynchronous dynamics. Storage capacity as related to the number of accessible neuron states is also estimated.},
	number = {6},
	journal = {IEEE Transactions on Neural Networks},
	author = {Jankowski, S. and Lozowski, A. and Zurada, J. M.},
	month = nov,
	year = {1996},
	keywords = {Associative memory, asynchronous dynamics, complex-valued multistate neural associative memory, computational energy function, Computer networks, content-addressable storage, fully connected attractor neural network, Gray-scale, gray-scale image recall, gray-scale image storage, Hopfield network, Hopfield neural networks, Image coding, Image recognition, multivalued associative memory, network stability, Neural networks, Neurons, Stability, State estimation, storage capacity},
	pages = {1491--1496},
	file = {IEEE Xplore Abstract Record:/Users/ivannazarov/Zotero/storage/DRI8RCY8/548176.html:text/html;IEEE Xplore Full Text PDF:/Users/ivannazarov/Zotero/storage/MJAQ75F6/Jankowski et al. - 1996 - Complex-valued multistate neural associative memor.pdf:application/pdf}
}

@article{amin_complex-valued_nodate,
	title = {Complex-{Valued} {Neural} {Networks}: {Learning} {Algorithms} and {Applications}},
	language = {en},
	author = {Amin, Faijul},
	pages = {134},
	file = {Amin - Complex-Valued Neural Networks Learning Algorithm.pdf:/Users/ivannazarov/Zotero/storage/KJIZEIY3/Amin - Complex-Valued Neural Networks Learning Algorithm.pdf:application/pdf}
}

@article{gale_state_2019,
	title = {The {State} of {Sparsity} in {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1902.09574},
	abstract = {We rigorously evaluate three state-of-the-art techniques for inducing sparsity in deep neural networks on two large-scale learning tasks: Transformer trained on WMT 2014 English-to-German, and ResNet-50 trained on ImageNet. Across thousands of experiments, we demonstrate that complex techniques (Molchanov et al., 2017; Louizos et al., 2017b) shown to yield high compression rates on smaller datasets perform inconsistently, and that simple magnitude pruning approaches achieve comparable or better results. Additionally, we replicate the experiments performed by (Frankle \& Carbin, 2018) and (Liu et al., 2018) at scale and show that unstructured sparse architectures learned through pruning cannot be trained from scratch to the same test set performance as a model trained with joint sparsification and optimization. Together, these results highlight the need for large-scale benchmarks in the field of model compression. We open-source our code, top performing model checkpoints, and results of all hyperparameter configurations to establish rigorous baselines for future work on compression and sparsification.},
	urldate = {2019-06-03},
	journal = {arXiv:1902.09574 [cs, stat]},
	author = {Gale, Trevor and Elsen, Erich and Hooker, Sara},
	month = feb,
	year = {2019},
	note = {arXiv: 1902.09574},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, printed},
	file = {arXiv\:1902.09574 PDF:/Users/ivannazarov/Zotero/storage/FCNZDP2H/Gale et al. - 2019 - The State of Sparsity in Deep Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/ivannazarov/Zotero/storage/KBCWQR35/1902.html:text/html}
}

@article{taubock_complex-valued_2012,
	title = {Complex-{Valued} {Random} {Vectors} and {Channels}: {Entropy}, {Divergence}, and {Capacity}},
	volume = {58},
	issn = {0018-9448, 1557-9654},
	shorttitle = {Complex-{Valued} {Random} {Vectors} and {Channels}},
	url = {http://ieeexplore.ieee.org/document/6142094/},
	doi = {10.1109/TIT.2012.2184638},
	abstract = {Recent research has demonstrated signiﬁcant achievable performance gains by exploiting circularity/noncircularity or properness/improperness of complex-valued signals. In this paper, we investigate the inﬂuence of these properties on important information theoretic quantities such as entropy, divergence, and capacity. We prove two maximum entropy theorems that strengthen previously known results. The proof of the ﬁrst maximum entropy theorem is based on the so-called circular analog of a given complex-valued random vector. The introduction of the circular analog is additionally supported by a characterization theorem that employs a minimum Kullback–Leibler divergence criterion. In the proof of the second maximum entropy theorem, results about the second-order structure of complex-valued random vectors are exploited. Furthermore, we address the capacity of multiple-input multiple-output (MIMO) channels. Regardless of the speciﬁc distribution of the channel parameters (noise vector and channel matrix, if modeled as random), we show that the capacity-achieving input vector is circular for a broad range of MIMO channels (including coherent and noncoherent scenarios). Finally, we investigate the situation of an improper and Gaussian distributed noise vector. We compute both capacity and capacity-achieving input vector and show that improperness increases capacity, provided that the complementary covariance matrix is exploited. Otherwise, a capacity loss occurs, for which we derive an explicit expression.},
	language = {en},
	number = {5},
	urldate = {2019-06-03},
	journal = {IEEE Transactions on Information Theory},
	author = {Taubock, Georg},
	month = may,
	year = {2012},
	pages = {2729--2744},
	file = {Taubock - 2012 - Complex-Valued Random Vectors and Channels Entrop.pdf:/Users/ivannazarov/Zotero/storage/ECG6YEF2/Taubock - 2012 - Complex-Valued Random Vectors and Channels Entrop.pdf:application/pdf}
}

@article{sarroff_complex_nodate,
	title = {Complex {Neural} {Networks} for {Audio}},
	language = {en},
	author = {Sarroff, Andy M},
	pages = {114},
	file = {Sarroff - Complex Neural Networks for Audio.pdf:/Users/ivannazarov/Zotero/storage/94MGKBCC/Sarroff - Complex Neural Networks for Audio.pdf:application/pdf}
}

@incollection{kingma_variational_2015,
	title = {Variational {Dropout} and the {Local} {Reparameterization} {Trick}},
	url = {http://papers.nips.cc/paper/5666-variational-dropout-and-the-local-reparameterization-trick.pdf},
	urldate = {2019-06-03},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 28},
	publisher = {Curran Associates, Inc.},
	author = {Kingma, Durk P and Salimans, Tim and Welling, Max},
	editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
	year = {2015},
	keywords = {printed},
	pages = {2575--2583},
	file = {NIPS Full Text PDF:/Users/ivannazarov/Zotero/storage/9X5Y6NEQ/Kingma et al. - 2015 - Variational Dropout and the Local Reparameterizati.pdf:application/pdf;NIPS Snapshot:/Users/ivannazarov/Zotero/storage/D3863CBQ/5666-variational-dropout-and-the-local-reparameterization-trick.html:text/html}
}

@article{maddison_concrete_2016,
	title = {The {Concrete} {Distribution}: {A} {Continuous} {Relaxation} of {Discrete} {Random} {Variables}},
	shorttitle = {The {Concrete} {Distribution}},
	url = {http://arxiv.org/abs/1611.00712},
	abstract = {The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack useful reparameterizations due to the discontinuous nature of discrete states. In this work we introduce Concrete random variables---continuous relaxations of discrete random variables. The Concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, Concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate the effectiveness of Concrete relaxations on density estimation and structured prediction tasks using neural networks.},
	urldate = {2019-06-08},
	journal = {arXiv:1611.00712 [cs, stat]},
	author = {Maddison, Chris J. and Mnih, Andriy and Teh, Yee Whye},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.00712},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, queue},
	file = {arXiv\:1611.00712 PDF:/Users/ivannazarov/Zotero/storage/LT3MHHE3/Maddison et al. - 2016 - The Concrete Distribution A Continuous Relaxation.pdf:application/pdf;arXiv.org Snapshot:/Users/ivannazarov/Zotero/storage/MHLVTNSN/1611.html:text/html}
}