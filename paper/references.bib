
@book{petersen_matrix_2012,
	title = {The {Matrix} {Cookbook}},
	abstract = {These pages are a collection of facts (identities, approximations, inequalities, relations, ...) about matrices and matters relating to them. It is collected in this form for the convenience of anyone who wants a quick desktop reference.},
	language = {en},
	publisher = {Technical University of Denmark},
	author = {Petersen, Kaare Brandt and Pedersen, Michael Syskind},
	month = nov,
	year = {2012},
	keywords = {inverse, matrix derivative, Matrix identity, matrix relations}
}

@inproceedings{trabelsi_deep_2018,
	title = {Deep {Complex} {Networks}},
	abstract = {At present, the vast majority of building blocks, techniques, and architectures for deep learning are based on real-valued operations and representations. However, recent work on recurrent neural networks and older fundamental theoretical analysis suggests that complex numbers could have a richer representational capacity and could also facilitate noise-robust memory retrieval mechanisms. Despite their attractive properties and potential for opening up entirely new neural architectures, complex-valued deep neural networks have been marginalized due to the absence of the building blocks required to design such models. In this work, we provide the key atomic components for complex-valued deep neural networks and apply them to convolutional feed-forward networks and convolutional LSTMs. More precisely, we rely on complex convolutions and present algorithms for complex batch-normalization, complex weight initialization strategies for complex-valued neural nets and we use them in experiments with end-to-end training schemes. We demonstrate that such complex-valued models are competitive with their real-valued counterparts. We test deep complex models on several computer vision tasks, on music transcription using the MusicNet dataset and on Speech Spectrum Prediction using the TIMIT dataset. We achieve state-of-the-art performance on these audio-related tasks.},
	urldate = {2019-06-03},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Trabelsi, Chiheb and Bilaniuk, Olexa and Zhang, Ying and Serdyuk, Dmitriy and Subramanian, Sandeep and Santos, João Felipe and Mehri, Soroush and Rostamzadeh, Negar and Bengio, Yoshua and Pal, Christopher J.},
	year = {2018},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, printed}
}

@article{gale_state_2019,
	title = {The {State} of {Sparsity} in {Deep} {Neural} {Networks}},
	abstract = {We rigorously evaluate three state-of-the-art techniques for inducing sparsity in deep neural networks on two large-scale learning tasks: Transformer trained on WMT 2014 English-to-German, and ResNet-50 trained on ImageNet. Across thousands of experiments, we demonstrate that complex techniques (Molchanov et al., 2017; Louizos et al., 2017b) shown to yield high compression rates on smaller datasets perform inconsistently, and that simple magnitude pruning approaches achieve comparable or better results. Additionally, we replicate the experiments performed by (Frankle \& Carbin, 2018) and (Liu et al., 2018) at scale and show that unstructured sparse architectures learned through pruning cannot be trained from scratch to the same test set performance as a model trained with joint sparsification and optimization. Together, these results highlight the need for large-scale benchmarks in the field of model compression. We open-source our code, top performing model checkpoints, and results of all hyperparameter configurations to establish rigorous baselines for future work on compression and sparsification.},
	urldate = {2019-06-03},
	journal = {arXiv:1902.09574 [cs, stat]},
	author = {Gale, Trevor and Elsen, Erich and Hooker, Sara},
	month = feb,
	year = {2019},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, printed}
}

@phdthesis{sarroff_complex_2018,
	title = {Complex {Neural} {Networks} for {Audio}},
	abstract = {Audio is represented in two mathematically equivalent ways: the real-valued time domain (i.e., waveform) and the complex-valued frequency domain (i.e., spectrum). There are advantages to the frequency-domain representation, e.g., the human auditory system is known to process sound in the frequency-domain. Furthermore, linear time-invariant systems are convolved with sources in the time-domain, whereas they may be factorized in the frequency-domain. Neural networks have become rather useful when applied to audio tasks such as machine listening and audio synthesis, which are related by their dependencies on high quality acoustic models. They ideally encapsulate fine-scale temporal structure, such as that encoded in the phase of frequency-domain audio, yet there are no authoritative deep learning methods for complex audio. This manuscript is dedicated to addressing the shortcoming.},
	language = {en},
	school = {Dartmouth College},
	author = {Sarroff, Andy M},
	month = may,
	year = {2018}
}

@article{taubock_complex-valued_2012,
	title = {Complex-{Valued} {Random} {Vectors} and {Channels}: {Entropy}, {Divergence}, and {Capacity}},
	volume = {58},
	issn = {0018-9448, 1557-9654},
	shorttitle = {Complex-{Valued} {Random} {Vectors} and {Channels}},
	doi = {10.1109/TIT.2012.2184638},
	abstract = {Recent research has demonstrated signiﬁcant achievable performance gains by exploiting circularity/noncircularity or properness/improperness of complex-valued signals. In this paper, we investigate the inﬂuence of these properties on important information theoretic quantities such as entropy, divergence, and capacity. We prove two maximum entropy theorems that strengthen previously known results. The proof of the ﬁrst maximum entropy theorem is based on the so-called circular analog of a given complex-valued random vector. The introduction of the circular analog is additionally supported by a characterization theorem that employs a minimum Kullback–Leibler divergence criterion. In the proof of the second maximum entropy theorem, results about the second-order structure of complex-valued random vectors are exploited. Furthermore, we address the capacity of multiple-input multiple-output (MIMO) channels. Regardless of the speciﬁc distribution of the channel parameters (noise vector and channel matrix, if modeled as random), we show that the capacity-achieving input vector is circular for a broad range of MIMO channels (including coherent and noncoherent scenarios). Finally, we investigate the situation of an improper and Gaussian distributed noise vector. We compute both capacity and capacity-achieving input vector and show that improperness increases capacity, provided that the complementary covariance matrix is exploited. Otherwise, a capacity loss occurs, for which we derive an explicit expression.},
	language = {en},
	number = {5},
	urldate = {2019-06-03},
	journal = {IEEE Transactions on Information Theory},
	author = {Taubock, Georg},
	month = may,
	year = {2012},
	pages = {2729--2744}
}

@phdthesis{amin_complex-valued_2012,
	address = {Department of System Design Engineering},
	title = {Complex-{Valued} {Neural} {Networks}: {Learning} {Algorithms} and {Applications}},
	abstract = {Complex-valued data arise in various applications, such as radar and array signal processing, magnetic resonance imaging, communication systems, and processing data in the frequency domain. To deal with such data properly, neural networks are extended to the complex domain, referred to as complex-valued neural networks (CVNNs), allowing the network parameters to be complex numbers and the computations to follow the complex algebraic rules. Unlike the real-valued case, the nonlinear functions in the CVNNs do not have standard complex derivatives as the Cauchy-Riemann equations do not hold for them. Consequently, the traditional approach for deriving learning algorithms reformulates the problem in the real domain which is often tedious. In this thesis, we first develop a systematic and simpler approach using Wirtinger calculus to derive the learning algorithms in the CVNNs. It is shown that adopting three steps: (i) computing a pair of derivatives in the conjugate coordinate system, (ii) using coordinate transformation between real and conjugate coordinates, and (iii) organizing derivative computations through functional dependency graph greatly simplify the derivations. To illustrate, a gradient descent and LevenbergMarquardt algorithms are considered.

Although a single-layered network, referred to as functional link network (FLN), has been widely used in the real domain because of its simplicity and faster processing, no such study exists in the complex domain. In the FLN, the nonlinearity is endowed in the input layer by constructing linearly independent basis functions in addition to the original variables. We design a parsimonious complex-valued FLN (CFLN) using orthogonal least squares (OLS) method, where the basis functions are multivariate polynomial terms. It is observed that the OLS based CFLN yields simple structure with favorable performance comparing to the multilayer CVNNs in several applications.

It is well known and interesting that a complex-valued neuron can solve several nonlinearly separable problems, including the XOR, parity-n, and symmetry detection problems, which a real-valued neuron cannot. With this motivation, we perform an empirical study of classification performance of single-layered CVNNs on several real-world benchmark classification problems with two new activation functions. The experimental results exhibit that the classification performances of single-layered CVNNs are comparable to those of multilayer real-valued neural networks. Further enhancement of discrimination ability has been obtained using the ensemble approach.},
	language = {en},
	school = {University of Fukui},
	author = {Amin, Faijul},
	year = {2012}
}

@article{jankowski_complex-valued_1996,
	title = {Complex-valued multistate neural associative memory},
	volume = {7},
	issn = {1045-9227},
	doi = {10.1109/72.548176},
	abstract = {A model of a multivalued associative memory is presented. This memory has the form of a fully connected attractor neural network composed of multistate complex-valued neurons. Such a network is able to perform the task of storing and recalling gray-scale images. It is also shown that the complex-valued fully connected neural network may be considered as a generalization of a Hopfield network containing real-valued neurons. A computational energy function is introduced and evaluated in order to prove network stability for asynchronous dynamics. Storage capacity as related to the number of accessible neuron states is also estimated.},
	number = {6},
	journal = {IEEE Transactions on Neural Networks},
	author = {Jankowski, S. and Lozowski, A. and Zurada, J. M.},
	month = nov,
	year = {1996},
	keywords = {Associative memory, asynchronous dynamics, complex-valued multistate neural associative memory, computational energy function, Computer networks, content-addressable storage, fully connected attractor neural network, Gray-scale, gray-scale image recall, gray-scale image storage, Hopfield network, Hopfield neural networks, Image coding, Image recognition, multivalued associative memory, network stability, Neural networks, Neurons, Stability, State estimation, storage capacity},
	pages = {1491--1496}
}

@article{pav_moments_2015,
	title = {Moments of the log non-central chi-square distribution},
	abstract = {The cumulants and moments of the log of the non-central chi-square distribution are derived. For example, the expected log of a chi-square random variable with v degrees of freedom is log(2) + psi(v/2). Applications to modeling probability distributions are discussed.},
	urldate = {2019-06-03},
	journal = {arXiv:1503.06266 [math, stat]},
	author = {Pav, Steven E.},
	month = mar,
	year = {2015},
	keywords = {Statistics - Applications, Mathematics - Probability, 60E07}
}

@phdthesis{wang_study_2017,
	title = {Study on complexity reduction of digital predistortion for power amplifier linearization},
	abstract = {This dissertation contributes to the linearization techniques of high power amplifier using digital predistortion method. High power amplifier is one of the most nonlinear components in radio transmitters. Baseband adaptive digital predistortion is a powerful technique to linearize the power amplifiers and allows to push the power amplifier operation point towards its high efficiency region. Linearization of power amplifiers using digital predistortion with low complexities is the focus of this dissertation. An algorithm is proposed to determine an optimal model structure of single-stage or multi-stage predistorter according to a trade-off between modeling accuracy and model complexity. Multi-stage cascaded digital predistortions are studied with different identification methods, which have advantages on complexity of model identification compared with single-stage structure. In terms of experimental implementations, this dissertation studies the impact of different gain choices on linearized power amplifier. All studies are evaluated with a Doherty power amplifier.},
	language = {en},
	school = {Université Paris-Est},
	author = {Wang, Siqi},
	month = dec,
	year = {2017}
}

@misc{moore_introduction_2006,
	title = {An {Introduction} to {Iterative} {Learning} {Control} {Theory}},
	language = {en},
	author = {Moore, Kevin L},
	year = {2006}
}

@phdthesis{chani-cahuana_digital_2015,
	address = {Department of Signals and Systems},
	title = {Digital {Predistortion} for the {Linearization} of {Power} {Ampliﬁers}},
	abstract = {High efficiency and linearity are indispensable requirements of power amplifiers. Unfortunately they are difficult to obtain simultaneously, since high efficiency PAs are nonlinear and linear PAs may have low efficiency. In order to satisfy the efficiency and linearity requirements, designers preferred to prioritize the efficiency of PAs in the design process and to later recover the linearity using external linearization techniques or architectures. Among the linearization techniques proposed in the literature, digital predistortion (DPD) has drawn the most attention of the industrial and academic sectors because it can provide a good compromise between linearity performance and implementation complexity. This thesis investigates digital predistortion techniques to suppress nonlinear distortion in radio transmitters.},
	language = {en},
	school = {Chalmers University of Technology},
	author = {Chani-Cahuana, Jessica},
	year = {2015}
}

@misc{noauthor_bandpower_nodate,
	title = {Bandpower of an {EEG} signal},
	urldate = {2019-06-03}
}

@misc{karseras_caution:_2014,
	title = {Caution: {The} {Complex} {Normal} {Distribution} !},
	language = {en},
	author = {Karseras, Evripidis},
	month = may,
	year = {2014}
}

@techreport{hunger_introduction_2007,
	title = {An {Introduction} to {Complex} {Differentials} and {Complex} {Differentiability}},
	abstract = {This technical report gives a brief introduction to some elements of complex function theory. First, general definitions for complex differentiability and holomorphic functions are presented. Since non-analytic functions are not complex differentiable, the concept of differentials is explained both for complex-valued and real-valued mappings. Finally, multivariate differentials and Wirtinger derivatives are investigated.},
	language = {en},
	number = {TUM-LNS-TR-07-06},
	institution = {Technische Universität München},
	author = {Hunger, Raphael},
	year = {2007},
	pages = {20}
}

@inproceedings{cohen_group_2016,
	title = {Group {Equivariant} {Convolutional} {Networks}},
	abstract = {We introduce Group equivariant Convolutional Neural Networks (G-CNNs), a natural generalization of convolutional neural networks that reduces sample complexity by exploiting symmetries. G-CNNs use ...},
	language = {en},
	urldate = {2019-06-03},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Cohen, Taco and Welling, Max},
	month = jun,
	year = {2016},
	pages = {2990--2999}
}

@article{maddison_concrete_2016,
	title = {The {Concrete} {Distribution}: {A} {Continuous} {Relaxation} of {Discrete} {Random} {Variables}},
	shorttitle = {The {Concrete} {Distribution}},
	abstract = {The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack useful reparameterizations due to the discontinuous nature of discrete states. In this work we introduce Concrete random variables---continuous relaxations of discrete random variables. The Concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, Concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate the effectiveness of Concrete relaxations on density estimation and structured prediction tasks using neural networks.},
	urldate = {2019-06-08},
	journal = {arXiv:1611.00712 [cs, stat]},
	author = {Maddison, Chris J. and Mnih, Andriy and Teh, Yee Whye},
	month = nov,
	year = {2016},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, queue}
}

@misc{moser_expected_2015,
	title = {Expected {Logarithm} of a {Noncentral} {Chi}-{Square} {Random} {Variable}},
	urldate = {2019-07-26},
	author = {Moser, Stefan M.},
	month = sep,
	year = {2015}
}

@article{kharitonov_variational_2018,
	title = {Variational {Dropout} via {Empirical} {Bayes}},
	abstract = {We study the Automatic Relevance Determination procedure applied to deep neural networks. We show that ARD applied to Bayesian DNNs with Gaussian approximate posterior distributions leads to a variational bound similar to that of variational dropout, and in the case of a fixed dropout rate, objectives are exactly the same. Experimental results show that the two approaches yield comparable results in practice even when the dropout rates are trained. This leads to an alternative Bayesian interpretation of dropout and mitigates some of the theoretical issues that arise with the use of improper priors in the variational dropout model. Additionally, we explore the use of the hierarchical priors in ARD and show that it helps achieve higher sparsity for the same accuracy.},
	urldate = {2019-09-18},
	journal = {arXiv:1811.00596 [cs, stat]},
	author = {Kharitonov, Valery and Molchanov, Dmitry and Vetrov, Dmitry},
	month = nov,
	year = {2018},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, printed}
}

@article{hellings_measuring_2019,
	title = {Measuring impropriety in complex and real representations},
	volume = {164},
	issn = {0165-1684},
	doi = {10.1016/j.sigpro.2019.05.030},
	abstract = {So-called improper signals, i.e., signals which are correlated with their complex conjugates, can occur in many signal processing applications such as communication systems, medical imaging, audio and speech processing, analysis of oceanographic data, and many more. Being aware of potential impropriety can be crucial whenever we model signals as complex random quantities since an appropriate treatment of improper signals, e.g., by widely linear filtering, can significantly improve the system performance. After a brief introduction into the fundamentals of improper signals, this article focuses on the problem of quantifying the impropriety of complex random vectors and gives a survey of various impropriety measures in both the composite real representation and the augmented complex formulation. Unlike in previous publications, these two frameworks are presented side by side to reveal the differences and common points between them. Moreover, their applicability is compared in several practical examples. As additional aspects, we consider the problem of testing for impropriety based on measurement data, and the differential entropy of Gaussian vectors as an impropriety measure in information theoretic studies. The article includes a tutorial-style introduction, a collection of important formulae, a comparison of various mathematical approaches, as well as some new reformulations.},
	urldate = {2019-09-20},
	journal = {Signal Processing},
	author = {Hellings, Christoph and Utschick, Wolfgang},
	month = nov,
	year = {2019},
	keywords = {Augmented complex formulation, Composite real representation, Differential entropy, Improper signals, Impropriety test, Widely linear filtering},
	pages = {267--283}
}

@article{lapidoth_capacity_2003,
	title = {Capacity bounds via duality with applications to multiple-antenna systems on flat-fading channels},
	volume = {49},
	doi = {10.1109/TIT.2003.817449},
	abstract = {A technique is proposed for the derivation of upper bounds on channel capacity. It is based on a dual expression for channel capacity where the maximization (of mutual information) over distributions on the channel input alphabet is replaced with a minimization (of average relative entropy) over distributions on the channel output alphabet. We also propose a technique for the analysis of the asymptotic capacity of cost-constrained channels. The technique is based on the observation that under fairly mild conditions capacity achieving input distributions "escape to infinity." The above techniques are applied to multiple-antenna flat-fading channels with memory where the realization of the fading process is unknown at the transmitter and unknown (or only partially known) at the receiver. It is demonstrated that, for high signal-to-noise ratio (SNR), the capacity of such channels typically grows only double-logarithmically in the SNR. To better understand this phenomenon and the rates at which it occurs, we introduce the fading number as the second-order term in the high-SNR asymptotic expansion of capacity, and derive estimates on its value for various systems. It is suggested that at rates that are significantly higher than the fading number, communication becomes extremely power inefficient, thus posing a practical limit on practically achievable rates. Upper and lower bounds on the fading number are also presented. For single-input-single-output (SISO) systems the bounds coincide, thus yielding a complete characterization of the fading number for general stationary and ergodic fading processes. We also demonstrate that for memoryless multiple-input single-output (MISO) channels, the fading number is achievable using beam-forming, and we derive an expression for the optimal beam direction. This direction depends on the fading law and is, in general, not the direction that maximizes the SNR on the induced SISO channel. Using a new closed-form expression for the expectation of the logarithm of a noncentral chi-square distributed random variable we provide some closed-form expressions for the fading number of some systems with Gaussian fading, including SISO systems with circularly symmetric stationary and ergodic Gaussian fading. The fading number of the latter is determined by the fading mean, fading variance, and the mean squared error in predicting the present fading from its past; it is not directly related to the Doppler spread. For the Rayleigh, Ricean, and multiple-antenna Rayleigh-fading channels we also present firm (nonasymptotic) upper and lower bounds on channel capacity. These bounds are asymptotically tight in the sense that their difference from capacity approaches zero at high SNR, and their ratio to capacity approaches one at low SNR.},
	number = {10},
	journal = {IEEE Transactions on Information Theory},
	author = {Lapidoth, Amos and Moser, Stefan M.},
	month = oct,
	year = {2003},
	keywords = {Signal to noise ratio, antenna arrays, antenna theory, asymptotic capacity, beam direction, beamforming, capacity bounds, channel capacity, Channel capacity, circularly symmetric stationary, circularly symmetric stationary fading, closed-form expression, Closed-form solution, cost-constrained channels, dual expression, duality, Entropy, ergodic fading processes, ergodic Gaussian fading, Fading, fading mean, fading variance, flat-fading channels, H infinity control, lower bounds, mean squared error, memoryless multiple-input single-output channels, MIMO systems, MISO channels, multiple-antenna systems, Mutual information, noncentral chi-square distributed random variable, Rayleigh channels, Ricean channels, Rician channels, second-order term, signal-to-noise ratio, single-input-single-output channel, SISO systems, stationary fading processes, Transmitters, Upper bound, upper bounds},
	pages = {2426--2467}
}

@article{schoukens_obtaining_2017,
	title = {Obtaining the {Pre}-{Inverse} of a {Power} {Amplifier} using {Iterative} {Learning} {Control}},
	volume = {65},
	issn = {0018-9480, 1557-9670},
	doi = {10.1109/TMTT.2017.2694822},
	abstract = {Telecommunication networks make extensive use of power amplifiers to broaden the coverage from transmitter to receiver. Achieving high power efficiency is challenging and comes at a price: the wanted linear performance is degraded due to nonlinear effects. To compensate for these nonlinear disturbances, existing techniques compute the pre-inverse of the power amplifier by estimation of a nonlinear model. However, the extraction of this nonlinear model is involved and requires advanced system identification techniques. We used the plant inversion iterative learning control algorithm to investigate whether the nonlinear modeling step can be simplified. This paper introduces the iterative learning control framework for the pre-inverse estimation and predistortion of power amplifiers. The iterative learning control algorithm is used to obtain a high quality predistorted input for the power amplifier under study without requiring a nonlinear model of the power amplifier. In a second step a nonlinear pre-inverse model of the amplifier is obtained. Both the nonlinear and memory effects of a power amplifier can be compensated by this approach. The convergence of the iterative approach, and the predistortion results are illustrated on a simulation of a Motorola LDMOS transistor based power amplifier and a measurement example using the Chalmers RF WebLab measurement setup.},
	number = {11},
	urldate = {2019-10-03},
	journal = {IEEE Transactions on Microwave Theory and Techniques},
	author = {Schoukens, Maarten and Hammenecker, Jules and Cooman, Adam},
	month = nov,
	year = {2017},
	keywords = {Electrical Engineering and Systems Science - Systems and Control},
	pages = {4266--4273}
}

@article{traverso_low_2019,
	title = {Low {Complexity} {Time} {Synchronization} {Based} on {Digital} {Predistortion} {Coefficients}},
	volume = {29},
	doi = {10.1109/LMWC.2019.2895544},
	abstract = {In this letter, we propose an efficient loop-delay estimation approach for digital predistortion (DPD) based on the DPD linear coefficients. The basic concept of our approach consists in considering that the linear coefficients as computed by the DPD act as a linear filter that represents the linear response of the power amplifier, providing very accurate information about loop delay. Our approach has the great advantage of avoiding the need of a dedicated high-precision loop-delay estimator while using DPD. Two different estimators based on the DPD linear coefficients are presented, and their performance characteristics are illustrated on a measurement example using the Chalmers RF WebLab measurement setup.},
	number = {3},
	journal = {IEEE Microwave and Wireless Components Letters},
	author = {Traverso, S. and Bernier, J.},
	month = mar,
	year = {2019},
	keywords = {Baseband, Chalmers RF WebLab measurement setup, Correlation, delay estimation, Delay estimation, delays, Delays, digital predistortion, digital predistortion (DPD), distortion, DPD linear coefficients, efficient loop-delay estimation approach, Estimation, high-precision loop-delay estimator, Indexes, linear filter, linear response, linearisation techniques, loop delay, low complexity time synchronization, performance characteristics, power amplifier, power amplifiers, synchronisation, Synchronization, Wireless communication},
	pages = {240--242}
}

@article{yan_generalized_2018,
	title = {Generalized {Proper} {Complex} {Gaussian} {Ratio} {Distribution} and {Its} {Application} to {Statistical} {Inference} for {Frequency} {Response} {Functions}},
	volume = {144},
	doi = {10.1061/(ASCE)EM.1943-7889.0001504},
	abstract = {The frequency response function governs many important processes. Defined as the quotients of the fast Fourier transform coefficients, frequency response functions can be modeled as ratio random variables in the complex domain. The circularly symmetric complex normal ratio distribution proposed previously is restricted to quantifying the uncertainties for the quotients of complex Gaussian random variables with zero mean. Such limitation motivates research to enlarge the group of probability distributions, allowing a wider scope of applicability. This study provides a theoretical proof for the closed-form of a generalized proper complex Gaussian ratio distribution using the principle of probability density transformation in tandem with an advanced integral technique. The equivalence between the distribution properties of complex ratio random variables and their counterparts in the real-valued domain is also proven. The generalized proper complex Gaussian ratio distribution is then used to infer the statistics of frequency response functions. Stochastic simulation and experimental study are used to demonstrate the goodness and efficiency of the proposed probabilistic model.},
	number = {9},
	journal = {Journal of Engineering Mechanics},
	author = {Yan, Wang-Ji and Ren, Wei-Xin},
	year = {2018},
	pages = {04018080}
}

@article{ancarani_derivatives_2008,
	title = {Derivatives of any order of the confluent hypergeometric function {F11}(a,b,z) with respect to the parameter a or b},
	volume = {49},
	issn = {0022-2488},
	doi = {10.1063/1.2939395},
	number = {6},
	urldate = {2019-10-21},
	journal = {Journal of Mathematical Physics},
	author = {Ancarani, L. U. and Gasaneo, G.},
	month = jun,
	year = {2008},
	pages = {063508}
}

@incollection{novikov_tensorizing_2015,
	title = {Tensorizing {Neural} {Networks}},
	urldate = {2019-11-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 28},
	publisher = {Curran Associates, Inc.},
	author = {Novikov, Alexander and Podoprikhin, Dmitrii and Osokin, Anton and Vetrov, Dmitry P},
	editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
	year = {2015},
	pages = {442--450}
}

@article{kingma_auto-encoding_2014,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	urldate = {2019-11-04},
	journal = {arXiv:1312.6114 [cs, stat]},
	author = {Kingma, Diederik P. and Welling, Max},
	month = may,
	year = {2014},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{figurnov_implicit_2019,
	title = {Implicit {Reparameterization} {Gradients}},
	abstract = {By providing a simple and efficient way of computing low-variance gradients of continuous random variables, the reparameterization trick has become the technique of choice for training a variety of latent variable models. However, it is not applicable to a number of important continuous distributions. We introduce an alternative approach to computing reparameterization gradients based on implicit differentiation and demonstrate its broader applicability by applying it to Gamma, Beta, Dirichlet, and von Mises distributions, which cannot be used with the classic reparameterization trick. Our experiments show that the proposed approach is faster and more accurate than the existing gradient estimators for these distributions.},
	urldate = {2019-11-04},
	journal = {arXiv:1805.08498 [cs, stat]},
	author = {Figurnov, Michael and Mohamed, Shakir and Mnih, Andriy},
	month = jan,
	year = {2019},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{ranganath_operator_2018,
	title = {Operator {Variational} {Inference}},
	abstract = {Variational inference is an umbrella term for algorithms which cast Bayesian inference as optimization. Classically, variational inference uses the Kullback-Leibler divergence to define the optimization. Though this divergence has been widely used, the resultant posterior approximation can suffer from undesirable statistical properties. To address this, we reexamine variational inference from its roots as an optimization problem. We use operators, or functions of functions, to design variational objectives. As one example, we design a variational objective with a Langevin-Stein operator. We develop a black box algorithm, operator variational inference (OPVI), for optimizing any operator objective. Importantly, operators enable us to make explicit the statistical and computational tradeoffs for variational inference. We can characterize different properties of variational objectives, such as objectives that admit data subsampling---allowing inference to scale to massive data---as well as objectives that admit variational programs---a rich class of posterior approximations that does not require a tractable density. We illustrate the benefits of OPVI on a mixture model and a generative model of images.},
	urldate = {2019-11-04},
	journal = {arXiv:1610.09033 [cs, stat]},
	author = {Ranganath, Rajesh and Altosaar, Jaan and Tran, Dustin and Blei, David M.},
	month = mar,
	year = {2018},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Computation, Statistics - Methodology}
}

@article{jang_categorical_2017,
	title = {Categorical {Reparameterization} with {Gumbel}-{Softmax}},
	abstract = {Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.},
	urldate = {2019-11-04},
	journal = {arXiv:1611.01144 [cs, stat]},
	author = {Jang, Eric and Gu, Shixiang and Poole, Ben},
	month = aug,
	year = {2017},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{yang_complex_2019,
	title = {Complex {Transformer}: {A} {Framework} for {Modeling} {Complex}-{Valued} {Sequence}},
	shorttitle = {Complex {Transformer}},
	abstract = {While deep learning has received a surge of interest in a variety of fields in recent years, major deep learning models barely use complex numbers. However, speech, signal and audio data are naturally complex-valued after Fourier Transform, and studies have shown a potentially richer representation of complex nets. In this paper, we propose a Complex Transformer, which incorporates the transformer model as a backbone for sequence modeling; we also develop attention and encoder-decoder network operating for complex input. The model achieves state-of-the-art performance on the MusicNet dataset and an In-phase Quadrature (IQ) signal dataset.},
	urldate = {2019-11-04},
	journal = {arXiv:1910.10202 [cs, eess, stat]},
	author = {Yang, Muqiao and Ma, Martin Q. and Li, Dongyu and Tsai, Yao-Hung Hubert and Salakhutdinov, Ruslan},
	month = oct,
	year = {2019},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@article{benvenuto_complex_1992,
	title = {On the complex backpropagation algorithm},
	volume = {40},
	issn = {1053-587X, 1941-0476},
	doi = {10.1109/78.127967},
	abstract = {A recursive algorithm for updating the coefficients of a neural network structure for complex signals is presented. Various complex activation functions are considered and a practical definition is proposed. The method, associated to a mean-square-error criterion, yields the complex form of the conventional backpropagation algorithm.{\textless}{\textgreater}},
	number = {4},
	journal = {IEEE Transactions on Signal Processing},
	author = {Benvenuto, N. and Piazza, F.},
	month = apr,
	year = {1992},
	keywords = {Signal processing algorithms, signal processing, Signal processing, Equations, Neural networks, Neurons, Backpropagation algorithms, coefficients updating, complex activation functions, complex backpropagation algorithm, complex signals, mean-square-error criterion, Multi-layer neural network, Multilayer perceptrons, neural nets, neural network, Nonlinear filters, recursive algorithm, Wiener filter},
	pages = {967--969}
}

@inproceedings{titsias_doubly_2014,
	title = {Doubly {Stochastic} {Variational} {Bayes} for non-{Conjugate} {Inference}},
	abstract = {We propose a simple and effective variational inference algorithm based on stochastic optimisation   that can be widely applied for Bayesian non-conjugate inference in continuous parameter spaces. ...},
	language = {en},
	urldate = {2019-11-14},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Titsias, Michalis and Lázaro-Gredilla, Miguel},
	month = jan,
	year = {2014},
	pages = {1971--1979}
}

@inproceedings{wang_fast_2013,
	title = {Fast dropout training},
	abstract = {Preventing feature co-adaptation by encouraging independent contributions from different features often improves classification and regression performance.  Dropout training (Hinton et al., 2012) d...},
	language = {en},
	urldate = {2019-11-05},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Wang, Sida and Manning, Christopher},
	month = feb,
	year = {2013},
	pages = {118--126}
}

@inproceedings{wan_regularization_2013,
	title = {Regularization of {Neural} {Networks} using {DropConnect}},
	abstract = {We introduce DropConnect, a generalization of DropOut, for regularizing large fully-connected layers within neural networks. When training with Dropout, a randomly selected subset of activations ar...},
	language = {en},
	urldate = {2019-11-05},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Wan, Li and Zeiler, Matthew and Zhang, Sixin and Cun, Yann Le and Fergus, Rob},
	month = feb,
	year = {2013},
	pages = {1058--1066}
}

@article{srivastava_dropout:_2014,
	title = {Dropout: {A} {Simple} {Way} to {Prevent} {Neural} {Networks} from {Overfitting}},
	volume = {15},
	shorttitle = {Dropout},
	urldate = {2019-11-05},
	journal = {Journal of Machine Learning Research},
	author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	year = {2014},
	pages = {1929--1958}
}

@article{hinton_improving_2012,
	title = {Improving neural networks by preventing co-adaptation of feature detectors},
	abstract = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.},
	urldate = {2019-11-05},
	journal = {arXiv:1207.0580 [cs]},
	author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
	month = jul,
	year = {2012},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing}
}

@misc{javed_reinforce_nodate,
	title = {{REINFORCE} vs {Reparameterization} {Trick}},
	abstract = {An introduction and comparison of two popular techniques for estimating gradients in machine learning models},
	urldate = {2019-11-05},
	author = {Javed, Syed Ashar}
}

@article{williams_simple_1992,
	title = {Simple statistical gradient-following algorithms for connectionist reinforcement learning},
	volume = {8},
	issn = {1573-0565},
	doi = {10.1007/BF00992696},
	abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
	language = {en},
	number = {3},
	urldate = {2019-11-05},
	journal = {Machine Learning},
	author = {Williams, Ronald J.},
	month = may,
	year = {1992},
	keywords = {connectionist networks, gradient descent, mathematical analysis, Reinforcement learning},
	pages = {229--256}
}

@article{zuo_compression_2019,
	title = {On {Compression} of {Unsupervised} {Neural} {Nets} by {Pruning} {Weak} {Connections}},
	abstract = {Unsupervised neural nets such as Restricted Boltzmann Machines(RBMs) and Deep Belif Networks(DBNs), are powerful in automatic feature extraction,unsupervised weight initialization and density estimation. In this paper,we demonstrate that the parameters of these neural nets can be dramatically reduced without affecting their performance. We describe a method to reduce the parameters required by RBM which is the basic building block for deep architectures. Further we propose an unsupervised sparse deep architectures selection algorithm to form sparse deep neural networks.Experimental results show that there is virtually no loss in either generative or discriminative performance.},
	urldate = {2019-11-15},
	journal = {arXiv:1901.07066 [cs]},
	author = {Zuo, Zhiwen and Zhao, Lei and Zuo, Liwen and Jiang, Feng and Xing, Wei and Lu, Dongming},
	month = jan,
	year = {2019},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing}
}

@article{gomez_learning_2019,
	title = {Learning {Sparse} {Networks} {Using} {Targeted} {Dropout}},
	abstract = {Neural networks are easier to optimise when they have many more weights than are required for modelling the mapping from inputs to outputs. This suggests a two-stage learning procedure that first learns a large net and then prunes away connections or hidden units. But standard training does not necessarily encourage nets to be amenable to pruning. We introduce targeted dropout, a method for training a neural network so that it is robust to subsequent pruning. Before computing the gradients for each weight update, targeted dropout stochastically selects a set of units or weights to be dropped using a simple self-reinforcing sparsity criterion and then computes the gradients for the remaining weights. The resulting network is robust to post hoc pruning of weights or units that frequently occur in the dropped sets. The method improves upon more complicated sparsifying regularisers while being simple to implement and easy to tune.},
	urldate = {2019-11-15},
	journal = {arXiv:1905.13678 [cs, stat]},
	author = {Gomez, Aidan N. and Zhang, Ivan and Kamalakara, Siddhartha Rao and Madaan, Divyam and Swersky, Kevin and Gal, Yarin and Hinton, Geoffrey E.},
	month = sep,
	year = {2019},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{cheung_superposition_2019,
	title = {Superposition of many models into one},
	abstract = {We present a method for storing multiple models within a single set of parameters. Models can coexist in superposition and still be retrieved individually. In experiments with neural networks, we show that a surprisingly large number of models can be effectively stored within a single parameter instance. Furthermore, each of these models can undergo thousands of training steps without significantly interfering with other models within the superposition. This approach may be viewed as the online complement of compression: rather than reducing the size of a network after training, we make use of the unrealized capacity of a network during training.},
	urldate = {2019-11-21},
	journal = {arXiv:1902.05522 [cs]},
	author = {Cheung, Brian and Terekhov, Alex and Chen, Yubei and Agrawal, Pulkit and Olshausen, Bruno},
	month = jun,
	year = {2019},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing}
}

@article{thickstun_learning_2017,
	title = {Learning {Features} of {Music} from {Scratch}},
	abstract = {This paper introduces a new large-scale music dataset, MusicNet, to serve as a source of supervision and evaluation of machine learning methods for music research. MusicNet consists of hundreds of freely-licensed classical music recordings by 10 composers, written for 11 instruments, together with instrument/note annotations resulting in over 1 million temporal labels on 34 hours of chamber music performances under various studio and microphone conditions. The paper defines a multi-label classification task to predict notes in musical recordings, along with an evaluation protocol, and benchmarks several machine learning architectures for this task: i) learning from spectrogram features; ii) end-to-end learning with a neural net; iii) end-to-end learning with a convolutional neural net. These experiments show that end-to-end models trained for note prediction learn frequency selective filters as a low-level representation of audio.},
	urldate = {2019-11-24},
	journal = {arXiv:1611.09827 [cs, stat]},
	author = {Thickstun, John and Harchaoui, Zaid and Kakade, Sham},
	month = apr,
	year = {2017},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Sound}
}

@article{aizenberg_multilayer_2007,
	title = {Multilayer feedforward neural network based on multi-valued neurons ({MLMVN}) and a backpropagation learning algorithm},
	volume = {11},
	abstract = {A multilayer neural network based on multi-valued neurons is considered in the paper. A multivalued neuron (MVN) is based on the principles of multiple-valued threshold logic over the field of the complex numbers. The most important properties of MVN are: the complex-valued weights, inputs and output coded by the k th roots of unity and the activation function, which maps the complex plane into the unit circle. MVN learning is reduced to the movement along the unit circle, it is based on a simple linear error correction rule and it does not require a derivative. It is shown that using a traditional architecture of multilayer feedforward neural network (MLF) and the high functionality of the multi-valued neuron, it is possible to obtain a new powerful neural network. Its training does not require a derivative of the activation function and its functionality is higher than the functionality of MLF containing the same number of layers and neurons. These advantages of MLMVN are confirmed by testing using parity n, two spirals and "sonar" benchmarks and the Mackey-Glass time series prediction.},
	number = {2},
	journal = {Soft Computing},
	author = {Aizenberg, Igor and Moraga, Claudio},
	year = {2007},
	pages = {2007}
}

@article{faijul_amin_single-layered_2009,
	series = {Brain {Inspired} {Cognitive} {Systems} ({BICS} 2006) / {Interplay} {Between} {Natural} and {Artificial} {Computation} ({IWINAC} 2007)},
	title = {Single-layered complex-valued neural network for real-valued classification problems},
	volume = {72},
	issn = {0925-2312},
	doi = {10.1016/j.neucom.2008.04.006},
	abstract = {This paper presents a model of complex-valued neuron (CVN) for real-valued classification problems, introducing two new activation functions. In this CVN model, each real-valued input is encoded into a phase between 0 and π of a complex number of unity magnitude, and multiplied by a complex-valued weight. The weighted sum of inputs is then fed to an activation function. Both the proposed activation functions map complex values into real values, and their role is to divide the net-input (weighted sum) space into multiple regions representing the classes of input patterns. Gradient-based learning rules are derived for each of the activation functions. The ability of such CVN is discussed and tested with two-class problems, such as two- and three-input Boolean problems, and the symmetry detection in binary sequences. We show here that the CVN with both activation functions can form proper boundaries for these linear and nonlinear problems. For solving n-class problems, a complex-valued neural network (CVNN) consisting of n CVNs is also studied. We defined the one exhibiting the largest output among all the neurons as representing the output class. We tested such single-layered CVNNs on several real world benchmark problems. The results show that the classification ability of single-layered CVNN on unseen data is comparable to the conventional real-valued neural network (RVNN) having one hidden layer. Moreover, convergence of the CVNN is much faster than that of the RVNN in most cases.},
	language = {en},
	number = {4},
	urldate = {2019-11-22},
	journal = {Neurocomputing},
	author = {Faijul Amin, Md. and Murase, Kazuyuki},
	month = jan,
	year = {2009},
	keywords = {Activation function, Classification, Complex-valued neural networks, Generalization, Phase-encoding},
	pages = {945--955}
}

@inproceedings{wolter_complex_2018,
	address = {USA},
	series = {{NIPS}'18},
	title = {Complex {Gated} {Recurrent} {Neural} {Networks}},
	abstract = {Complex numbers have long been favoured for digital signal processing, yet complex representations rarely appear in deep learning architectures. RNNs, widely used to process time series and sequence information, could greatly benefit from complex representations. We present a novel complex gated recurrent cell, which is a hybrid cell combining complex-valued and norm-preserving state transitions with a gating mechanism. The resulting RNN exhibits excellent stability and convergence properties and performs competitively on the synthetic memory and adding task, as well as on the real-world tasks of human motion prediction.},
	urldate = {2019-11-22},
	booktitle = {Proceedings of the {32Nd} {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Wolter, Moritz and Yao, Angela},
	year = {2018},
	pages = {10557--10567}
}

@inproceedings{hirose_complex-valued_2009,
	title = {Complex-valued neural networks: {The} merits and their origins},
	shorttitle = {Complex-valued neural networks},
	doi = {10.1109/IJCNN.2009.5178754},
	abstract = {This paper discusses what the merits of complex-valued neural networks (CVNNs) arise from. First we look back the mathematical history to elucidate the features of complex numbers, in particular to confirm the importance of the phase-and-amplitude viewpoint for designing and constructing CVNNs to enhance the features. The viewpoint is essential in general to deal with waves such as electromagnetic-wave and lightwave. Then we point out that, although we represent a complex number as an ordered pair of real numbers for example, we can reduce ineffective degree of freedom in learning or self-organization in CVNNs to achieve better generalization characteristics. This wave-oriented merit is useful widely for general signal processing with Fourier synthesis or in frequency-domain treatment through Fourier transform.},
	booktitle = {2009 {International} {Joint} {Conference} on {Neural} {Networks}},
	author = {Hirose, Akira},
	month = jun,
	year = {2009},
	keywords = {signal processing, Neural networks, neural nets, complex numbers, complex-valued neural networks, electromagnetic-wave, Fourier synthesis, Fourier transform, Fourier transforms, frequency-domain analysis, frequency-domain treatment, general signal processing},
	pages = {1237--1244}
}

@inproceedings{arjovsky_unitary_2016,
	title = {Unitary {Evolution} {Recurrent} {Neural} {Networks}},
	abstract = {Recurrent neural networks (RNNs) are notoriously difficult to train. When the eigenvalues of the hidden to hidden weight matrix deviate from absolute value 1, optimization becomes difficult due to ...},
	language = {en},
	urldate = {2019-11-22},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Arjovsky, Martin and Shah, Amar and Bengio, Yoshua},
	month = jun,
	year = {2016},
	pages = {1120--1128}
}

@incollection{wisdom_full-capacity_2016,
	title = {Full-{Capacity} {Unitary} {Recurrent} {Neural} {Networks}},
	urldate = {2019-11-22},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 29},
	publisher = {Curran Associates, Inc.},
	author = {Wisdom, Scott and Powers, Thomas and Hershey, John and Le Roux, Jonathan and Atlas, Les},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	year = {2016},
	pages = {4880--4888}
}

@inproceedings{he_amc:_2018,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{AMC}: {AutoML} for {Model} {Compression} and {Acceleration} on {Mobile} {Devices}},
	isbn = {978-3-030-01234-2},
	shorttitle = {{AMC}},
	doi = {10.1007/978-3-030-01234-2_48},
	abstract = {Model compression is an effective technique to efficiently deploy neural network models on mobile devices which have limited computation resources and tight power budgets. Conventional model compression techniques rely on hand-crafted features and require domain experts to explore the large design space trading off among model size, speed, and accuracy, which is usually sub-optimal and time-consuming. In this paper, we propose AutoML for Model Compression (AMC) which leverages reinforcement learning to efficiently sample the design space and can improve the model compression quality. We achieved state-of-the-art model compression results in a fully automated way without any human efforts. Under 4××{\textbackslash}times FLOPs reduction, we achieved 2.7\% better accuracy than the hand-crafted model compression method for VGG-16 on ImageNet. We applied this automated, push-the-button compression pipeline to MobileNet-V1 and achieved a speedup of 1.53××{\textbackslash}times on the GPU (Titan Xp) and 1.95××{\textbackslash}times on an Android phone (Google Pixel 1), with negligible loss of accuracy.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {He, Yihui and Lin, Ji and Liu, Zhijian and Wang, Hanrui and Li, Li-Jia and Han, Song},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	keywords = {Reinforcement learning, AutoML, CNN acceleration, Mobile vision, Model compression},
	pages = {815--832}
}

@article{boeddeker_computation_2019,
	title = {On the {Computation} of {Complex}-valued {Gradients} with {Application} to {Statistically} {Optimum} {Beamforming}},
	abstract = {This report describes the computation of gradients by algorithmic differentiation for statistically optimum beamforming operations. Especially the derivation of complex-valued functions is a key component of this approach. Therefore the real-valued algorithmic differentiation is extended via the complex-valued chain rule. In addition to the basic mathematic operations the derivative of the eigenvalue problem with complex-valued eigenvectors is one of the key results of this report. The potential of this approach is shown with experimental results on the CHiME-3 challenge database. There, the beamforming task is used as a front-end for an ASR system. With the developed derivatives a joint optimization of a speech enhancement and speech recognition system w.r.t. the recognition optimization criterion is possible.},
	urldate = {2019-12-02},
	journal = {arXiv:1701.00392 [cs]},
	author = {Boeddeker, Christoph and Hanebrink, Patrick and Drude, Lukas and Heymann, Jahn and Haeb-Umbach, Reinhold},
	month = feb,
	year = {2019},
	keywords = {Mathematics - Numerical Analysis, Computer Science - Computational Engineering, Finance, and Science}
}

@article{wolter_fourier_2019,
	title = {Fourier {RNNs} for {Sequence} {Prediction}},
	abstract = {Fourier methods have a long and proven track record as an excellent tool in data processing. We propose to integrate Fourier methods into complex recurrent neural network architectures and show accuracy improvements on prediction tasks as well as computational load reductions. We predict synthetic data drawn from synthetic-equations as well as real world power load data.},
	urldate = {2019-12-02},
	journal = {arXiv:1812.05645 [cs, stat]},
	author = {Wolter, Moritz and Yao, Angela},
	month = may,
	year = {2019},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{duan_bounding_2011,
	title = {Bounding the {Fat} {Shattering} {Dimension} of a {Composition} {Function} {Class} {Built} {Using} a {Continuous} {Logic} {Connective}},
	abstract = {We begin this report by describing the Probably Approximately Correct (PAC) model for learning a concept class, consisting of subsets of a domain, and a function class, consisting of functions from the domain to the unit interval. Two combinatorial parameters, the Vapnik-Chervonenkis (VC) dimension and its generalization, the Fat Shattering dimension of scale e, are explained and a few examples of their calculations are given with proofs. We then explain Sauer's Lemma, which involves the VC dimension and is used to prove the equivalence of a concept class being distribution-free PAC learnable and it having finite VC dimension. As the main new result of our research, we explore the construction of a new function class, obtained by forming compositions with a continuous logic connective, a uniformly continuous function from the unit hypercube to the unit interval, from a collection of function classes. Vidyasagar had proved that such a composition function class has finite Fat Shattering dimension of all scales if the classes in the original collection do; however, no estimates of the dimension were known. Using results by Mendelson-Vershynin and Talagrand, we bound the Fat Shattering dimension of scale e of this new function class in terms of the Fat Shattering dimensions of the collection's classes. We conclude this report by providing a few open questions and future research topics involving the PAC learning model.},
	urldate = {2020-01-31},
	journal = {arXiv:1105.4618 [cs]},
	author = {Duan, Hubert Haoyang},
	month = may,
	year = {2011},
	keywords = {Computer Science - Machine Learning}
}

@article{schmitt_complexity_2002,
	title = {On the {Complexity} of {Computing} and {Learning} with {Multiplicative} {Neural} {Networks}},
	volume = {14},
	issn = {0899-7667},
	doi = {10.1162/08997660252741121},
	abstract = {In a great variety of neuron models, neural inputs are combined using the summing operation. We introduce the concept of multiplicative neural networks that contain units that multiply their inputs instead of summing them and thus allow inputs to interact nonlinearly. The class of multiplicative neural networks comprises such widely known and well-studied network types as higher-order networks and product unit networks. We investigate the complexity of computing and learning for multiplicative neural networks. In particular, we derive upper and lower bounds on the Vapnik-Chervonenkis (VC) dimension and the pseudo-dimension for various types of networks with multiplicative units. As the most general case, we consider feedforward networks consisting of product and sigmoidal units, showing that their pseudo-dimension is bounded from above by a polynomial with the same order of magnitude as the currently best-known bound for purely sigmoidal networks. Moreover, we show that this bound holds even when the unit type, product or sigmoidal, may be learned. Crucial for these results are calculations of solution set components bounds for new network classes. As to lower bounds, we construct product unit networks of fixed depth with super-linear VC dimension. For sigmoidal networks of higher order, we establish polynomial bounds that, in contrast to previous results, do not involve any restriction of the network order. We further consider various classes of higher-order units, also known as sigma-pi units, that are characterized by connectivity constraints. In terms of these, we derive some asymptotically tight bounds. Multiplication plays an important role in both neural modeling of biological behavior and computing and learning with artificial neural networks. We briefly survey research in biology and in applications where multiplication is considered an essential computational element. The results we present here provide new tools for assessing the impact of multiplication on the computational power and the learning capabilities of neural networks.},
	number = {2},
	urldate = {2020-01-31},
	journal = {Neural Computation},
	author = {Schmitt, Michael},
	month = feb,
	year = {2002},
	pages = {241--301}
}

@article{hinton_distilling_2015,
	title = {Distilling the {Knowledge} in a {Neural} {Network}},
	abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
	urldate = {2020-01-17},
	journal = {arXiv:1503.02531 [cs, stat]},
	author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	month = mar,
	year = {2015},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing}
}

@inproceedings{chen_fxpnet_2017,
	title = {{FxpNet}: {Training} a deep convolutional neural network in fixed-point representation},
	shorttitle = {{FxpNet}},
	doi = {10.1109/IJCNN.2017.7966159},
	abstract = {We introduce FxpNet, a framework to train deep convolutional neural networks with low bit-width arithmetics in both forward pass and backward pass. During training FxpNet further reduces the bit-width of stored parameters (also known as primal parameters) by adaptively updating their fixed-point formats. These primal parameters are usually represented in the full resolution of floating-point values in previous binarized and quantized neural networks. In FxpNet, during forward pass fixed-point primal weights and activations are first binarized before computation, while in backward pass all gradients are represented as low resolution fixed-point values and then accumulated to corresponding fixed-point primal parameters. To have highly efficient implementations in FPGAs, ASICs and other dedicated devices, FxpNet introduces Integer Batch Normalization (IBN) and Fixed-point ADAM (FxpADAM) methods to further reduce the required floating-point operations, which will save considerable power and chip area. The evaluation on CIFAR-10 dataset indicates the effectiveness that FxpNet with 12-bit primal parameters and 12-bit gradients achieves comparable prediction accuracy with state-of-the-art binarized and quantized neural networks.},
	booktitle = {2017 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Chen, Xi and Hu, Xiaolin and Zhou, Hucheng and Xu, Ningyi},
	month = may,
	year = {2017},
	keywords = {Neural networks, neural nets, 12-bit gradients, 12-bit primal parameters, Acceleration, application specific integrated circuits, ASICs, backward pass, binarized neural networks, bit-width arithmetics, CIFAR-10 dataset, convolution, Convolution, deep convolutional neural network, field programmable gate arrays, Field programmable gate arrays, fixed point arithmetic, fixed-point ADAM, fixed-point primal parameters, fixed-point primal weights, fixed-point representation, floating point arithmetic, floating-point values, forward pass, FPGAs, FxpADAM, FxpNet, IBN, integer batch normalization, Kernel, low resolution fixed-point values, Quantization (signal), quantized neural networks, Training},
	pages = {2494--2501}
}

@article{simonyan_very_2015,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	urldate = {2020-02-04},
	journal = {arXiv:1409.1556 [cs]},
	author = {Simonyan, Karen and Zisserman, Andrew},
	month = apr,
	year = {2015},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{wilson_bayesian_2020,
	title = {Bayesian {Deep} {Learning} and a {Probabilistic} {Perspective} of {Generalization}},
	abstract = {The key distinguishing property of a Bayesian approach is marginalization, rather than using a single setting of weights. Bayesian marginalization can particularly improve the accuracy and calibration of modern deep neural networks, which are typically underspecified by the data, and can represent many compelling but different solutions. We show that deep ensembles provide an effective mechanism for approximate Bayesian marginalization, and propose a related approach that further improves the predictive distribution by marginalizing within basins of attraction, without significant overhead. We also investigate the prior over functions implied by a vague distribution over neural network weights, explaining the generalization properties of such models from a probabilistic perspective. From this perspective, we explain results that have been presented as mysterious and distinct to neural network generalization, such as the ability to fit images with random labels, and show that these results can be reproduced with Gaussian processes. Finally, we provide a Bayesian perspective on tempering for calibrating predictive distributions.},
	urldate = {2020-03-04},
	journal = {arXiv:2002.08791 [cs, stat]},
	author = {Wilson, Andrew Gordon and Izmailov, Pavel},
	month = feb,
	year = {2020},
	keywords = {Computer Science - Machine Learning, read, Statistics - Machine Learning}
}

@book{neal_bayesian_1996,
	address = {New York, NY},
	series = {Lecture {Notes} in {Statistics}},
	title = {Bayesian {Learning} for {Neural} {Networks}},
	volume = {118},
	isbn = {978-0-387-94724-2 978-1-4612-0745-0},
	abstract = {Two features distinguish the Bayesian approach to learning models from data. First, beliefs derived from background knowledge are used to select a prior probability distribution for the model parameters. Second, predictions of future observations are made by integrating the model's predictions with respect to the posterior parameter distribution obtained by updating this prior to take account of the data. For neural network models, both these aspects present di culties {\textbar} the prior over network parameters has no obvious relation to our prior knowledge, and integration over the posterior is computationally very demanding.},
	language = {en},
	urldate = {2020-03-18},
	publisher = {Springer New York},
	author = {Neal, Radford M.},
	editor = {Bickel, P. and Diggle, P. and Fienberg, S. and Krickeberg, K. and Olkin, I. and Wermuth, N. and Zeger, S.},
	year = {1996},
	doi = {10.1007/978-1-4612-0745-0}
}

@article{mackay_probable_1995,
	title = {Probable networks and plausible predictions — a review of practical {Bayesian} methods for supervised neural networks},
	volume = {6},
	issn = {0954-898X},
	doi = {10.1088/0954-898X_6_3_011},
	abstract = {Bayesian probability theory provides a unifying framework for data modelling. In this framework the overall aims are to find models that are well-matched to the data, and to use these models to make optimal predictions. Neural network learning is interpreted as an inference of the most probable parameters for the model, given the training data. The search in model space (i.e., the space of architectures, noise models, preprocessings, regularizers and weight decay constants) can then also be treated as an inference problem, in which we infer the relative probability of alternative models, given the data. This review describes practical techniques based on Gaussian approximations for implementation of these powerful methods for controlling, comparing and using adaptive networks.},
	number = {3},
	urldate = {2020-03-20},
	journal = {Network: Computation in Neural Systems},
	author = {Mackay, David J. C.},
	month = jan,
	year = {1995},
\_eprint: https://doi.org/10.1088/0954-898X\_6\_3\_011},
	pages = {469--505}
}

@inproceedings{mackay_bayesian_1994,
	title = {Bayesian {Non}-linear {Modelling} for the {Prediction} {Competition}},
	abstract = {The 1993 energy prediction competition involved the prediction of a series of building energy loads from a series of environmental input variables. Non-linear regression using `neural networks' is a popular technique for such modeling tasks. Since it is not obvious how large a time-window of inputs is appropriate, or what preprocessing of inputs is best, this can be viewed as a regression problem in which there are many possible input variables, some of which may actually be irrelevant to the prediction of the output variable. Because a finite data set will show random correlations between the irrelevant inputs and the output, any conventional neural network (even with regularisation or `weight decay') will not set the coefficients for these junk inputs to zero. Thus the irrelevant variables will hurt the model's performance. The Automatic Relevance Determination (ARD) model puts a prior over the regression parameters which embodies the concept of relevance. This is done in a simple The Automatic Relevance Determination (ARD) model puts a prior over the regression parameters which embodies the concept of relevance. This is done in a simple and 'soft' way by introducing multiple regularisation constants, one associated with each input. Using Bayesian methods, the regularisation constants for junk inputs are automatically inferred to be large, preventing those inputs from causing significant overfitting. An entry using the ARD model won the competition by a significant margin.},
	booktitle = {In {ASHRAE} {Transactions}, {V}.100, {Pt}.2},
	publisher = {ASHRAE},
	author = {MacKay, David J. C.},
	year = {1994},
	pages = {1053--1062}
}

@article{lecun_gradient-based_1998,
	title = {Gradient-{Based} {Learning} {Applied} to {Document} {Recognition}},
	volume = {86},
	number = {11},
	journal = {Proceedings of the IEEE},
	author = {LeCun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	month = nov,
	year = {1998},
	pages = {2278--2324}
}

@misc{xiao_fashion-mnist_2017,
	title = {Fashion-{MNIST}: a {Novel} {Image} {Dataset} for {Benchmarking} {Machine} {Learning} {Algorithms}},
	author = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
	month = aug,
	year = {2017},
}

@misc{clanuwat_deep_2018,
	title = {Deep {Learning} for {Classical} {Japanese} {Literature}},
	author = {Clanuwat, Tarin and Bober-Irizar, Mikel and Kitamoto, Asanobu and Lamb, Alex and Yamamoto, Kazuaki and Ha, David},
	month = dec,
	year = {2018},
}

@article{cohen_emnist_2017,
	title = {{EMNIST}: an extension of {MNIST} to handwritten letters},
	shorttitle = {{EMNIST}},
	abstract = {The MNIST dataset has become a standard benchmark for learning, classification and computer vision systems. Contributing to its widespread adoption are the understandable and intuitive nature of the task, its relatively small size and storage requirements and the accessibility and ease-of-use of the database itself. The MNIST database was derived from a larger dataset known as the NIST Special Database 19 which contains digits, uppercase and lowercase handwritten letters. This paper introduces a variant of the full NIST dataset, which we have called Extended MNIST (EMNIST), which follows the same conversion paradigm used to create the MNIST dataset. The result is a set of datasets that constitute a more challenging classification tasks involving letters and digits, and that shares the same image structure and parameters as the original MNIST task, allowing for direct compatibility with all existing classifiers and systems. Benchmark results are presented along with a validation of the conversion process through the comparison of the classification results on converted NIST digits and the MNIST digits.},
	urldate = {2020-03-22},
	journal = {arXiv:1702.05373 [cs]},
	author = {Cohen, Gregory and Afshar, Saeed and Tapson, Jonathan and van Schaik, André},
	month = mar,
	year = {2017},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{hron_variational_2018,
	title = {Variational {Bayesian} dropout: pitfalls and fixes},
	shorttitle = {Variational {Bayesian} dropout},
	abstract = {Dropout, a stochastic regularisation technique for training of neural networks, has recently been reinterpreted as a specific type of approximate inference algorithm for Bayesian neural networks. The main contribution of the reinterpretation is in providing a theoretical framework useful for analysing and extending the algorithm. We show that the proposed framework suffers from several issues; from undefined or pathological behaviour of the true posterior related to use of improper priors, to an ill-defined variational objective due to singularity of the approximating distribution relative to the true posterior. Our analysis of the improper log uniform prior used in variational Gaussian dropout suggests the pathologies are generally irredeemable, and that the algorithm still works only because the variational formulation annuls some of the pathologies. To address the singularity issue, we proffer Quasi-KL (QKL) divergence, a new approximate inference objective for approximation of high-dimensional distributions. We show that motivations for variational Bernoulli dropout based on discretisation and noise have QKL as a limit. Properties of QKL are studied both theoretically and on a simple practical example which shows that the QKL-optimal approximation of a full rank Gaussian with a degenerate one naturally leads to the Principal Component Analysis solution.},
	urldate = {2020-03-23},
	journal = {arXiv:1807.01969 [cs, stat]},
	author = {Hron, Jiri and Matthews, Alexander G. de G. and Ghahramani, Zoubin},
	month = jul,
	year = {2018},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@techreport{krizhevsky_learning_2009,
	title = {Learning multiple layers of features from tiny images},
	abstract = {Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it difficult to learn a good set of filters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels. The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of each of 100 non-overlapping classes. Using these labels, we show that object recognition is significantly},
	author = {Krizhevsky, Alex},
	year = {2009}
}

@incollection{louizos_bayesian_2017,
	title = {Bayesian {Compression} for {Deep} {Learning}},
	urldate = {2020-03-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 30},
	publisher = {Curran Associates, Inc.},
	author = {Louizos, Christos and Ullrich, Karen and Welling, Max},
	editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
	pages = {3288--3298}
}

@article{nitta_extension_1997,
	title = {An {Extension} of the {Back}-{Propagation} {Algorithm} to {Complex} {Numbers}},
	volume = {10},
	issn = {0893-6080},
	doi = {10.1016/S0893-6080(97)00036-1},
	abstract = {This paper presents a complex-valued version of the back-propagation algorithm (called `Complex-BP'), which can be applied to multi-layered neural networks whose weights, threshold values, input and output signals are all complex numbers. Some inherent properties of this new algorithm are studied. The results may be summarized as follows. The updating rule of the Complex-BP is such that the probability for a “standstill in learning” is reduced. The average convergence speed is superior to that of the real-valued back-propagation, whereas the generalization performance remains unchanged. In addition, the number of weights and thresholds needed is only about the half of real-valued back-propagation, where a complex-valued parameter z=x+iy (where i=−1) is counted as two because it consists of a real part x and an imaginary part y. The Complex-BP can transform geometric figures, e.g. rotation, similarity transformation and parallel displacement of straight lines, circles, etc., whereas the real-valued back-propagation cannot. Mathematical analysis indicates that a Complex-BP network which has learned a transformation, has the ability to generalize that transformation with an error which is represented by the sine. It is interesting that the above characteristics appear only by extending neural networks to complex numbers.},
	language = {en},
	number = {8},
	urldate = {2020-04-10},
	journal = {Neural Networks},
	author = {Nitta, Tohru},
	month = nov,
	year = {1997},
	keywords = {Convergence, Neural networks, Back-propagation, Complex number, Figure transformation, Learning, Pattern classification, Pattern transformation},
	pages = {1391--1415}
}

@article{cheuk_impact_2020,
	title = {The impact of {Audio} input representations on neural network based music transcription},
	abstract = {This paper thoroughly analyses the effect of different input representations on polyphonic multi-instrument music transcription. We use our own GPU based spectrogram extraction tool, nnAudio, to investigate the influence of using a linear-frequency spectrogram, log-frequency spectrogram, Mel spectrogram, and constant-Q transform (CQT). Our results show that a \$8.33\$\% increase in transcription accuracy and a \$9.39\$\% reduction in error can be obtained by choosing the appropriate input representation (log-frequency spectrogram with STFT window length 4,096 and 2,048 frequency bins in the spectrogram) without changing the neural network design (single layer fully connected). Our experiments also show that Mel spectrogram is a compact representation for which we can reduce the number of frequency bins to only 512 while still keeping a relatively high music transcription accuracy.},
	urldate = {2020-04-10},
	journal = {arXiv:2001.09989 [cs, eess]},
	author = {Cheuk, Kin Wai and Agres, Kat and Herremans, Dorien},
	month = jan,
	year = {2020},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@article{purwins_deep_2019,
	title = {Deep {Learning} for {Audio} {Signal} {Processing}},
	volume = {13},
	issn = {1941-0484},
	doi = {10.1109/JSTSP.2019.2908700},
	abstract = {Given the recent surge in developments of deep learning, this paper provides a review of the state-of-the-art deep learning techniques for audio signal processing. Speech, music, and environmental sound processing are considered side-by-side, in order to point out similarities and differences between the domains, highlighting general methods, problems, key references, and potential for cross fertilization between areas. The dominant feature representations (in particular, log-mel spectra and raw waveform) and deep learning models are reviewed, including convolutional neural networks, variants of the long short-term memory architecture, as well as more audio-specific neural network models. Subsequently, prominent deep learning application areas are covered, i.e., audio recognition (automatic speech recognition, music information retrieval, environmental sound detection, localization and tracking) and synthesis and transformation (source separation, audio enhancement, generative models for speech, sound, and music synthesis). Finally, key issues and future questions regarding deep learning applied to audio signal processing are identified.},
	number = {2},
	journal = {IEEE Journal of Selected Topics in Signal Processing},
	author = {Purwins, Hendrik and Li, Bo and Virtanen, Tuomas and Schlüter, Jan and Chang, Shuo-Yiin and Sainath, Tara},
	month = may,
	year = {2019},
	keywords = {Computational modeling, Convolution, learning (artificial intelligence), Task analysis, audio enhancement, audio recognition, audio signal processing, audio-specific neural network models, automatic speech recognition, connectionist temporal memory, convolutional neural nets, convolutional neural networks, Deep learning, environmental sound detection, environmental sound processing, environmental sounds, feature extraction, Hidden Markov models, information retrieval, music, Music, music information retrieval, prominent deep learning application areas, source separation, speech recognition, state-of-the-art deep learning techniques},
	pages = {206--219}
}

@inproceedings{gaudet_deep_2018,
	title = {Deep {Quaternion} {Networks}},
	doi = {10.1109/IJCNN.2018.8489651},
	abstract = {The field of deep learning has seen significant advancement in recent years. However, much of the existing work has been focused on real-valued numbers. Recent work has shown that a deep learning system using the complex numbers can be deeper for a fixed parameter budget compared to its real-valued counterpart. In this work, we explore the benefits of generalizing one step further into the hyper-complex numbers, quaternions specifically, and provide the architecture components needed to build deep quaternion networks. We develop the theoretical basis by reviewing quaternion convolutions, developing a novel quaternion weight initialization scheme, and developing novel algorithms for quaternion batch-normalization. These pieces are tested in a classification model by end-to-end training on the CIFAR -10 and CIFAR -100 data sets and a segmentation model by end-to-end training on the KITTI Road Segmentation data set. These quaternion networks show improved convergence compared to real-valued and complex-valued networks, especially on the segmentation task, while having fewer parameters.},
	booktitle = {2018 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Gaudet, Chase J. and Maida, Anthony S.},
	month = jul,
	year = {2018},
	keywords = {Neural networks, neural nets, Kernel, Training, learning (artificial intelligence), architecture components, CIFAR -10 data sets, CIFAR -100 data sets, classification model, complex, complex-valued networks, Computer architecture, Covariance matrices, deep learning, deep learning system, deep quaternion networks, end-to-end training, hyper-complex numbers, Image color analysis, neural networks, number theory, pattern classification, quaternion, quaternion batch-normalization, quaternion convolutions, quaternion weight initialization scheme, Quaternions, real-valued numbers},
	pages = {1--8}
}

@inproceedings{wang_comparison_2018,
	title = {A {Comparison} of {Recent} {Waveform} {Generation} and {Acoustic} {Modeling} {Methods} for {Neural}-{Network}-{Based} {Speech} {Synthesis}},
	doi = {10.1109/ICASSP.2018.8461452},
	abstract = {Recent advances in speech synthesis suggest that limitations such as the lossy nature of the amplitude spectrum with minimum phase approximation and the over-smoothing effect in acoustic modeling can be overcome by using advanced machine learning approaches. In this paper, we build a framework in which we can fairly compare new vocoding and acoustic modeling techniques with conventional approaches by means of a large scale crowdsourced evaluation. Results on acoustic models showed that generative adversarial networks and an autoregressive (AR) model performed better than a normal recurrent network and the AR model performed best. Evaluation on vocoders by using the same AR acoustic model demonstrated that a Wavenet vocoder outperformed classical source-filter-based vocoders. Particularly, generated speech waveforms from the combination of AR acoustic model and Wavenet vocoder achieved a similar score of speech quality to vocoded speech.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Wang, Xin and Lorenzo-Trueba, Jaime and Takaki, Shinji and Juvela, Lauri and Yamagishi, Junichi},
	month = apr,
	year = {2018},
	keywords = {acoustic modeling methods, acoustic modeling techniques, Acoustics, advanced machine learning approaches, amplitude spectrum, AR acoustic model, AR model, Artificial neural networks, autoregressive model, autoregressive moving average processes, autoregressive neural network, deep learning, Feature extraction, Gallium nitride, general adversarial network, generated speech waveforms, generative adversarial networks, large scale crowdsourced evaluation, learning (artificial intelligence), Linguistics, minimum phase approximation, neural nets, neural-network-based speech synthesis, normal recurrent network, over-smoothing effect, recent waveform generation, speech coding, speech quality, speech synthesis, Speech synthesis, vocoded speech, vocoders, Vocoders, vocoding, Wavenet, Wavenet vocoder},
	pages = {4804--4808}
}

@inproceedings{havasi_minimal_2018,
	title = {Minimal {Random} {Code} {Learning}: {Getting} {Bits} {Back} from {Compressed} {Model} {Parameters}},
	shorttitle = {Minimal {Random} {Code} {Learning}},
	abstract = {While deep neural networks are a highly successful model class, their large memory footprint puts considerable strain on energy consumption, communication bandwidth, and storage requirements....},
	urldate = {2020-04-18},
	author = {Havasi, Marton and Peharz, Robert and Jos\&\#xE9 and Hern\&\#xE1, Miguel and ndez-Lobato},
	month = sep,
	year = {2018}
}

@inproceedings{popa_complex-valued_2018,
	title = {Complex-{Valued} {Deep} {Boltzmann} {Machines}},
	doi = {10.1109/IJCNN.2018.8489359},
	abstract = {Deep Boltzmann Machines (DBMs) are a type of undirected deep generative models. They are part of the class of models used for performing unsupervised pretraining of deep neural networks. This paper presents the full deduction of the learning algorithm for DBMs with values in the complex domain. Experiments done using the MNIST and FashionMNIST datasets show a better performance of complex-valued DBMs compared with real-valued DBMs, both in terms of average log-probability, and in terms of classification error for the deep neural network models initialized using complex-valued DBMs.},
	booktitle = {2018 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Popa, Călin-Adrian},
	month = jul,
	year = {2018},
	keywords = {Computational modeling, Neural networks, Boltzmann machines, complex domain, complex-valued DBMs, complex-valued deep Boltzmann Machines, Data models, deep neural network models, FashionMNIST datasets, learning (artificial intelligence), learning algorithm, Machine learning, Markov processes, MNIST datasets, probability, real-valued DBMs, Task analysis, undirected deep generative models, unsupervised pretraining},
	pages = {1--8}
}

@article{wu_compressing_2019,
	title = {Compressing complex convolutional neural network based on an improved deep compression algorithm},
	abstract = {Although convolutional neural network (CNN) has made great progress, large redundant parameters restrict its deployment on embedded devices, especially mobile devices. The recent compression works are focused on real-value convolutional neural network (Real CNN), however, to our knowledge, there is no attempt for the compression of complex-value convolutional neural network (Complex CNN). Compared with the real-valued network, the complex-value neural network is easier to optimize, generalize, and has better learning potential. This paper extends the commonly used deep compression algorithm from real domain to complex domain and proposes an improved deep compression algorithm for the compression of Complex CNN. The proposed algorithm compresses the network about 8 times on CIFAR-10 dataset with less than 3\% accuracy loss. On the ImageNet dataset, our method compresses the model about 16 times and the accuracy loss is about 2\% without retraining.},
	urldate = {2020-04-18},
	journal = {arXiv:1903.02358 [cs]},
	author = {Wu, Jiasong and Ren, Hongshan and Kong, Youyong and Yang, Chunfeng and Senhadji, Lotfi and Shu, Huazhong},
	month = mar,
	year = {2019},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{vecchi_compressing_2020,
	title = {Compressing deep quaternion neural networks with targeted regularization},
	abstract = {In recent years, hyper-complex deep networks (e.g., quaternion-based) have received increasing interest with applications ranging from image reconstruction to 3D audio processing. Similarly to their real-valued counterparts, quaternion neural networks might require custom regularization strategies to avoid overfitting. In addition, for many real-world applications and embedded implementations there is the need of designing sufficiently compact networks, with as few weights and units as possible. However, the problem of how to regularize and/or sparsify quaternion-valued networks has not been properly addressed in the literature as of now. In this paper we show how to address both problems by designing targeted regularization strategies, able to minimize the number of connections and neurons of the network during training. To this end, we investigate two extensions of \${\textbackslash}ell\_1\$ and structured regularization to the quaternion domain. In our experimental evaluation, we show that these tailored strategies significantly outperform classical (real-valued) regularization strategies, resulting in small networks especially suitable for low-power and real-time applications.},
	urldate = {2020-04-18},
	journal = {arXiv:1907.11546 [cs, stat]},
	author = {Vecchi, Riccardo and Scardapane, Simone and Comminiello, Danilo and Uncini, Aurelio},
	month = jan,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@inproceedings{thickstun_invariances_2018,
	title = {Invariances and {Data} {Augmentation} for {Supervised} {Music} {Transcription}},
	doi = {10.1109/ICASSP.2018.8461686},
	abstract = {This paper explores a variety of models for frame-based music transcription, with an emphasis on the methods needed to reach state-of-the-art on human recordings. The translation-invariant network discussed in this paper, which combines a traditional filterbank with a convolutional neural network, was the top-performing model in the 2017 MIREX Multiple Fundamental Frequency Estimation evaluation. This class of models shares parameters in the log-frequency domain, which exploits the frequency invariance of music to reduce the number of model parameters and avoid overfitting to the training data. All models in this paper were trained with supervision by labeled data from the MusicNet dataset, augmented by random label-preserving pitch-shift transformations.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Thickstun, John and Harchaoui, Zaid and Foster, Dean P. and Kakade, Sham M.},
	month = apr,
	year = {2018},
	keywords = {Computational modeling, data augmentation, convolution, Convolution, Data models, learning (artificial intelligence), Task analysis, audio signal processing, convolutional neural networks, information retrieval, music, Music, music information retrieval, Computer architecture, 2017 MIREX Multiple Fundamental Frequency Estimation evaluation, convolutional neural network, feedforward neural nets, filterbank, frame-based music transcription, frequency estimation, Frequency-domain analysis, human recordings, invariances, learning, log-frequency domain, model parameters, MusicNet dataset, random label-preserving pitch-shift transformations, supervised music transcription, translation-invariant network},
	pages = {2241--2245}
}

@article{tarver_design_2019,
	title = {Design and {Implementation} of a {Neural} {Network} {Based} {Predistorter} for {Enhanced} {Mobile} {Broadband}},
	abstract = {Digital predistortion is the process of correcting for nonlinearities in the analog RF front-end of a wireless transmitter. These nonlinearities contribute to adjacent channel leakage, degrade the error vector magnitude of transmitted signals, and often force the transmitter to reduce its transmission power into a more linear but less power-efficient region of the device. Most predistortion techniques are based on polynomial models with an indirect learning architecture which have been shown to be overly sensitive to noise. In this work, we use neural network based predistortion with a novel neural network training method that avoids the indirect learning architecture and that shows significant improvements in both the adjacent channel leakage ratio and error vector magnitude. Moreover, we show that, by using a neural network based predistorter, we are able to achieve a 42\% reduction in latency and 9.6\% increase in throughput on an FPGA accelerator with 15\% fewer multiplications per sample when compared to a similarly performing memory-polynomial implementation.},
	urldate = {2020-04-18},
	journal = {arXiv:1907.00766 [eess]},
	author = {Tarver, Chance and Balatsoukas-Stimming, Alexios and Cavallaro, Joseph R.},
	month = jul,
	year = {2019},
	keywords = {Electrical Engineering and Systems Science - Signal Processing}
}

@inproceedings{sivachitra_planning_2015,
	title = {Planning and relaxed state {EEG} signal classification using complex valued neural classifier for brain computer interface},
	doi = {10.1109/CCIP.2015.7100718},
	abstract = {Most of the Brain Computer Interface (BCI) techniques use EEG signals as a main source. Any BCI system consists of three modules and they are signal recorder, signal preprocessor and classifier. Development /Selection of efficient classifiers are a challenging task in this domain. The key work addressed in this paper is the classification of EEG signals measured under planning and relaxed state using advanced machine learning classifiers. Planning relax dataset is a benchmark data and it is obtained from UCI (University of California Irvine) machine learning repository. FC-FLC is a recently developed fast learning complex valued classifier and it is used for the EEG signal classification task. Complex valued classifier (FC-FLC) performs better than all the real valued classifiers as well as few fuzzy classifiers taken for comparison from the literature. The improvement is due to the use of Gd (gudermannian) activation function in the hidden layer of the network and the tuning free algorithm.},
	booktitle = {2015 {International} {Conference} on {Cognitive} {Computing} and {Information} {Processing}({CCIP})},
	author = {Sivachitra, M. and Vijayachitra, S.},
	month = mar,
	year = {2015},
	keywords = {neural nets, learning (artificial intelligence), advanced machine learning classifiers, BCI techniques, Biological neural networks, brain computer interface, brain-computer interfaces, Brain-computer interfaces, Classification algorithms, complex valued neural classifier, Complex valued neural network, Diseases, electroencephalography, Electroencephalography, Fast learning classifier, fast learning complex valued classifier, FC-FLC, fuzzy classifiers, fuzzy set theory, Gd activation function, medical signal processing, Planning, planning (artificial intelligence), Planning and relaxed dataset and Brain Computer Interface, planning EEG signal classification, planning relax dataset, Radial basis function networks, relaxed state EEG signal classification, signal classification, signal recorder, tuning free algorithm, UCI machine learning repository, University of California Irvine machine learning repository},
	pages = {1--4}
}

@article{higgins_beta-vae_2017,
	title = {beta-{VAE}: {Learning} {Basic} {Visual} {Concepts} with a {Constrained} {Variational} {Framework}},
	shorttitle = {beta-{VAE}},
	abstract = {Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artificial intelligence that is able to learn and reason in the same way that humans do. We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. Our approach is a modification of the variational autoencoder (VAE) framework. We introduce an adjustable hyperparameter beta that balances latent channel capacity and independence constraints with reconstruction accuracy. We demonstrate that beta-VAE with appropriately tuned  beta {\textgreater} 1 qualitatively outperforms VAE (beta = 1), as well as state of the art unsupervised (InfoGAN) and semi-supervised (DC-IGN) approaches to disentangled factor learning on a variety of datasets (celebA, faces and chairs). Furthermore, we devise a protocol to quantitatively compare the degree of disentanglement learnt by different models, and show that our approach also significantly outperforms all baselines quantitatively. Unlike InfoGAN, beta-VAE is stable to train, makes few assumptions about the data and relies on tuning a single hyperparameter, which can be directly optimised through a hyper parameter search using weakly labelled data or through heuristic visual inspection for purely unsupervised data.},
	urldate = {2020-04-18},
	author = {Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
	month = apr,
	year = {2017}
}

@inproceedings{hu_initial_2016,
	title = {Initial investigation of speech synthesis based on complex-valued neural networks},
	doi = {10.1109/ICASSP.2016.7472755},
	abstract = {Although frequency analysis often leads us to a speech signal in the complex domain, the acoustic models we frequently use are designed for real-valued data. Phase is usually ignored or modelled separately from spectral amplitude. Here, we propose a complex-valued neural network (CVNN) for directly modelling the results of the frequency analysis in the complex domain (such as the complex amplitude). We also introduce a phase encoding technique to map real-valued data (e.g. cepstra or log amplitudes) into the complex domain so we can use the same CVNN processing seamlessly. In this paper, a fully complex-valued neural network, namely a neural network where all of the weight matrices, activation functions and learning algorithms are in the complex domain, is applied for speech synthesis. Results show its ability to model both complex-valued and real-valued data.},
	booktitle = {2016 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Hu, Qiong and Yamagishi, Junichi and Richmond, Korin and Subramanian, Kartick and Stylianou, Yannis},
	month = mar,
	year = {2016},
	keywords = {acoustic models, acoustic signal processing, Acoustics, activation functions, complex amplitude, complex domain, complex-valued data, complex-valued neural network, complex-valued neural networks, CVNN, frequency analysis, Hidden Markov models, learning algorithms, Linear programming, matrix algebra, neural nets, Neural networks, phase coding, phase encoding, phase modelling, real-valued data, spectral amplitude, Speech, speech signal, speech synthesis, Speech synthesis, Training, weight matrices},
	pages = {5630--5634}
}

@inproceedings{louizos_learning_2018,
	title = {Learning {Sparse} {Neural} {Networks} through {L}\_0 {Regularization}},
	abstract = {We propose a practical method for \$L\_0\$ norm regularization for neural networks: pruning the network during training by encouraging weights to become exactly zero. Such regularization is interesting since (1) it can greatly speed up training and inference, and (2) it can improve generalization. AIC and BIC, well-known model selection criteria, are special cases of \$L\_0\$ regularization. However, since the \$L\_0\$ norm of weights is non-differentiable, we cannot incorporate it directly as a regularization term in the objective function. We propose a solution through the inclusion of a collection of non-negative stochastic gates, which collectively determine which weights to set to zero. We show that, somewhat surprisingly, for certain distributions over the gates, the expected \$L\_0\$ norm of the resulting gated weights is differentiable with respect to the distribution parameters. We further propose the {\textbackslash}emph\{hard concrete\} distribution for the gates, which is obtained by "stretching" a binary concrete distribution and then transforming its samples with a hard-sigmoid. The parameters of the distribution over the gates can then be jointly optimized with the original network parameters. As a result our method allows for straightforward and efficient learning of model structures with stochastic gradient descent and allows for conditional computation in a principled way. We perform various experiments to demonstrate the effectiveness of the resulting approach and regularizer.},
	urldate = {2020-04-19},
	author = {Louizos, Christos and Welling, Max and Kingma, Diederik P.},
	month = feb,
	year = {2018}
}

@article{zhu_prune_2018,
	title = {To {Prune}, or {Not} to {Prune}: {Exploring} the {Efficacy} of {Pruning} for {Model} {Compression}},
	shorttitle = {To {Prune}, or {Not} to {Prune}},
	abstract = {Model pruning seeks to induce sparsity in a deep neural network's various connection matrices, thereby reducing the number of nonzero-valued parameters in the model. Recent reports (Han et al., 2015; Narang et al., 2017) prune deep networks at the cost of only a marginal loss in accuracy and achieve a sizable reduction in model size. This hints at the possibility that the baseline models in these experiments are perhaps severely over-parameterized at the outset and a viable alternative for model compression might be to simply reduce the number of hidden units while maintaining the model's dense connection structure, exposing a similar trade-off in model size and accuracy. We investigate these two distinct paths for model compression within the context of energy-efficient inference in resource-constrained environments and propose a new gradual pruning technique that is simple and straightforward to apply across a variety of models/datasets with minimal tuning and can be seamlessly incorporated within the training process. We compare the accuracy of large, but pruned models (large-sparse) and their smaller, but dense (small-dense) counterparts with identical memory footprint. Across a broad range of neural network architectures (deep CNNs, stacked LSTM, and seq2seq LSTM models), we find large-sparse models to consistently outperform small-dense models and achieve up to 10x reduction in number of non-zero parameters with minimal loss in accuracy.},
	urldate = {2020-04-19},
	author = {Zhu, Michael H. and Gupta, Suyog},
	month = feb,
	year = {2018}
}

@inproceedings{molchanov_variational_2017,
	title = {Variational {Dropout} {Sparsifies} {Deep} {Neural} {Networks}},
	abstract = {We explore a recently proposed Variational Dropout technique that provided an elegant Bayesian interpretation to Gaussian Dropout. We extend Variational Dropout to the case when dropout rates are unbounded, propose a way to reduce the variance of the gradient estimator and report first experimental results with individual dropout rates per weight. Interestingly, it leads to extremely sparse solutions both in fully-connected and convolutional layers. This effect is similar to automatic relevance determination effect in empirical Bayes but has a number of advantages. We reduce the number of parameters up to 280 times on LeNet architectures and up to 68 times on VGG-like networks with a negligible decrease of accuracy.},
	language = {en},
	urldate = {2020-04-19},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Molchanov, Dmitry and Ashukha, Arsenii and Vetrov, Dmitry},
	month = jul,
	year = {2017},
	keywords = {printed},
	pages = {2498--2507}
}

@article{zhang_convergence_2014,
	title = {Convergence analysis of fully complex backpropagation algorithm based on {Wirtinger} calculus},
	volume = {8},
	issn = {1871-4080},
	doi = {10.1007/s11571-013-9276-7},
	abstract = {This paper considers the fully complex backpropagation algorithm (FCBPA) for training the fully complex-valued neural networks. We prove both the weak convergence and strong convergence of FCBPA under mild conditions. The decreasing monotonicity of the error functions during the training process is also obtained. The derivation and analysis of the algorithm are under the framework of Wirtinger calculus, which greatly reduces the description complexity. The theoretical results are substantiated by a simulation example.},
	number = {3},
	urldate = {2020-05-01},
	journal = {Cognitive Neurodynamics},
	author = {Zhang, Huisheng and Liu, Xiaodong and Xu, Dongpo and Zhang, Ying},
	month = jun,
	year = {2014},
	pmid = {24808934},
	pmcid = {PMC4012068},
	pages = {261--266}
}

@article{tagare_notes_nodate,
	title = {Notes on {Optimization} on {Stiefel} {Manifolds}},
	language = {en},
	author = {Tagare, Hemant D},
	pages = {10}
}

@inproceedings{sato_complex_2013,
	title = {A complex singular value decomposition algorithm based on the {Riemannian} {Newton} method},
	doi = {10.1109/CDC.2013.6760335},
	abstract = {In this paper, the problem of finding the singular value decomposition (SVD) of a complex matrix is formulated as an optimization problem on the product of two complex Stiefel manifolds. A new algorithm for the complex SVD is proposed on the basis of the Riemannian Newton method. This algorithm can provide the singular vectors associated with an arbitrary number of the singular values from the largest one down to a smaller one. Furthermore, once a sufficiently accurate approximate complex SVD is given, the Riemannian Newton method can improve it to be as accurate as the computer accuracy permits.},
	booktitle = {52nd {IEEE} {Conference} on {Decision} and {Control}},
	author = {Sato, Hiroyuki and Iwai, Toshihiro},
	month = dec,
	year = {2013},
	keywords = {Linear programming, optimisation, Optimization, Vectors, Equations, Matrix decomposition, matrix algebra, complex matrix, complex singular value decomposition algorithm, complex Stiefel manifolds, complex SVD, Manifolds, Newton method, optimization problem, Riemannian Newton method, singular value decomposition, singular vectors},
	pages = {2972--2978}
}

@inproceedings{sato_riemannian_2014,
	title = {Riemannian conjugate gradient method for complex singular value decomposition problem},
	doi = {10.1109/CDC.2014.7040305},
	abstract = {In this paper, a Riemannian conjugate gradient method for a Riemannian optimization problem related to the singular value decomposition of a complex matrix is developed. The proposed algorithm is globally convergent, unlike Newton's method. However, Newton's method for this problem is locally quadratically convergent. With this in mind, the proposed conjugate gradient method is combined with Newton's method to produce a hybrid algorithm, which is globally and quadratically convergent in practice.},
	booktitle = {53rd {IEEE} {Conference} on {Decision} and {Control}},
	author = {Sato, Hiroyuki},
	month = dec,
	year = {2014},
	keywords = {optimisation, Vectors, Gradient methods, Equations, Matrix decomposition, complex matrix, Manifolds, Newton method, singular value decomposition, complex singular value decomposition problem, conjugate gradient methods, hybrid algorithm, Riemannian conjugate gradient method, Riemannian optimization problem, SVD},
	pages = {5849--5854}
}

@inproceedings{han_deep_2016,
	title = {Deep {Compression}: {Compressing} {Deep} {Neural} {Network} with {Pruning}, {Trained} {Quantization} and {Huffman} {Coding}},
	abstract = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce "deep compression", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9x to 13x; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy efficiency.},
	booktitle = {4th {International} {Conference} on {Learning} {Representations}, {ICLR} 2016, {San} {Juan}, {Puerto} {Rico}, {May} 2-4, 2016, {Conference} {Track} {Proceedings}},
	author = {Han, Song and Mao, Huizi and Dally, William J.},
	editor = {Bengio, Yoshua and LeCun, Yann},
	year = {2016}
}

@article{balasubramanian_deep_2016,
	title = {Deep {Model} {Compression}: {Distilling} {Knowledge} from {Noisy} {Teachers}},
	shorttitle = {Deep {Model} {Compression}},
	abstract = {The remarkable successes of deep learning models across various applications have resulted in the design of deeper networks that can solve complex problems. However, the increasing depth of such models also results in a higher storage and runtime complexity, which restricts the deployability of such very deep models on mobile and portable devices, which have limited storage and battery capacity. While many methods have been proposed for deep model compression in recent years, almost all of them have focused on reducing storage complexity. In this work, we extend the teacher-student framework for deep model compression, since it has the potential to address runtime and train time complexity too. We propose a simple methodology to include a noise-based regularizer while training the student from the teacher, which provides a healthy improvement in the performance of the student network. Our experiments on the CIFAR-10, SVHN and MNIST datasets show promising improvement, with the best performance on the CIFAR-10 dataset. We also conduct a comprehensive empirical evaluation of the proposed method under related settings on the CIFAR-10 dataset to show the promise of the proposed approach},
	urldate = {2020-06-07},
	author = {Balasubramanian, Vineeth N.},
	year = {2016}
}

@article{zhang_complex-valued_2017,
	title = {Complex-{Valued} {Convolutional} {Neural} {Network} and {Its} {Application} in {Polarimetric} {SAR} {Image} {Classification}},
	volume = {55},
	issn = {1558-0644},
	doi = {10.1109/TGRS.2017.2743222},
	abstract = {Following the great success of deep convolutional neural networks (CNNs) in computer vision, this paper proposes a complex-valued CNN (CV-CNN) specifically for synthetic aperture radar (SAR) image interpretation. It utilizes both amplitude and phase information of complex SAR imagery. All elements of CNN including input-output layer, convolution layer, activation function, and pooling layer are extended to the complex domain. Moreover, a complex backpropagation algorithm based on stochastic gradient descent is derived for CV-CNN training. The proposed CV-CNN is then tested on the typical polarimetric SAR image classification task which classifies each pixel into known terrain types via supervised training. Experiments with the benchmark data sets of Flevoland and Oberpfaffenhofen show that the classification error can be further reduced if employing CV-CNN instead of conventional real-valued CNN with the same degrees of freedom. The performance of CV-CNN is comparable to that of existing state-of-the-art methods in terms of overall classification accuracy.},
	number = {12},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Zhang, Zhimian and Wang, Haipeng and Xu, Feng and Jin, Ya-Qiu},
	month = dec,
	year = {2017},
	keywords = {gradient methods, Neural networks, complex backpropagation algorithm, neural nets, learning (artificial intelligence), Machine learning, deep learning, convolutional neural network, Feature extraction, activation function, classification error, complex SAR imagery, complex-valued CNN, Complex-valued convolutional neural network (CV-CNN), computer vision, Computer vision, Convolutional neural networks, CV-CNN training, deep convolutional neural networks, image classification, phase information, pooling layer, radar computing, radar imaging, radar polarimetry, stochastic gradient descent, supervised training, synthetic aperture radar, Synthetic aperture radar, synthetic aperture radar (SAR), synthetic aperture radar image interpretation, terrain classification, Training data, typical polarimetric SAR image classification task},
	pages = {7177--7188}
}

@inproceedings{popa_complex-valued_2017,
	title = {Complex-valued convolutional neural networks for real-valued image classification},
	doi = {10.1109/IJCNN.2017.7965936},
	abstract = {In this paper, complex-valued convolutional neural networks are presented, by giving the full deduction of the gradient descent algorithm for training this type of networks. The performances of convolutional neural networks in the real-valued domain for image classification gave rise to the idea of extending them to the complex-valued domain, also. Real-valued image classification experiments done using the MNIST and CIFAR-10 datasets have shown an improvement in performance of complex-valued convolutional neural networks over their real-valued counterparts.},
	booktitle = {2017 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Popa, Călin-Adrian},
	month = may,
	year = {2017},
	keywords = {Image recognition, Zirconium, CIFAR-10 dataset, Convolution, learning (artificial intelligence), Machine learning, feedforward neural nets, Biological neural networks, image classification, complex-valued convolutional neural network training, Feedforward neural networks, gradient descent algorithm, MNIST dataset, performance improvement, real-valued image classification, visual databases},
	pages = {816--822}
}

@article{reichert_neuronal_2014,
	title = {Neuronal {Synchrony} in {Complex}-{Valued} {Deep} {Networks}},
	abstract = {Deep learning has recently led to great successes in tasks such as image recognition (e.g Krizhevsky et al., 2012). However, deep networks are still outmatched by the power and versatility of the brain, perhaps in part due to the richer neuronal computations available to cortical circuits. The challenge is to identify which neuronal mechanisms are relevant, and to find suitable abstractions to model them. Here, we show how aspects of spike timing, long hypothesized to play a crucial role in cortical information processing, could be incorporated into deep networks to build richer, versatile representations. We introduce a neural network formulation based on complex-valued neuronal units that is not only biologically meaningful but also amenable to a variety of deep learning frameworks. Here, units are attributed both a firing rate and a phase, the latter indicating properties of spike timing. We show how this formulation qualitatively captures several aspects thought to be related to neuronal synchrony, including gating of information processing and dynamic binding of distributed object representations. Focusing on the latter, we demonstrate the potential of the approach in several simple experiments. Thus, neuronal synchrony could be a flexible mechanism that fulfills multiple functional roles in deep networks.},
	urldate = {2020-06-11},
	journal = {arXiv:1312.6115 [cs, q-bio, stat]},
	author = {Reichert, David P. and Serre, Thomas},
	month = mar,
	year = {2014},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition}
}

@article{danihelka_associative_2016,
	title = {Associative {Long} {Short}-{Term} {Memory}},
	abstract = {We investigate a new method to augment recurrent neural networks with extra memory without increasing the number of network parameters. The system has an associative memory based on complex-valued vectors and is closely related to Holographic Reduced Representations and Long Short-Term Memory networks. Holographic Reduced Representations have limited capacity: as they store more information, each retrieval becomes noisier due to interference. Our system in contrast creates redundant copies of stored information, which enables retrieval with reduced noise. Experiments demonstrate faster learning on multiple memorization tasks.},
	urldate = {2020-06-11},
	journal = {arXiv:1602.03032 [cs]},
	author = {Danihelka, Ivo and Wayne, Greg and Uria, Benigno and Kalchbrenner, Nal and Graves, Alex},
	month = may,
	year = {2016},
	keywords = {Computer Science - Neural and Evolutionary Computing}
}

@article{minemoto_feed_2017,
	series = {Hypercomplex {Signal} {Processing}},
	title = {Feed forward neural network with random quaternionic neurons},
	volume = {136},
	issn = {0165-1684},
	doi = {10.1016/j.sigpro.2016.11.008},
	abstract = {A quaternionic extension of feed forward neural network, for processing multi-dimensional signals, is proposed in this paper. This neural network is based on the three layered network with random weights, called Extreme Learning Machines (ELMs), in which iterative least-mean-square algorithms are not required for training networks. All parameters and variables in the proposed network are encoded by quaternions and operations among them follow the quaternion algebra. Neurons in the proposed network are expected to operate multi-dimensional signals as single entities, rather than real-valued neurons deal with each element of signals independently. The performances for the proposed network are evaluated through two types of experiments: classifications and reconstructions for color images in the CIFAR-10 dataset. The experimental results show that the proposed networks are superior in terms of classification accuracies for input images than the conventional (real-valued) networks with similar degrees of freedom. The detailed investigations for operations in the proposed networks are conducted.},
	language = {en},
	urldate = {2020-06-11},
	journal = {Signal Processing},
	author = {Minemoto, Toshifumi and Isokawa, Teijiro and Nishimura, Haruhiko and Matsui, Nobuyuki},
	month = jul,
	year = {2017},
	keywords = {Classification, Affine transformation, Autoencoder, Extreme learning machine, Feed forward neural network, Quaternion},
	pages = {59--68}
}

@article{bruna_mathematical_2015,
	title = {A mathematical motivation for complex-valued convolutional networks},
	abstract = {A complex-valued convolutional network (convnet) implements the repeated application of the following composition of three operations, recursively applying the composition to an input vector of nonnegative real numbers: (1) convolution with complex-valued vectors followed by (2) taking the absolute value of every entry of the resulting vectors followed by (3) local averaging. For processing real-valued random vectors, complex-valued convnets can be viewed as "data-driven multiscale windowed power spectra," "data-driven multiscale windowed absolute spectra," "data-driven multiwavelet absolute values," or (in their most general configuration) "data-driven nonlinear multiwavelet packets." Indeed, complex-valued convnets can calculate multiscale windowed spectra when the convnet filters are windowed complex-valued exponentials. Standard real-valued convnets, using rectified linear units (ReLUs), sigmoidal (for example, logistic or tanh) nonlinearities, max. pooling, etc., do not obviously exhibit the same exact correspondence with data-driven wavelets (whereas for complex-valued convnets, the correspondence is much more than just a vague analogy). Courtesy of the exact correspondence, the remarkably rich and rigorous body of mathematical analysis for wavelets applies directly to (complex-valued) convnets.},
	urldate = {2020-06-11},
	journal = {arXiv:1503.03438 [cs, stat]},
	author = {Bruna, Joan and Chintala, Soumith and LeCun, Yann and Piantino, Serkan and Szlam, Arthur and Tygert, Mark},
	month = dec,
	year = {2015},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing}
}

@article{hirose_generalization_2012,
	title = {Generalization {Characteristics} of {Complex}-{Valued} {Feedforward} {Neural} {Networks} in {Relation} to {Signal} {Coherence}},
	volume = {23},
	issn = {2162-2388},
	doi = {10.1109/TNNLS.2012.2183613},
	abstract = {Applications of complex-valued neural networks (CVNNs) have expanded widely in recent years-in particular in radar and coherent imaging systems. In general, the most important merit of neural networks lies in their generalization ability. This paper compares the generalization characteristics of complex-valued and real-valued feedforward neural networks in terms of the coherence of the signals to be dealt with. We assume a task of function approximation such as interpolation of temporal signals. Simulation and real-world experiments demonstrate that CVNNs with amplitude-phase-type activation function show smaller generalization error than real-valued networks, such as bivariate and dual-univariate real-valued neural networks. Based on the results, we discuss how the generalization characteristics are influenced by the coherence of the signals depending on the degree of freedom in the learning and on the circularity in neural dynamics.},
	number = {4},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Hirose, Akira and Yoshida, Shotaro},
	month = apr,
	year = {2012},
	keywords = {Vectors, generalization, Signal to noise ratio, signal processing, Algorithms, Neurons, neural nets, learning (artificial intelligence), learning, Biological neural networks, Feedforward neural networks, amplitude-phase-type activation function, bivariate real-valued neural networks, Coherence, coherent imaging systems, complex-valued feedforward neural networks, Complex-valued neural network, Computer Simulation, dual-univariate real-valued neural networks, Feedback, function approximation, generalisation (artificial intelligence), generalization characteristics, Image Interpretation, Computer-Assisted, Models, Statistical, neural dynamics, Neural Networks (Computer), Pattern Recognition, Automated, radar systems, real-valued feedforward neural networks, signal coherence, supervised learning, temporal signal interpolation, Tomography, Optical Coherence},
	pages = {541--551}
}

@inproceedings{zimmermann_comparison_2011,
	title = {Comparison of the {Complex} {Valued} and {Real} {Valued} {Neural} {Networks} {Trained} with {Gradient} {Descent} and {Random} {Search} {Algorithms}},
	abstract = {Complex Valued Neural Network is one of the open topics in the machine learning society. In this paper we will try to go through the problems of the complex valued neural networks gradients computations by combining the global and local optimization algorithms. The outcome of the current research is the combined global-local algorithm for training the complex valued feed forward neural network which is appropriate for the considered chaotic problem.},
	booktitle = {{ESANN}},
	author = {Zimmermann, Hans-Georg and Minin, Alexey and Kusherbaeva, Victoria},
	year = {2011},
	pages = {6}
}

@inproceedings{savitha_fast_2011,
	title = {A {Fast} {Learning} {Complex}-valued {Neural} {Classifier} for real-valued classification problems},
	doi = {10.1109/IJCNN.2011.6033508},
	abstract = {This paper presents a fast learning fully complex-valued classifier to solve real-valued classification problems, called the `Fast Learning Complex-valued Neural Classifier' (FLCNC). The FLCNC is a single hidden layer network with a non-linear, real to complex transformed input layer, a hidden layer with a fully complex activation function and a linear output layer. The neurons in the input layer convert the real-valued input features to the Complex domain using an unique non-linear transformation. At the hidden layer, the complex-valued transformed input features are mapped onto a higher dimensional Complex plane using a fully complex-valued activation function of the type of `sech'. The parameters of the input and hidden neurons of the FLCNC are chosen randomly and the output parameters are estimated analytically which makes the FLCNC to perform fast classification. Moreover, the unique nonlinear input transformation and the orthogonal decision boundaries of the complex-valued neural network help the FLCNC to perform accurate classification. Performance of the FLCNC is demonstrated using a set of multi-category and binary real valued classification problems with both balanced and unbalanced data sets from the UCI machine learning repository. Performance comparison with existing complex-valued and real-valued classifiers show the superior classification performance of the FLCNC.},
	booktitle = {The 2011 {International} {Joint} {Conference} on {Neural} {Networks}},
	author = {Savitha, R. and Suresh, S. and Sundararajan, N.},
	month = jul,
	year = {2011},
	keywords = {Neurons, neural nets, Training, learning (artificial intelligence), Machine learning, pattern classification, Biological neural networks, UCI machine learning repository, Accuracy, artificial neural networks, Benchmark testing, complex plane, fast learning complex-valued neural classifier, fast learning fully complex-valued classifier, FLCNC, fully complex activation function, nonlinear input transformation, orthogonal decision boundaries, output parameter estimation, parameter estimation, real-valued classification problems, sech, Support vector machines, unbalanced data sets},
	pages = {2243--2249}
}

@inproceedings{hui_mri_1995,
	title = {{MRI} reconstruction from truncated data using a complex domain backpropagation neural network},
	doi = {10.1109/PACRIM.1995.519582},
	abstract = {We propose a new data (extrapolation) modeling approach for reconstructing truncated magnetic resonance (MR) data. In our method, available low-frequency MR data are used to train a complex domain backpropagation neural network. This network is used to extrapolate the MR data and recover the missing high-frequency components. The performance of the proposed approach is demonstrated with a comparison to an existing real-valued neural network based method. Better results are obtained with the new approach because the complex-valued network makes use of the correlated information in the complex data instead of treating the data as separate real and imaginary parts.},
	booktitle = {{IEEE} {Pacific} {Rim} {Conference} on {Communications}, {Computers}, and {Signal} {Processing}. {Proceedings}},
	author = {Hui, Y. and Smith, M.R.},
	month = may,
	year = {1995},
	keywords = {image reconstruction, extrapolation, Extrapolation, Neural networks, Neurons, feedforward neural nets, backpropagation, Backpropagation, Biomedical imaging, biomedical NMR, complex domain backpropagation neural network, correlated information, correlation methods, Data engineering, feed-forward neural network, Frequency, high-frequency components recovery, Image reconstruction, low-frequency MR data, Magnetic resonance, Magnetic resonance imaging, medical image processing, MRI images, MRI reconstruction, multilayer perceptrons, performance, real-valued neural network based method, truncated magnetic resonance data},
	pages = {513--516}
}

@article{wang_deepcomplexmri_2020,
	title = {{DeepcomplexMRI}: {Exploiting} deep residual network for fast parallel {MR} imaging with complex convolution},
	volume = {68},
	issn = {0730-725X},
	shorttitle = {{DeepcomplexMRI}},
	doi = {10.1016/j.mri.2020.02.002},
	abstract = {This paper proposes a multi-channel image reconstruction method, named DeepcomplexMRI, to accelerate parallel MR imaging with residual complex convolutional neural network. Different from most existing works which rely on the utilization of the coil sensitivities or prior information of predefined transforms, DeepcomplexMRI takes advantage of the availability of a large number of existing multi-channel groudtruth images and uses them as target data to train the deep residual convolutional neural network offline. In particular, a complex convolutional network is proposed to take into account the correlation between the real and imaginary parts of MR images. In addition, the k-space data consistency is further enforced repeatedly in between layers of the network. The evaluations on in vivo datasets show that the proposed method has the capability to recover the desired multi-channel images. Its comparison with state-of-the-art methods also demonstrates that the proposed method can reconstruct the desired MR images more accurately.},
	language = {en},
	urldate = {2020-06-11},
	journal = {Magnetic Resonance Imaging},
	author = {Wang, Shanshan and Cheng, Huitao and Ying, Leslie and Xiao, Taohui and Ke, Ziwen and Zheng, Hairong and Liang, Dong},
	month = may,
	year = {2020},
	keywords = {Deep learning, Convolutional neural network, Fast MR imaging, Parallel imaging, Prior knowledge},
	pages = {136--147}
}

@article{scardapane_complex-valued_2018,
	title = {Complex-valued {Neural} {Networks} with {Non}-parametric {Activation} {Functions}},
	abstract = {Complex-valued neural networks (CVNNs) are a powerful modeling tool for domains where data can be naturally interpreted in terms of complex numbers. However, several analytical properties of the complex domain (e.g., holomorphicity) make the design of CVNNs a more challenging task than their real counterpart. In this paper, we consider the problem of flexible activation functions (AFs) in the complex domain, i.e., AFs endowed with sufficient degrees of freedom to adapt their shape given the training data. While this problem has received considerable attention in the real case, a very limited literature exists for CVNNs, where most activation functions are generally developed in a split fashion (i.e., by considering the real and imaginary parts of the activation separately) or with simple phase-amplitude techniques. Leveraging over the recently proposed kernel activation functions (KAFs), and related advances in the design of complex-valued kernels, we propose the first fully complex, non-parametric activation function for CVNNs, which is based on a kernel expansion with a fixed dictionary that can be implemented efficiently on vectorized hardware. Several experiments on common use cases, including prediction and channel equalization, validate our proposal when compared to real-valued neural networks and CVNNs with fixed activation functions.},
	urldate = {2020-06-11},
	journal = {arXiv:1802.08026 [cs]},
	author = {Scardapane, Simone and Van Vaerenbergh, Steven and Hussain, Amir and Uncini, Aurelio},
	month = feb,
	year = {2018},
	keywords = {Computer Science - Neural and Evolutionary Computing}
}

@article{tipping_sparse_2001,
	title = {Sparse {Bayesian} {Learning} and the {Relevance} {Vector} {Machine}},
	volume = {1},
	issn = {ISSN 1533-7928},
	number = {Jun},
	urldate = {2020-06-11},
	journal = {Journal of Machine Learning Research},
	author = {Tipping, Michael E.},
	year = {2001},
	pages = {211--244}
}

@inproceedings{mandt_variational_2016,
	title = {Variational {Tempering}},
	abstract = {Variational inference (VI) combined with data subsampling enables approximate posterior inference with large data sets for otherwise intractable models, but suffers from poor local optima. We first...},
	language = {en},
	urldate = {2020-06-12},
	booktitle = {Artificial {Intelligence} and {Statistics}},
	author = {Mandt, Stephan and McInerney, James and Abrol, Farhan and Ranganath, Rajesh and Blei, David},
	month = may,
	year = {2016},
	pages = {704--712}
}

@incollection{denil_predicting_2013,
	title = {Predicting {Parameters} in {Deep} {Learning}},
	urldate = {2020-06-12},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 26},
	publisher = {Curran Associates, Inc.},
	author = {Denil, Misha and Shakibi, Babak and Dinh, Laurent and Ranzato, Marc{\textbackslash}textquotesingle Aurelio and de Freitas, Nando},
	editor = {Burges, C. J. C. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
	year = {2013},
	pages = {2148--2156}
}

@inproceedings{nakkiran_compressing_2015,
	title = {Compressing {Deep} {Neural} {Networks} using a {Rank}-{Constrained} {Topology}},
	booktitle = {Proceedings of {Annual} {Conference} of the {International} {Speech} {Communication} {Association} ({Interspeech})},
	author = {Nakkiran, Preetum and Alvarez, Raziel and Prabhavalkar, Rohit and Parada, Carolina},
	year = {2015},
	pages = {1473--1477}
}

@incollection{denton_exploiting_2014,
	title = {Exploiting {Linear} {Structure} {Within} {Convolutional} {Networks} for {Efficient} {Evaluation}},
	urldate = {2020-06-12},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 27},
	publisher = {Curran Associates, Inc.},
	author = {Denton, Emily L and Zaremba, Wojciech and Bruna, Joan and LeCun, Yann and Fergus, Rob},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
	year = {2014},
	pages = {1269--1277}
}

@article{anwar_compact_2016,
	title = {Compact {Deep} {Convolutional} {Neural} {Networks} {With} {Coarse} {Pruning}},
	abstract = {The learning capability of a neural network improves with increasing depth at higher computational costs. Wider layers with dense kernel connectivity patterns furhter increase this cost and may hinder real-time inference. We propose feature map and kernel level pruning for reducing the computational complexity of a deep convolutional neural network. Pruning feature maps reduces the width of a layer and hence does not need any sparse representation. Further, kernel pruning converts the dense connectivity pattern into a sparse one. Due to coarse nature, these pruning granularities can be exploited by GPUs and VLSI based implementations. We propose a simple and generic strategy to choose the least adversarial pruning masks for both granularities. The pruned networks are retrained which compensates the loss in accuracy. We obtain the best pruning ratios when we prune a network with both granularities. Experiments with the CIFAR-10 dataset show that more than 85\% sparsity can be induced in the convolution layers with less than 1\% increase in the missclassification rate of the baseline network.},
	urldate = {2020-06-12},
	journal = {arXiv:1610.09639 [cs]},
	author = {Anwar, Sajid and Sung, Wonyong},
	month = oct,
	year = {2016},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing}
}

@incollection{han_learning_2015,
	title = {Learning both {Weights} and {Connections} for {Efficient} {Neural} {Network}},
	urldate = {2020-06-12},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 28},
	publisher = {Curran Associates, Inc.},
	author = {Han, Song and Pool, Jeff and Tran, John and Dally, William},
	editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
	year = {2015},
	pages = {1135--1143}
}

@article{seide_conversational_2011,
	title = {Conversational {Speech} {Transcription} {Using} {Context}-{Dependent} {Deep} {Neural} {Networks}},
	abstract = {We apply the recently proposed Context-Dependent Deep- Neural-Network HMMs, or CD-DNN-HMMs, to speech-to-text transcription. For single-pass speaker-independent recognition on the RT03S Fisher portion of phone-call transcription benchmark (Switchboard), the word-error rate is reduced from 27.4\%, obtained by discriminatively trained Gaussian-mixture HMMs, to 18.5\%?aa 33\% relative improvement. CD-DNN-HMMs combine classic artificial-neural-network HMMs with traditional tied-state triphones …},
	language = {en-US},
	urldate = {2020-06-12},
	author = {Seide, Frank and Li, Gang and Yu, Dong},
	month = aug,
	year = {2011}
}

@incollection{lecun_optimal_1990,
	title = {Optimal {Brain} {Damage}},
	abstract = {We have used information-theoretic ideas to derive a class of practical and nearly optimal schemes for adapting the size of a neural network. By removing unimportant weights from a network, several improvements can be expected: better generalization, fewer training examples required, and improved speed of learning and/or classification. The basic idea is to use second-derivative information to make a tradeoff between network complexity and training set error. Experiments confirm the usefulness of the methods on a real-world application.},
	urldate = {2020-06-12},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 2},
	publisher = {Morgan-Kaufmann},
	author = {LeCun, Yann and Denker, John S. and Solla, Sara A.},
	editor = {Touretzky, D. S.},
	year = {1990},
	pages = {598--605}
}

@inproceedings{vanhoucke_improving_2011,
	title = {Improving the speed of neural networks on {CPUs}},
	booktitle = {Deep {Learning} and {Unsupervised} {Feature} {Learning} {Workshop}, {NIPS} 2011},
	author = {Vanhoucke, Vincent and Senior, Andrew and Mao, Mark Z.},
	year = {2011}
}

@inproceedings{chen_compressing_2015,
	title = {Compressing {Neural} {Networks} with the {Hashing} {Trick}},
	abstract = {As deep nets are increasingly used in applications suited for mobile devices, a fundamental dilemma becomes apparent: the trend in deep learning is to grow models to absorb ever-increasing data set...},
	language = {en},
	urldate = {2020-06-12},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Chen, Wenlin and Wilson, James and Tyree, Stephen and Weinberger, Kilian and Chen, Yixin},
	month = jun,
	year = {2015},
	pages = {2285--2294}
}

@article{gong_compressing_2014,
	title = {Compressing {Deep} {Convolutional} {Networks} using {Vector} {Quantization}},
	abstract = {Deep convolutional neural networks (CNN) has become the most promising method for object recognition, repeatedly demonstrating record breaking results for image classification and object detection in recent years. However, a very deep CNN generally involves many layers with millions of parameters, making the storage of the network model to be extremely large. This prohibits the usage of deep CNNs on resource limited hardware, especially cell phones or other embedded devices. In this paper, we tackle this model storage issue by investigating information theoretical vector quantization methods for compressing the parameters of CNNs. In particular, we have found in terms of compressing the most storage demanding dense connected layers, vector quantization methods have a clear gain over existing matrix factorization methods. Simply applying k-means clustering to the weights or conducting product quantization can lead to a very good balance between model size and recognition accuracy. For the 1000-category classification task in the ImageNet challenge, we are able to achieve 16-24 times compression of the network with only 1\% loss of classification accuracy using the state-of-the-art CNN.},
	urldate = {2020-06-12},
	journal = {arXiv:1412.6115 [cs]},
	author = {Gong, Yunchao and Liu, Liu and Yang, Ming and Bourdev, Lubomir},
	month = dec,
	year = {2014},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing}
}

@article{courbariaux_training_2015,
	title = {Training deep neural networks with low precision multiplications},
	abstract = {Multipliers are the most space and power-hungry arithmetic operators of the digital implementation of deep neural networks. We train a set of state-of-the-art neural networks (Maxout networks) on three benchmark datasets: MNIST, CIFAR-10 and SVHN. They are trained with three distinct formats: floating point, fixed point and dynamic fixed point. For each of those datasets and for each of those formats, we assess the impact of the precision of the multiplications on the final error after training. We find that very low precision is sufficient not just for running trained networks but also for training them. For example, it is possible to train Maxout networks with 10 bits multiplications.},
	urldate = {2020-06-12},
	journal = {arXiv:1412.7024 [cs]},
	author = {Courbariaux, Matthieu and Bengio, Yoshua and David, Jean-Pierre},
	month = sep,
	year = {2015},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing}
}

@incollection{courbariaux_binaryconnect_2015,
	title = {{BinaryConnect}: {Training} {Deep} {Neural} {Networks} with binary weights during propagations},
	shorttitle = {{BinaryConnect}},
	urldate = {2020-06-12},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 28},
	publisher = {Curran Associates, Inc.},
	author = {Courbariaux, Matthieu and Bengio, Yoshua and David, Jean-Pierre},
	editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
	year = {2015},
	pages = {3123--3131}
}

@incollection{kingma_variational_2015,
	title = {Variational {Dropout} and the {Local} {Reparameterization} {Trick}},
	urldate = {2020-06-12},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 28},
	publisher = {Curran Associates, Inc.},
	author = {Kingma, Durk P and Salimans, Tim and Welling, Max},
	editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
	year = {2015},
	keywords = {printed},
	pages = {2575--2583}
}

@incollection{sonderby_ladder_2016,
	title = {Ladder {Variational} {Autoencoders}},
	urldate = {2020-06-12},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 29},
	publisher = {Curran Associates, Inc.},
	author = {Sønderby, Casper Kaae and Raiko, Tapani and Maaløe, Lars and Sønderby, Søren Kaae and Winther, Ole},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	year = {2016},
	pages = {3738--3746}
}

@article{campese_fourth_2015,
	title = {Fourth {Moment} {Theorems} for complex {Gaussian} approximation},
	abstract = {We prove a bound for the Wasserstein distance between vectors of smooth complex random variables and complex Gaussians in the framework of complex Markov diffusion generators. For the special case of chaotic eigenfunctions, this bound can be expressed in terms of certain fourth moments of the vector, yielding a quantitative Fourth Moment Theorem for complex Gaussian approximation on complex Markov diffusion chaos. This extends results of Azmoodeh, Campese, Poly (2014) and Campese, Nourdin, Peccati (2015) for the real case. Our main ingredients are a complex version of the so called \${\textbackslash}Gamma\$-calculus and Stein's method for the multivariate complex Gaussian distribution.},
	urldate = {2020-06-13},
	journal = {arXiv:1511.00547 [math]},
	author = {Campese, Simon},
	month = nov,
	year = {2015},
	keywords = {Mathematics - Probability}
}

@article{gallager_circularly-symmetric_nodate,
	title = {Circularly-{Symmetric} {Gaussian} random vectors},
	abstract = {A number of basic properties about circularly-symmetric Gaussian random vectors are stated and proved here. These properties are each probably well known to most researchers who work with Gaussian noise, but I have not found them stated together with simple proofs in the literature. They are usually viewed as too advanced or too detailed for elementary texts but are used (correctly or incorrectly) without discussion in more advanced work. These results should have appeared in Section 7.8.1 of my book, R. G. Gallager, “Principles of Digital Communication,” Cambridge Press, 2008 (PDC08), but I came to understand them only while preparing a solution manual when the book was in the ﬁnal production stage.},
	language = {en},
	author = {Gallager, Robert G},
	pages = {9}
}

@article{hankin_complex_2015,
	title = {The {Complex} {Multivariate} {Gaussian} {Distribution}},
	volume = {7},
	issn = {2073-4859},
	doi = {10.32614/RJ-2015-006},
	abstract = {Here I introduce cmvnorm, a complex generalization of the mvtnorm package. A complex generalization of the Gaussian process is suggested and numerical results presented using the package. An application in the context of approximating the Weierstrass sigma function using a complex Gaussian process is given. An earlier version of this vignette appeared as Hankin (2015), and the package is maintained at http: //github.com/RobinHankin/cmvnorm.},
	language = {en},
	number = {1},
	urldate = {2020-06-13},
	journal = {The R Journal},
	author = {Hankin, K., S., Robin},
	year = {2015},
	pages = {73}
}

@article{sanh_movement_2020,
	title = {Movement {Pruning}: {Adaptive} {Sparsity} by {Fine}-{Tuning}},
	shorttitle = {Movement {Pruning}},
	abstract = {Magnitude pruning is a widely used strategy for reducing model size in pure supervised learning; however, it is less effective in the transfer learning regime that has become standard for state-of-the-art natural language processing applications. We propose the use of movement pruning, a simple, deterministic first-order weight pruning method that is more adaptive to pretrained model fine-tuning. We give mathematical foundations to the method and compare it to existing zeroth- and first-order pruning methods. Experiments show that when pruning large pretrained language models, movement pruning shows significant improvements in high-sparsity regimes. When combined with distillation, the approach achieves minimal accuracy loss with down to only 3\% of the model parameters.},
	urldate = {2020-06-14},
	journal = {arXiv:2005.07683 [cs]},
	author = {Sanh, Victor and Wolf, Thomas and Rush, Alexander M.},
	month = may,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language}
}

@inproceedings{hassibi_optimal_1993,
	title = {Optimal {Brain} {Surgeon} and general network pruning},
	doi = {10.1109/ICNN.1993.298572},
	abstract = {The use of information from all second-order derivatives of the error function to perform network pruning (i.e., removing unimportant weights from a trained network) in order to improve generalization, simplify networks, reduce hardware or storage requirements, increase the speed of further training, and, in some cases, enable rule extraction, is investigated. The method, Optimal Brain Surgeon (OBS), is significantly better than magnitude-based methods and Optimal Brain Damage, which often remove the wrong weights. OBS, permits pruning of more weights than other methods (for the same error on the training set), and thus yields better generalization on test data. Crucial to OBS is a recursion relation for calculating the inverse Hessian matrix H/sup -1/ from training data and structural information of the set. OBS deletes the correct weights from a trained XOR network in every case.{\textless}{\textgreater}},
	booktitle = {{IEEE} {International} {Conference} on {Neural} {Networks}},
	author = {Hassibi, B. and Stork, D.G. and Wolff, G.J.},
	month = mar,
	year = {1993},
	keywords = {generalization, neural nets, learning (artificial intelligence), Machine learning, Biological neural networks, Training data, generalisation (artificial intelligence), Benchmark testing, Backpropagation, Data mining, error function, general network pruning, Hardware, inverse Hessian matrix, Optimal Brain Surgeon, Pattern recognition, recursion relation, rule extraction, second-order derivatives, Statistics, storage requirements, structural information, Surges, trained XOR network},
	pages = {293--299 vol.1}
}

@incollection{wen_learning_2016,
	title = {Learning {Structured} {Sparsity} in {Deep} {Neural} {Networks}},
	urldate = {2020-06-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 29},
	publisher = {Curran Associates, Inc.},
	author = {Wen, Wei and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	year = {2016},
	pages = {2074--2082}
}

@inproceedings{romero-jerez_distribution_2015,
	title = {On the distribution of the squared norm of non-circular complex {Gaussian} random variables with applications},
	doi = {10.1109/ISIT.2015.7282473},
	abstract = {We present a novel relationship between the distribution of circular and non-circular complex Gaussian random variables. Specifically, we show that the distribution of the squared norm of a non-circular complex Gaussian random variable, usually referred to as squared Hoyt distribution, can be constructed from a conditional exponential distribution. Leveraging this approach, we provide novel results on the Shannon capacity of adaptive transmission techniques in Hoyt fading channels, showing that the asymptotic capacity loss per bandwidth unit in the high-SNR regime is up to 1.83 bps/Hz, compared to the AWGN case.},
	booktitle = {2015 {IEEE} {International} {Symposium} on {Information} {Theory} ({ISIT})},
	author = {Romero-Jerez, Juan M. and Lopez-Martinez, F. Javier},
	month = jun,
	year = {2015},
	keywords = {Signal to noise ratio, asymptotic capacity, Rayleigh channels, Random variables, adaptive transmission techniques, AWGN channels, Bandwidth, Communication systems, conditional exponential distribution, fading channels, Gaussian processes, Hoyt fading channels, Measurement, noncircular complex Gaussian random variables, Shannon capacity, squared Hoyt distribution, squared norm distribution},
	pages = {341--345}
}

@article{loesch_cramer-rao_2013,
	title = {Cramér-{Rao} {Bound} for {Circular} and {Noncircular} {Complex} {Independent} {Component} {Analysis}},
	volume = {61},
	issn = {1941-0476},
	doi = {10.1109/TSP.2012.2226166},
	abstract = {Despite an increased interest in complex independent component analysis (ICA) during the last two decades, a closed form expression for the Cramér-Rao bound (CRB) for the demixing matrix is not known yet. In this paper, we fill this gap by deriving a closed-form expression for the CRB of the demixing matrix for instantaneous noncircular complex ICA. It contains the CRB for circular complex ICA and noncircular complex Gaussian ICA as two special cases. We also study the CRB numerically for the family of noncircular complex generalized Gaussian distributions and compare it to simulation results of two ICA estimators. Furthermore, we show how to extend the CRB to the case where the source signals are not temporally independent and identically distributed.},
	number = {2},
	journal = {IEEE Transactions on Signal Processing},
	author = {Loesch, Benedikt and Yang, Bin},
	month = jan,
	year = {2013},
	keywords = {Vectors, Algorithm design and analysis, independent component analysis, Indexes, matrix algebra, circular complex ICA, Closed-form solutions, Covariance matrix, Cramér-Rao bound, demixing matrix, Gaussian distribution, Gaussian distributions, Gaussian ICA, generalized Gaussian distribution, Independent component analysis, noncircular complex, noncircular complex ICA, Probability density function},
	pages = {365--379}
}

@article{adali_complex-valued_2011,
	title = {Complex-{Valued} {Signal} {Processing}: {The} {Proper} {Way} to {Deal} {With} {Impropriety}},
	volume = {59},
	issn = {1941-0476},
	shorttitle = {Complex-{Valued} {Signal} {Processing}},
	doi = {10.1109/TSP.2011.2162954},
	abstract = {Complex-valued signals occur in many areas of science and engineering and are thus of fundamental interest. In the past, it has often been assumed, usually implicitly, that complex random signals are proper or circular. A proper complex random variable is uncorrelated with its complex conjugate, and a circular complex random variable has a probability distribution that is invariant under rotation in the complex plane. While these assumptions are convenient because they simplify computations, there are many cases where proper and circular random signals are very poor models of the underlying physics. When taking impropriety and noncircularity into account, the right type of processing can provide significant performance gains. There are two key ingredients in the statistical signal processing of complex-valued data: 1) utilizing the complete statistical characterization of complex-valued random signals; and 2) the optimization of real-valued cost functions with respect to complex parameters. In this overview article, we review the necessary tools, among which are widely linear transformations, augmented statistical descriptions, and Wirtinger calculus. We also present some selected recent developments in the field of complex-valued signal processing, addressing the topics of model selection, filtering, and source separation.},
	number = {11},
	journal = {IEEE Transactions on Signal Processing},
	author = {Adali, Tülay and Schreier, Peter J. and Scharf, Louis L.},
	month = nov,
	year = {2011},
	keywords = {optimisation, Vectors, Cost function, signal processing, Signal processing, augmented statistical descriptions, Calculus, complex random signals, complex-valued signal processing, CR calculus, estimation, improper, independent component analysis, linear transformations, Materials, model selection, noncircular, optimization, probability distribution, Random variables, statistical distributions, statistical signal processing, widely linear, Wirtinger calculus, Zirconium},
	pages = {5101--5125}
}

@article{tanaka_pruning_2020,
	title = {Pruning neural networks without any data by iteratively conserving synaptic flow},
	abstract = {Pruning the parameters of deep neural networks has generated intense interest due to potential savings in time, memory and energy both during training and at test time. Recent works have identified, through an expensive sequence of training and pruning cycles, the existence of winning lottery tickets or sparse trainable subnetworks at initialization. This raises a foundational question: can we identify highly sparse trainable subnetworks at initialization, without ever training, or indeed without ever looking at the data? We provide an affirmative answer to this question through theory driven algorithm design. We first mathematically formulate and experimentally verify a conservation law that explains why existing gradient-based pruning algorithms at initialization suffer from layer-collapse, the premature pruning of an entire layer rendering a network untrainable. This theory also elucidates how layer-collapse can be entirely avoided, motivating a novel pruning algorithm Iterative Synaptic Flow Pruning (SynFlow). This algorithm can be interpreted as preserving the total flow of synaptic strengths through the network at initialization subject to a sparsity constraint. Notably, this algorithm makes no reference to the training data and consistently outperforms existing state-of-the-art pruning algorithms at initialization over a range of models (VGG and ResNet), datasets (CIFAR-10/100 and Tiny ImageNet), and sparsity constraints (up to 99.9 percent). Thus our data-agnostic pruning algorithm challenges the existing paradigm that data must be used to quantify which synapses are important.},
	urldate = {2020-06-14},
	journal = {arXiv:2006.05467 [cond-mat, q-bio, stat]},
	author = {Tanaka, Hidenori and Kunin, Daniel and Yamins, Daniel L. K. and Ganguli, Surya},
	month = jun,
	year = {2020},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning}
}

@article{knill_short_nodate,
	title = {A short introduction to several complex variables},
	language = {en},
	author = {Knill, Oliver},
	pages = {11}
}

@inproceedings{virtue_better_2017,
	title = {Better than real: {Complex}-valued neural nets for {MRI} fingerprinting},
	shorttitle = {Better than real},
	doi = {10.1109/ICIP.2017.8297024},
	abstract = {The task of MRI fingerprinting is to identify tissue parameters from complex-valued MRI signals. The prevalent approach is dictionary based, where a test MRI signal is compared to stored MRI signals with known tissue parameters and the most similar signals and tissue parameters retrieved. Such an approach does not scale with the number of parameters and is rather slow when the tissue parameter space is large. Our first novel contribution is to use deep learning as an efficient nonlinear inverse mapping approach. We generate synthetic (tissue, MRI) data from an MRI simulator, and use them to train a deep net to map the MRI signal to the tissue parameters directly. Our second novel contribution is to develop a complex-valued neural network with new cardioid activation functions. Our results demonstrate that complex-valued neural nets could be much more accurate than real-valued neural nets at complex-valued MRI fingerprinting.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
	author = {Virtue, Patrick and Yu, Stella X. and Lustig, Michael},
	month = sep,
	year = {2017},
	keywords = {biological tissues, biomedical MRI, Calculus, complex-valued MRI fingerprinting, complex-valued MRI signals, complex-valued neural nets, complex-valued neural network, Complex-valued Neural Networks, Dictionaries, fingerprint identification, Fingerprint recognition, learning (artificial intelligence), Magnetic resonance, Magnetic Resonance Fingerprinting, Magnetic resonance imaging, medical image processing, neural nets, Neural networks, nonlinear inverse mapping approach, Parameter Mapping, Protons, real-valued neural nets, tissue parameter identification},
	pages = {3953--3957}
}

@article{chen_ancfis_2011,
	title = {{ANCFIS}: {A} {Neurofuzzy} {Architecture} {Employing} {Complex} {Fuzzy} {Sets}},
	volume = {19},
	issn = {1941-0034},
	shorttitle = {{ANCFIS}},
	doi = {10.1109/TFUZZ.2010.2096469},
	abstract = {Complex fuzzy sets (CFSs) are an extension of type-1 fuzzy sets in which the membership of an object to the set is a value from the unit disc of the complex plane. Although there has been considerable progress made in determining the properties of CFSs and complex fuzzy logic, there has yet to be any practical application of this concept. We present the adaptive neurocomplex-fuzzy-inferential system (ANCFIS), which is the first neurofuzzy system architecture to implement complex fuzzy rules (and, in particular, the signature property of rule interference). We have applied this neurofuzzy system to the domain of time-series forecasting, which is an important machine-learning problem. We find that ANCFIS performs well in one synthetic and five real-world forecasting problems and is also very parsimonious. Experimental comparisons show that ANCFIS is comparable with existing approaches on our five datasets. This work demonstrates the utility of complex fuzzy logic on real-world problems.},
	number = {2},
	journal = {IEEE Transactions on Fuzzy Systems},
	author = {Chen, Zhifei and Aghakhani, Sara and Man, James and Dick, Scott},
	month = apr,
	year = {2011},
	keywords = {adaptive neurocomplex fuzzy inferential system, ANCFIS, Artificial neural networks, complex fuzzy logic, complex fuzzy rules, complex fuzzy sets, Complex fuzzy sets (CFSs), Convolution, Forecasting, Fuzzy logic, fuzzy neural nets, fuzzy set theory, Fuzzy sets, learning (artificial intelligence), machine learning, machine-learning problem, neurofuzzy architecture, neurofuzzy systems, rule interference, Simulated annealing, Time series analysis, time-series forecasting, type-1 fuzzy sets},
	pages = {305--322}
}

@article{hansch_complex-valued_2010,
	title = {Complex-{Valued} {Multi}-{Layer} {Perceptrons} – {An} {Application} to {Polarimetric} {SAR} {Data}},
	volume = {76},
	doi = {10.14358/PERS.76.9.1081},
	abstract = {Multi-Layer Perceptrons (MLPs) are powerful function approximators. In the last decades they were successfully applied to many different regression and classification problems. Their characteristics and convergence properties are well studied and relatively well understood, but they
were originally designed to work with real-valued data. The main focus of this paper is the classification of polarimetric synthetic aperture radar (POLSAR) data which are a complexvalued signal. Instead of using an arbitrarily projection of this complex-valued data to the real domain, the
paper proposes the usage of complex-valued MLPs (CV-MLPs), which are an extension of MLPs to the complex domain. The paper provides a detailed yet general derivation of the complex backpropagation algorithm and mentions related problems as well as possible solutions. Furthermore, it evaluates
the performance of CV-MLPs in a land-cover classification task in POLSAR images under several learning conditions, and compares the proposed classifier with standard methods. The experimental results show that CV-MLPs are successfully applicable to classification tasks in POLSAR data. They
show good convergence properties and a better performance if compared to real-valued MLPs.},
	number = {9},
	journal = {Photogrammetric Engineering \& Remote Sensing},
	author = {Hänsch, Ronny},
	month = sep,
	year = {2010},
	pages = {1081--1088}
}

@inproceedings{haensch_complex-valued_2010,
	title = {Complex-{Valued} {Convolutional} {Neural} {Networks} for {Object} {Detection} in {PolSAR} data},
	abstract = {Detection methods for generic object categories, which are more sophisticated than pixel-wise classification, have been rarely introduced to polarimetric synthetic aperture radar (PolSAR) images. Despite the great success in other computer vision applications, the transfer to PolSAR data has been delayed due to the different statistical properties. This paper provides a first investigation of Complex-Valued Convolutional Neural Networks (CC-NN) for object recognition in PolSAR data. Although an architecture with only one single convolutional layer was used, the results are already superior to those obtained by a standard complex-valued neural network.},
	booktitle = {8th {European} {Conference} on {Synthetic} {Aperture} {Radar}},
	author = {Haensch, Ronny and Hellwich, Olaf},
	month = jun,
	year = {2010},
	keywords = {Artificial neural networks, Computer architecture, Convolution, Kernel, Neurons, Pixel, Speckle},
	pages = {1--4}
}

@article{guberman_complex_2016,
	title = {On {Complex} {Valued} {Convolutional} {Neural} {Networks}},
	abstract = {Convolutional neural networks (CNNs) are the cutting edge model for supervised machine learning in computer vision. In recent years CNNs have outperformed traditional approaches in many computer vision tasks such as object detection, image classification and face recognition. CNNs are vulnerable to overfitting, and a lot of research focuses on finding regularization methods to overcome it. One approach is designing task specific models based on prior knowledge. Several works have shown that properties of natural images can be easily captured using complex numbers. Motivated by these works, we present a variation of the CNN model with complex valued input and weights. We construct the complex model as a generalization of the real model. Lack of order over the complex field raises several difficulties both in the definition and in the training of the network. We address these issues and suggest possible solutions. The resulting model is shown to be a restricted form of a real valued CNN with twice the parameters. It is sensitive to phase structure, and we suggest it serves as a regularized model for problems where such structure is important. This suggestion is verified empirically by comparing the performance of a complex and a real network in the problem of cell detection. The two networks achieve comparable results, and although the complex model is hard to train, it is significantly less vulnerable to overfitting. We also demonstrate that the complex network detects meaningful phase structure in the data.},
	urldate = {2020-06-17},
	journal = {arXiv:1602.09046 [cs]},
	author = {Guberman, Nitzan},
	month = feb,
	year = {2016},
	keywords = {Computer Science - Neural and Evolutionary Computing}
}

@article{leung_complex_1991,
	title = {The complex backpropagation algorithm},
	volume = {39},
	issn = {1941-0476},
	doi = {10.1109/78.134446},
	abstract = {The backpropagation (BP) algorithm that provides a popular method for the design of a multilayer neural network to include complex coefficients and complex signals so that it can be applied to general radar signal processing and communications problems. It is shown that the network can classify complex signals. The generalization of the BP to deal with complex signals should make it possible to expand the line of applications of this powerful nonlinear signal processing algorithm.{\textless}{\textgreater}},
	number = {9},
	journal = {IEEE Transactions on Signal Processing},
	author = {Leung, H. and Haykin, S.},
	month = sep,
	year = {1991},
	keywords = {Adaptive signal processing, Array signal processing, Backpropagation algorithms, Circuits, complex backpropagation algorithm, complex coefficients, complex signals, design, Least squares approximation, Matrix decomposition, Multi-layer neural network, multilayer neural network, neural nets, Noise cancellation, nonlinear signal processing algorithm, radar signal processing, radar theory, signal processing, Signal processing algorithms, Speech processing},
	pages = {2101--2104}
}

@inproceedings{uhlich_mixed_2020,
	title = {Mixed {Precision} {DNNs}: {All} you need is a good parametrization},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Uhlich, Stefan and Mauch, Lukas and Cardinaux, Fabien and Yoshiyama, Kazuki and Garcia, Javier Alonso and Tiedemann, Stephen and Kemp, Thomas and Nakamura, Akira},
	year = {2020},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{monning_evaluation_2018,
	title = {Evaluation of {Complex}-{Valued} {Neural} {Networks} on {Real}-{Valued} {Classification} {Tasks}},
	abstract = {Complex-valued neural networks are not a new concept, however, the use of real-valued models has often been favoured over complex-valued models due to difficulties in training and performance. When comparing real-valued versus complex-valued neural networks, existing literature often ignores the number of parameters, resulting in comparisons of neural networks with vastly different sizes. We find that when real and complex neural networks of similar capacity are compared, complex models perform equal to or slightly worse than real-valued models for a range of real-valued classification tasks. The use of complex numbers allows neural networks to handle noise on the complex plane. When classifying real-valued data with a complex-valued neural network, the imaginary parts of the weights follow their real parts. This behaviour is indicative for a task that does not require a complex-valued model. We further investigated this in a synthetic classification task. We can transfer many activation functions from the real to the complex domain using different strategies. The weight initialisation of complex neural networks, however, remains a significant problem.},
	urldate = {2020-06-17},
	journal = {arXiv:1811.12351 [cs, stat]},
	author = {Mönning, Nils and Manandhar, Suresh},
	month = nov,
	year = {2018},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@inproceedings{lin_fixed_2016,
	title = {Fixed {Point} {Quantization} of {Deep} {Convolutional} {Networks}},
	abstract = {In recent years increasingly complex architectures for deep convolution networks (DCNs) have been proposed to boost the performance on image recognition tasks. However, the gains in performance hav...},
	language = {en},
	urldate = {2020-06-17},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Lin, Darryl and Talathi, Sachin and Annapureddy, Sreekanth},
	month = jun,
	year = {2016},
	pages = {2849--2858}
}

@article{hoffman_stochastic_2013,
	title = {Stochastic {Variational} {Inference}},
	volume = {14},
	issn = {1533-7928},
	number = {4},
	urldate = {2020-06-18},
	journal = {Journal of Machine Learning Research},
	author = {Hoffman, Matthew D. and Blei, David M. and Wang, Chong and Paisley, John},
	year = {2013},
	pages = {1303--1347}
}

@article{jordan_introduction_1999,
	title = {An {Introduction} to {Variational} {Methods} for {Graphical} {Models}},
	volume = {37},
	issn = {1573-0565},
	doi = {10.1023/A:1007665907178},
	abstract = {This paper presents a tutorial introduction to the use of variational methods for inference and learning in graphical models (Bayesian networks and Markov random fields). We present a number of examples of graphical models, including the QMR-DT database, the sigmoid belief network, the Boltzmann machine, and several variants of hidden Markov models, in which it is infeasible to run exact inference algorithms. We then introduce variational methods, which exploit laws of large numbers to transform the original graphical model into a simplified graphical model in which inference is efficient. Inference in the simpified model provides bounds on probabilities of interest in the original model. We describe a general framework for generating variational transformations based on convex duality. Finally we return to the examples and demonstrate how variational algorithms can be formulated in each case.},
	language = {en},
	number = {2},
	urldate = {2020-06-18},
	journal = {Machine Learning},
	author = {Jordan, Michael I. and Ghahramani, Zoubin and Jaakkola, Tommi S. and Saul, Lawrence K.},
	month = nov,
	year = {1999},
	pages = {183--233}
}

@incollection{titsias_local_2015,
	title = {Local {Expectation} {Gradients} for {Black} {Box} {Variational} {Inference}},
	urldate = {2020-05-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 28},
	publisher = {Curran Associates, Inc.},
	author = {Titsias, Michalis and Lázaro-Gredilla, Miguel},
	editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
	year = {2015},
	pages = {2638--2646}
}

@inproceedings{gal_dropout_2016,
	title = {Dropout as a {Bayesian} {Approximation}: {Representing} {Model} {Uncertainty} in {Deep} {Learning}},
	shorttitle = {Dropout as a {Bayesian} {Approximation}},
	abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian mode...},
	language = {en},
	urldate = {2020-06-20},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Gal, Yarin and Ghahramani, Zoubin},
	month = jun,
	year = {2016},
	pages = {1050--1059}
}

@inproceedings{paszke_pytorch_2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	shorttitle = {{PyTorch}},
	abstract = {Electronic Proceedings of Neural Information Processing Systems},
	urldate = {2020-06-21},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	year = {2019},
	pages = {8026--8037}
}

@article{draguns_residual_2020,
	title = {Residual {Shuffle}-{Exchange} {Networks} for {Fast} {Processing} of {Long} {Sequences}},
	abstract = {Attention is a commonly used mechanism in sequence processing, but it is of O(n{\textasciicircum}2) complexity which prevents its application to long sequences. The recently introduced Neural Shuffle-Exchange network offers a computation-efficient alternative, enabling the modelling of long-range dependencies in O(n log n) time. The model, however, is quite complex, involving a sophisticated gating mechanism derived from Gated Recurrent Unit. In this paper, we present a simple and lightweight variant of the Shuffle-Exchange network, which is based on a residual network employing GELU and Layer Normalization. The proposed architecture not only scales to longer sequences but also converges faster and provides better accuracy. It surpasses Shuffle-Exchange network on the LAMBADA language modelling task and achieves state-of-the-art performance on the MusicNet dataset for music transcription while using significantly fewer parameters. We show how to combine Shuffle-Exchange network with convolutional layers establishing it as a useful building block in long sequence processing applications.},
	urldate = {2020-06-22},
	journal = {arXiv:2004.04662 [cs, eess]},
	author = {Draguns, Andis and Ozoliņš, Emīls and Šostaks, Agris and Apinis, Matīss and Freivalds, Karlis},
	month = apr,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing}
}
