
@article{monning_evaluation_2018,
	title = {Evaluation of {Complex}-{Valued} {Neural} {Networks} on {Real}-{Valued} {Classification} {Tasks}},
	url = {http://arxiv.org/abs/1811.12351},
	abstract = {Complex-valued neural networks are not a new concept, however, the use of real-valued models has often been favoured over complex-valued models due to difficulties in training and performance. When comparing real-valued versus complex-valued neural networks, existing literature often ignores the number of parameters, resulting in comparisons of neural networks with vastly different sizes. We find that when real and complex neural networks of similar capacity are compared, complex models perform equal to or slightly worse than real-valued models for a range of real-valued classification tasks. The use of complex numbers allows neural networks to handle noise on the complex plane. When classifying real-valued data with a complex-valued neural network, the imaginary parts of the weights follow their real parts. This behaviour is indicative for a task that does not require a complex-valued model. We further investigated this in a synthetic classification task. We can transfer many activation functions from the real to the complex domain using different strategies. The weight initialisation of complex neural networks, however, remains a significant problem.},
	urldate = {2019-10-03},
	journal = {arXiv:1811.12351 [cs, stat]},
	author = {Mönning, Nils and Manandhar, Suresh},
	month = nov,
	year = {2018},
	note = {arXiv: 1811.12351},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	file = {arXiv\:1811.12351 PDF:/Users/ivannazarov/Zotero/storage/T6Q35ZRS/Mönning and Manandhar - 2018 - Evaluation of Complex-Valued Neural Networks on Re.pdf:application/pdf;arXiv.org Snapshot:/Users/ivannazarov/Zotero/storage/RR3QY9IA/1811.html:text/html}
}

@article{schoukens_obtaining_2017,
	title = {Obtaining the {Pre}-{Inverse} of a {Power} {Amplifier} using {Iterative} {Learning} {Control}},
	volume = {65},
	issn = {0018-9480, 1557-9670},
	url = {http://arxiv.org/abs/1606.08663},
	doi = {10.1109/TMTT.2017.2694822},
	abstract = {Telecommunication networks make extensive use of power amplifiers to broaden the coverage from transmitter to receiver. Achieving high power efficiency is challenging and comes at a price: the wanted linear performance is degraded due to nonlinear effects. To compensate for these nonlinear disturbances, existing techniques compute the pre-inverse of the power amplifier by estimation of a nonlinear model. However, the extraction of this nonlinear model is involved and requires advanced system identification techniques. We used the plant inversion iterative learning control algorithm to investigate whether the nonlinear modeling step can be simplified. This paper introduces the iterative learning control framework for the pre-inverse estimation and predistortion of power amplifiers. The iterative learning control algorithm is used to obtain a high quality predistorted input for the power amplifier under study without requiring a nonlinear model of the power amplifier. In a second step a nonlinear pre-inverse model of the amplifier is obtained. Both the nonlinear and memory effects of a power amplifier can be compensated by this approach. The convergence of the iterative approach, and the predistortion results are illustrated on a simulation of a Motorola LDMOS transistor based power amplifier and a measurement example using the Chalmers RF WebLab measurement setup.},
	number = {11},
	urldate = {2019-10-03},
	journal = {IEEE Transactions on Microwave Theory and Techniques},
	author = {Schoukens, Maarten and Hammenecker, Jules and Cooman, Adam},
	month = nov,
	year = {2017},
	note = {arXiv: 1606.08663},
	keywords = {Electrical Engineering and Systems Science - Systems and Control},
	pages = {4266--4273},
	file = {arXiv\:1606.08663 PDF:/Users/ivannazarov/Zotero/storage/REJ8YR5N/Schoukens et al. - 2017 - Obtaining the Pre-Inverse of a Power Amplifier usi.pdf:application/pdf;arXiv.org Snapshot:/Users/ivannazarov/Zotero/storage/N283JRF9/1606.html:text/html}
}

@article{traverso_low_2019,
	title = {Low {Complexity} {Time} {Synchronization} {Based} on {Digital} {Predistortion} {Coefficients}},
	volume = {29},
	doi = {10.1109/LMWC.2019.2895544},
	abstract = {In this letter, we propose an efficient loop-delay estimation approach for digital predistortion (DPD) based on the DPD linear coefficients. The basic concept of our approach consists in considering that the linear coefficients as computed by the DPD act as a linear filter that represents the linear response of the power amplifier, providing very accurate information about loop delay. Our approach has the great advantage of avoiding the need of a dedicated high-precision loop-delay estimator while using DPD. Two different estimators based on the DPD linear coefficients are presented, and their performance characteristics are illustrated on a measurement example using the Chalmers RF WebLab measurement setup.},
	number = {3},
	journal = {IEEE Microwave and Wireless Components Letters},
	author = {Traverso, S. and Bernier, J.},
	month = mar,
	year = {2019},
	keywords = {Baseband, Chalmers RF WebLab measurement setup, Correlation, delay estimation, Delay estimation, delays, Delays, digital predistortion, digital predistortion (DPD), distortion, DPD linear coefficients, efficient loop-delay estimation approach, Estimation, high-precision loop-delay estimator, Indexes, linear filter, linear response, linearisation techniques, loop delay, low complexity time synchronization, performance characteristics, power amplifier, power amplifiers, synchronisation, Synchronization, Wireless communication},
	pages = {240--242},
	file = {IEEE Xplore Abstract Record:/Users/ivannazarov/Zotero/storage/B6XEJIFN/8637798.html:text/html;IEEE Xplore Full Text PDF:/Users/ivannazarov/Zotero/storage/Q6MUDADG/Traverso and Bernier - 2019 - Low Complexity Time Synchronization Based on Digit.pdf:application/pdf}
}

@article{lapidoth_capacity_2003,
	title = {Capacity bounds via duality with applications to multiple-antenna systems on flat-fading channels},
	volume = {49},
	url = {http://moser-isi.ethz.ch/docs/papers/alap-smos-2003-3.pdf},
	doi = {10.1109/TIT.2003.817449},
	abstract = {A technique is proposed for the derivation of upper bounds on channel capacity. It is based on a dual expression for channel capacity where the maximization (of mutual information) over distributions on the channel input alphabet is replaced with a minimization (of average relative entropy) over distributions on the channel output alphabet. We also propose a technique for the analysis of the asymptotic capacity of cost-constrained channels. The technique is based on the observation that under fairly mild conditions capacity achieving input distributions "escape to infinity." The above techniques are applied to multiple-antenna flat-fading channels with memory where the realization of the fading process is unknown at the transmitter and unknown (or only partially known) at the receiver. It is demonstrated that, for high signal-to-noise ratio (SNR), the capacity of such channels typically grows only double-logarithmically in the SNR. To better understand this phenomenon and the rates at which it occurs, we introduce the fading number as the second-order term in the high-SNR asymptotic expansion of capacity, and derive estimates on its value for various systems. It is suggested that at rates that are significantly higher than the fading number, communication becomes extremely power inefficient, thus posing a practical limit on practically achievable rates. Upper and lower bounds on the fading number are also presented. For single-input-single-output (SISO) systems the bounds coincide, thus yielding a complete characterization of the fading number for general stationary and ergodic fading processes. We also demonstrate that for memoryless multiple-input single-output (MISO) channels, the fading number is achievable using beam-forming, and we derive an expression for the optimal beam direction. This direction depends on the fading law and is, in general, not the direction that maximizes the SNR on the induced SISO channel. Using a new closed-form expression for the expectation of the logarithm of a noncentral chi-square distributed random variable we provide some closed-form expressions for the fading number of some systems with Gaussian fading, including SISO systems with circularly symmetric stationary and ergodic Gaussian fading. The fading number of the latter is determined by the fading mean, fading variance, and the mean squared error in predicting the present fading from its past; it is not directly related to the Doppler spread. For the Rayleigh, Ricean, and multiple-antenna Rayleigh-fading channels we also present firm (nonasymptotic) upper and lower bounds on channel capacity. These bounds are asymptotically tight in the sense that their difference from capacity approaches zero at high SNR, and their ratio to capacity approaches one at low SNR.},
	number = {10},
	journal = {IEEE Transactions on Information Theory},
	author = {Lapidoth, Amos and Moser, Stefan M.},
	month = oct,
	year = {2003},
	keywords = {Signal to noise ratio, antenna arrays, antenna theory, asymptotic capacity, beam direction, beamforming, capacity bounds, channel capacity, Channel capacity, circularly symmetric stationary, circularly symmetric stationary fading, closed-form expression, Closed-form solution, cost-constrained channels, dual expression, duality, Entropy, ergodic fading processes, ergodic Gaussian fading, Fading, fading mean, fading variance, flat-fading channels, H infinity control, lower bounds, mean squared error, memoryless multiple-input single-output channels, MIMO systems, MISO channels, multiple-antenna systems, Mutual information, noncentral chi-square distributed random variable, Rayleigh channels, Ricean channels, Rician channels, second-order term, signal-to-noise ratio, single-input-single-output channel, SISO systems, stationary fading processes, Transmitters, Upper bound, upper bounds},
	pages = {2426--2467},
	file = {IEEE Xplore Abstract Record:/Users/ivannazarov/Zotero/storage/PV67KSZD/1237131.html:text/html;Lapidoth and Moser - 2003 - Capacity bounds via duality with applications to m.pdf:/Users/ivannazarov/Zotero/storage/N8NA3TBL/Lapidoth and Moser - 2003 - Capacity bounds via duality with applications to m.pdf:application/pdf}
}

@article{tarver_design_2019,
	title = {Design and {Implementation} of a {Neural} {Network} {Based} {Predistorter} for {Enhanced} {Mobile} {Broadband}},
	url = {http://arxiv.org/abs/1907.00766},
	abstract = {Digital predistortion is the process of correcting for nonlinearities in the analog RF front-end of a wireless transmitter. These nonlinearities contribute to adjacent channel leakage, degrade the error vector magnitude of transmitted signals, and often force the transmitter to reduce its transmission power into a more linear but less power-efficient region of the device. Most predistortion techniques are based on polynomial models with an indirect learning architecture which have been shown to be overly sensitive to noise. In this work, we use neural network based predistortion with a novel neural network training method that avoids the indirect learning architecture and that shows significant improvements in both the adjacent channel leakage ratio and error vector magnitude. Moreover, we show that, by using a neural network based predistorter, we are able to achieve a 42\% reduction in latency and 9.6\% increase in throughput on an FPGA accelerator with 15\% fewer multiplications per sample when compared to a similarly performing memory-polynomial implementation.},
	urldate = {2019-09-23},
	journal = {arXiv:1907.00766 [eess]},
	author = {Tarver, Chance and Balatsoukas-Stimming, Alexios and Cavallaro, Joseph R.},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.00766},
	keywords = {Electrical Engineering and Systems Science - Signal Processing},
	file = {arXiv\:1907.00766 PDF:/Users/ivannazarov/Zotero/storage/V3HLZ9X5/Tarver et al. - 2019 - Design and Implementation of a Neural Network Base.pdf:application/pdf;arXiv.org Snapshot:/Users/ivannazarov/Zotero/storage/YQ4G7L3P/1907.html:text/html}
}

@article{hellings_measuring_2019,
	title = {Measuring impropriety in complex and real representations},
	volume = {164},
	issn = {0165-1684},
	url = {http://www.sciencedirect.com/science/article/pii/S0165168419301951},
	doi = {10.1016/j.sigpro.2019.05.030},
	abstract = {So-called improper signals, i.e., signals which are correlated with their complex conjugates, can occur in many signal processing applications such as communication systems, medical imaging, audio and speech processing, analysis of oceanographic data, and many more. Being aware of potential impropriety can be crucial whenever we model signals as complex random quantities since an appropriate treatment of improper signals, e.g., by widely linear filtering, can significantly improve the system performance. After a brief introduction into the fundamentals of improper signals, this article focuses on the problem of quantifying the impropriety of complex random vectors and gives a survey of various impropriety measures in both the composite real representation and the augmented complex formulation. Unlike in previous publications, these two frameworks are presented side by side to reveal the differences and common points between them. Moreover, their applicability is compared in several practical examples. As additional aspects, we consider the problem of testing for impropriety based on measurement data, and the differential entropy of Gaussian vectors as an impropriety measure in information theoretic studies. The article includes a tutorial-style introduction, a collection of important formulae, a comparison of various mathematical approaches, as well as some new reformulations.},
	urldate = {2019-09-20},
	journal = {Signal Processing},
	author = {Hellings, Christoph and Utschick, Wolfgang},
	month = nov,
	year = {2019},
	keywords = {Augmented complex formulation, Composite real representation, Differential entropy, Improper signals, Impropriety test, Widely linear filtering},
	pages = {267--283},
	file = {Hellings and Utschick - 2019 - Measuring impropriety in complex and real represen.pdf:/Users/ivannazarov/Zotero/storage/Z6NGD2DC/Hellings and Utschick - 2019 - Measuring impropriety in complex and real represen.pdf:application/pdf;ScienceDirect Snapshot:/Users/ivannazarov/Zotero/storage/6TN9LFLY/S0165168419301951.html:text/html}
}

@article{adali_complex-valued_2011,
	title = {Complex-{Valued} {Signal} {Processing}: {The} {Proper} {Way} to {Deal} {With} {Impropriety}},
	volume = {59},
	shorttitle = {Complex-{Valued} {Signal} {Processing}},
	doi = {10.1109/TSP.2011.2162954},
	abstract = {Complex-valued signals occur in many areas of science and engineering and are thus of fundamental interest. In the past, it has often been assumed, usually implicitly, that complex random signals are proper or circular. A proper complex random variable is uncorrelated with its complex conjugate, and a circular complex random variable has a probability distribution that is invariant under rotation in the complex plane. While these assumptions are convenient because they simplify computations, there are many cases where proper and circular random signals are very poor models of the underlying physics. When taking impropriety and noncircularity into account, the right type of processing can provide significant performance gains. There are two key ingredients in the statistical signal processing of complex-valued data: 1) utilizing the complete statistical characterization of complex-valued random signals; and 2) the optimization of real-valued cost functions with respect to complex parameters. In this overview article, we review the necessary tools, among which are widely linear transformations, augmented statistical descriptions, and Wirtinger calculus. We also present some selected recent developments in the field of complex-valued signal processing, addressing the topics of model selection, filtering, and source separation.},
	number = {11},
	journal = {IEEE Transactions on Signal Processing},
	author = {Adali, T. and Schreier, P. J. and Scharf, L. L.},
	month = nov,
	year = {2011},
	keywords = {Cost function, optimisation, Vectors, augmented statistical descriptions, Calculus, complex random signals, complex-valued signal processing, CR calculus, estimation, improper, independent component analysis, linear transformations, Materials, model selection, noncircular, optimization, probability distribution, Random variables, signal processing, Signal processing, statistical distributions, statistical signal processing, widely linear, Wirtinger calculus, Zirconium},
	pages = {5101--5125},
	file = {Citeseer - Full Text PDF:/Users/ivannazarov/Zotero/storage/93P87AS2/Adalı et al. - Complex-Valued Signal Processing The Proper Way t.pdf:application/pdf;Citeseer - Snapshot:/Users/ivannazarov/Zotero/storage/BNHT7G5I/summary.html:text/html;IEEE Xplore Abstract Record:/Users/ivannazarov/Zotero/storage/838AHKVW/5961645.html:text/html}
}

@article{kharitonov_variational_2018,
	title = {Variational {Dropout} via {Empirical} {Bayes}},
	url = {http://arxiv.org/abs/1811.00596},
	abstract = {We study the Automatic Relevance Determination procedure applied to deep neural networks. We show that ARD applied to Bayesian DNNs with Gaussian approximate posterior distributions leads to a variational bound similar to that of variational dropout, and in the case of a fixed dropout rate, objectives are exactly the same. Experimental results show that the two approaches yield comparable results in practice even when the dropout rates are trained. This leads to an alternative Bayesian interpretation of dropout and mitigates some of the theoretical issues that arise with the use of improper priors in the variational dropout model. Additionally, we explore the use of the hierarchical priors in ARD and show that it helps achieve higher sparsity for the same accuracy.},
	urldate = {2019-09-18},
	journal = {arXiv:1811.00596 [cs, stat]},
	author = {Kharitonov, Valery and Molchanov, Dmitry and Vetrov, Dmitry},
	month = nov,
	year = {2018},
	note = {arXiv: 1811.00596},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	file = {arXiv\:1811.00596 PDF:/Users/ivannazarov/Zotero/storage/VPQCYVWU/Kharitonov et al. - 2018 - Variational Dropout via Empirical Bayes.pdf:application/pdf;arXiv.org Snapshot:/Users/ivannazarov/Zotero/storage/HY3FB5HQ/1811.html:text/html}
}

@article{yan_generalized_2018,
	title = {Generalized {Proper} {Complex} {Gaussian} {Ratio} {Distribution} and {Its} {Application} to {Statistical} {Inference} for {Frequency} {Response} {Functions}},
	volume = {144},
	url = {https://ascelibrary.org/doi/abs/10.1061/%28ASCE%29EM.1943-7889.0001504},
	doi = {10.1061/(ASCE)EM.1943-7889.0001504},
	abstract = {The frequency response function governs many important processes. Defined as the quotients of the fast Fourier transform coefficients, frequency response functions can be modeled as ratio random variables in the complex domain. The circularly symmetric complex normal ratio distribution proposed previously is restricted to quantifying the uncertainties for the quotients of complex Gaussian random variables with zero mean. Such limitation motivates research to enlarge the group of probability distributions, allowing a wider scope of applicability. This study provides a theoretical proof for the closed-form of a generalized proper complex Gaussian ratio distribution using the principle of probability density transformation in tandem with an advanced integral technique. The equivalence between the distribution properties of complex ratio random variables and their counterparts in the real-valued domain is also proven. The generalized proper complex Gaussian ratio distribution is then used to infer the statistics of frequency response functions. Stochastic simulation and experimental study are used to demonstrate the goodness and efficiency of the proposed probabilistic model.},
	number = {9},
	journal = {Journal of Engineering Mechanics},
	author = {Yan, Wang-Ji and Ren, Wei-Xin},
	year = {2018},
	pages = {04018080}
}

@misc{noauthor_expected_nodate,
	title = {Expected {Logarithm} of a {Noncentral} {Chi}-{Square} {Random} {Variable}},
	url = {http://moser-isi.ethz.ch/explog.html},
	urldate = {2019-07-26},
	file = {Expected Logarithm of a Noncentral Chi-Square Random Variable:/Users/ivannazarov/Zotero/storage/VAYLKLQN/explog.html:text/html}
}

@article{louizos_learning_2017,
	title = {Learning {Sparse} {Neural} {Networks} through \${L}\_0\$ {Regularization}},
	url = {http://arxiv.org/abs/1712.01312},
	abstract = {We propose a practical method for \$L\_0\$ norm regularization for neural networks: pruning the network during training by encouraging weights to become exactly zero. Such regularization is interesting since (1) it can greatly speed up training and inference, and (2) it can improve generalization. AIC and BIC, well-known model selection criteria, are special cases of \$L\_0\$ regularization. However, since the \$L\_0\$ norm of weights is non-differentiable, we cannot incorporate it directly as a regularization term in the objective function. We propose a solution through the inclusion of a collection of non-negative stochastic gates, which collectively determine which weights to set to zero. We show that, somewhat surprisingly, for certain distributions over the gates, the expected \$L\_0\$ norm of the resulting gated weights is differentiable with respect to the distribution parameters. We further propose the {\textbackslash}emph\{hard concrete\} distribution for the gates, which is obtained by "stretching" a binary concrete distribution and then transforming its samples with a hard-sigmoid. The parameters of the distribution over the gates can then be jointly optimized with the original network parameters. As a result our method allows for straightforward and efficient learning of model structures with stochastic gradient descent and allows for conditional computation in a principled way. We perform various experiments to demonstrate the effectiveness of the resulting approach and regularizer.},
	urldate = {2019-07-22},
	journal = {arXiv:1712.01312 [cs, stat]},
	author = {Louizos, Christos and Welling, Max and Kingma, Diederik P.},
	month = dec,
	year = {2017},
	note = {arXiv: 1712.01312},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	file = {arXiv\:1712.01312 PDF:/Users/ivannazarov/Zotero/storage/IRL3ETE4/Louizos et al. - 2017 - Learning Sparse Neural Networks through \$L_0\$ Regu.pdf:application/pdf;arXiv.org Snapshot:/Users/ivannazarov/Zotero/storage/XHLFTFR3/1712.html:text/html}
}

@article{maddison_concrete_2016,
	title = {The {Concrete} {Distribution}: {A} {Continuous} {Relaxation} of {Discrete} {Random} {Variables}},
	shorttitle = {The {Concrete} {Distribution}},
	url = {http://arxiv.org/abs/1611.00712},
	abstract = {The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack useful reparameterizations due to the discontinuous nature of discrete states. In this work we introduce Concrete random variables---continuous relaxations of discrete random variables. The Concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, Concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate the effectiveness of Concrete relaxations on density estimation and structured prediction tasks using neural networks.},
	urldate = {2019-06-08},
	journal = {arXiv:1611.00712 [cs, stat]},
	author = {Maddison, Chris J. and Mnih, Andriy and Teh, Yee Whye},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.00712},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, queue},
	file = {arXiv\:1611.00712 PDF:/Users/ivannazarov/Zotero/storage/LT3MHHE3/Maddison et al. - 2016 - The Concrete Distribution A Continuous Relaxation.pdf:application/pdf;arXiv.org Snapshot:/Users/ivannazarov/Zotero/storage/MHLVTNSN/1611.html:text/html}
}

@article{trabelsi_deep_2017,
	title = {Deep {Complex} {Networks}},
	url = {http://arxiv.org/abs/1705.09792},
	abstract = {At present, the vast majority of building blocks, techniques, and architectures for deep learning are based on real-valued operations and representations. However, recent work on recurrent neural networks and older fundamental theoretical analysis suggests that complex numbers could have a richer representational capacity and could also facilitate noise-robust memory retrieval mechanisms. Despite their attractive properties and potential for opening up entirely new neural architectures, complex-valued deep neural networks have been marginalized due to the absence of the building blocks required to design such models. In this work, we provide the key atomic components for complex-valued deep neural networks and apply them to convolutional feed-forward networks and convolutional LSTMs. More precisely, we rely on complex convolutions and present algorithms for complex batch-normalization, complex weight initialization strategies for complex-valued neural nets and we use them in experiments with end-to-end training schemes. We demonstrate that such complex-valued models are competitive with their real-valued counterparts. We test deep complex models on several computer vision tasks, on music transcription using the MusicNet dataset and on Speech Spectrum Prediction using the TIMIT dataset. We achieve state-of-the-art performance on these audio-related tasks.},
	urldate = {2019-06-03},
	journal = {arXiv:1705.09792 [cs]},
	author = {Trabelsi, Chiheb and Bilaniuk, Olexa and Zhang, Ying and Serdyuk, Dmitriy and Subramanian, Sandeep and Santos, João Felipe and Mehri, Soroush and Rostamzadeh, Negar and Bengio, Yoshua and Pal, Christopher J.},
	month = may,
	year = {2017},
	note = {arXiv: 1705.09792},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, printed},
	file = {arXiv\:1705.09792 PDF:/Users/ivannazarov/Zotero/storage/I6HPMFT6/Trabelsi et al. - 2017 - Deep Complex Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/ivannazarov/Zotero/storage/AJS6T5BF/1705.html:text/html}
}

@incollection{kingma_variational_2015,
	title = {Variational {Dropout} and the {Local} {Reparameterization} {Trick}},
	url = {http://papers.nips.cc/paper/5666-variational-dropout-and-the-local-reparameterization-trick.pdf},
	urldate = {2019-06-03},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 28},
	publisher = {Curran Associates, Inc.},
	author = {Kingma, Durk P and Salimans, Tim and Welling, Max},
	editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
	year = {2015},
	keywords = {printed},
	pages = {2575--2583},
	file = {NIPS Full Text PDF:/Users/ivannazarov/Zotero/storage/9X5Y6NEQ/Kingma et al. - 2015 - Variational Dropout and the Local Reparameterizati.pdf:application/pdf;NIPS Snapshot:/Users/ivannazarov/Zotero/storage/D3863CBQ/5666-variational-dropout-and-the-local-reparameterization-trick.html:text/html}
}

@article{gale_state_2019,
	title = {The {State} of {Sparsity} in {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1902.09574},
	abstract = {We rigorously evaluate three state-of-the-art techniques for inducing sparsity in deep neural networks on two large-scale learning tasks: Transformer trained on WMT 2014 English-to-German, and ResNet-50 trained on ImageNet. Across thousands of experiments, we demonstrate that complex techniques (Molchanov et al., 2017; Louizos et al., 2017b) shown to yield high compression rates on smaller datasets perform inconsistently, and that simple magnitude pruning approaches achieve comparable or better results. Additionally, we replicate the experiments performed by (Frankle \& Carbin, 2018) and (Liu et al., 2018) at scale and show that unstructured sparse architectures learned through pruning cannot be trained from scratch to the same test set performance as a model trained with joint sparsification and optimization. Together, these results highlight the need for large-scale benchmarks in the field of model compression. We open-source our code, top performing model checkpoints, and results of all hyperparameter configurations to establish rigorous baselines for future work on compression and sparsification.},
	urldate = {2019-06-03},
	journal = {arXiv:1902.09574 [cs, stat]},
	author = {Gale, Trevor and Elsen, Erich and Hooker, Sara},
	month = feb,
	year = {2019},
	note = {arXiv: 1902.09574},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, printed},
	file = {arXiv\:1902.09574 PDF:/Users/ivannazarov/Zotero/storage/FCNZDP2H/Gale et al. - 2019 - The State of Sparsity in Deep Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/ivannazarov/Zotero/storage/KBCWQR35/1902.html:text/html}
}

@article{molchanov_variational_2017,
	title = {Variational {Dropout} {Sparsifies} {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1701.05369},
	abstract = {We explore a recently proposed Variational Dropout technique that provided an elegant Bayesian interpretation to Gaussian Dropout. We extend Variational Dropout to the case when dropout rates are unbounded, propose a way to reduce the variance of the gradient estimator and report first experimental results with individual dropout rates per weight. Interestingly, it leads to extremely sparse solutions both in fully-connected and convolutional layers. This effect is similar to automatic relevance determination effect in empirical Bayes but has a number of advantages. We reduce the number of parameters up to 280 times on LeNet architectures and up to 68 times on VGG-like networks with a negligible decrease of accuracy.},
	urldate = {2019-06-03},
	journal = {arXiv:1701.05369 [cs, stat]},
	author = {Molchanov, Dmitry and Ashukha, Arsenii and Vetrov, Dmitry},
	month = jan,
	year = {2017},
	note = {arXiv: 1701.05369},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, printed},
	file = {arXiv\:1701.05369 PDF:/Users/ivannazarov/Zotero/storage/8SCBTQQH/Molchanov et al. - 2017 - Variational Dropout Sparsifies Deep Neural Network.pdf:application/pdf;arXiv.org Snapshot:/Users/ivannazarov/Zotero/storage/J6DEYHHG/1701.html:text/html}
}

@phdthesis{sarroff_complex_2018,
	title = {Complex {Neural} {Networks} for {Audio}},
	abstract = {Audio is represented in two mathematically equivalent ways: the real-valued time domain (i.e., waveform) and the complex-valued frequency domain (i.e., spectrum). There are advantages to the frequency-domain representation, e.g., the human auditory system is known to process sound in the frequency-domain. Furthermore, linear time-invariant systems are convolved with sources in the time-domain, whereas they may be factorized in the frequency-domain. Neural networks have become rather useful when applied to audio tasks such as machine listening and audio synthesis, which are related by their dependencies on high quality acoustic models. They ideally encapsulate fine-scale temporal structure, such as that encoded in the phase of frequency-domain audio, yet there are no authoritative deep learning methods for complex audio. This manuscript is dedicated to addressing the shortcoming.},
	language = {en},
	school = {Dartmouth College},
	author = {Sarroff, Andy M},
	month = may,
	year = {2018},
	file = {Sarroff - Complex Neural Networks for Audio.pdf:/Users/ivannazarov/Zotero/storage/94MGKBCC/Sarroff - Complex Neural Networks for Audio.pdf:application/pdf}
}

@article{taubock_complex-valued_2012,
	title = {Complex-{Valued} {Random} {Vectors} and {Channels}: {Entropy}, {Divergence}, and {Capacity}},
	volume = {58},
	issn = {0018-9448, 1557-9654},
	shorttitle = {Complex-{Valued} {Random} {Vectors} and {Channels}},
	url = {http://ieeexplore.ieee.org/document/6142094/},
	doi = {10.1109/TIT.2012.2184638},
	abstract = {Recent research has demonstrated signiﬁcant achievable performance gains by exploiting circularity/noncircularity or properness/improperness of complex-valued signals. In this paper, we investigate the inﬂuence of these properties on important information theoretic quantities such as entropy, divergence, and capacity. We prove two maximum entropy theorems that strengthen previously known results. The proof of the ﬁrst maximum entropy theorem is based on the so-called circular analog of a given complex-valued random vector. The introduction of the circular analog is additionally supported by a characterization theorem that employs a minimum Kullback–Leibler divergence criterion. In the proof of the second maximum entropy theorem, results about the second-order structure of complex-valued random vectors are exploited. Furthermore, we address the capacity of multiple-input multiple-output (MIMO) channels. Regardless of the speciﬁc distribution of the channel parameters (noise vector and channel matrix, if modeled as random), we show that the capacity-achieving input vector is circular for a broad range of MIMO channels (including coherent and noncoherent scenarios). Finally, we investigate the situation of an improper and Gaussian distributed noise vector. We compute both capacity and capacity-achieving input vector and show that improperness increases capacity, provided that the complementary covariance matrix is exploited. Otherwise, a capacity loss occurs, for which we derive an explicit expression.},
	language = {en},
	number = {5},
	urldate = {2019-06-03},
	journal = {IEEE Transactions on Information Theory},
	author = {Taubock, Georg},
	month = may,
	year = {2012},
	pages = {2729--2744},
	file = {Taubock - 2012 - Complex-Valued Random Vectors and Channels Entrop.pdf:/Users/ivannazarov/Zotero/storage/ECG6YEF2/Taubock - 2012 - Complex-Valued Random Vectors and Channels Entrop.pdf:application/pdf}
}

@phdthesis{amin_complex-valued_2012,
	address = {Department of System Design Engineering},
	title = {Complex-{Valued} {Neural} {Networks}: {Learning} {Algorithms} and {Applications}},
	abstract = {Complex-valued data arise in various applications, such as radar and array signal processing, magnetic resonance imaging, communication systems, and processing data in the frequency domain. To deal with such data properly, neural networks are extended to the complex domain, referred to as complex-valued neural networks (CVNNs), allowing the network parameters to be complex numbers and the computations to follow the complex algebraic rules. Unlike the real-valued case, the nonlinear functions in the CVNNs do not have standard complex derivatives as the Cauchy-Riemann equations do not hold for them. Consequently, the traditional approach for deriving learning algorithms reformulates the problem in the real domain which is often tedious. In this thesis, we first develop a systematic and simpler approach using Wirtinger calculus to derive the learning algorithms in the CVNNs. It is shown that adopting three steps: (i) computing a pair of derivatives in the conjugate coordinate system, (ii) using coordinate transformation between real and conjugate coordinates, and (iii) organizing derivative computations through functional dependency graph greatly simplify the derivations. To illustrate, a gradient descent and LevenbergMarquardt algorithms are considered.

Although a single-layered network, referred to as functional link network (FLN), has been widely used in the real domain because of its simplicity and faster processing, no such study exists in the complex domain. In the FLN, the nonlinearity is endowed in the input layer by constructing linearly independent basis functions in addition to the original variables. We design a parsimonious complex-valued FLN (CFLN) using orthogonal least squares (OLS) method, where the basis functions are multivariate polynomial terms. It is observed that the OLS based CFLN yields simple structure with favorable performance comparing to the multilayer CVNNs in several applications.

It is well known and interesting that a complex-valued neuron can solve several nonlinearly separable problems, including the XOR, parity-n, and symmetry detection problems, which a real-valued neuron cannot. With this motivation, we perform an empirical study of classification performance of single-layered CVNNs on several real-world benchmark classification problems with two new activation functions. The experimental results exhibit that the classification performances of single-layered CVNNs are comparable to those of multilayer real-valued neural networks. Further enhancement of discrimination ability has been obtained using the ensemble approach.},
	language = {en},
	school = {University of Fukui},
	author = {Amin, Faijul},
	year = {2012},
	file = {Amin - Complex-Valued Neural Networks Learning Algorithm.pdf:/Users/ivannazarov/Zotero/storage/KJIZEIY3/Amin - Complex-Valued Neural Networks Learning Algorithm.pdf:application/pdf}
}

@article{jankowski_complex-valued_1996,
	title = {Complex-valued multistate neural associative memory},
	volume = {7},
	issn = {1045-9227},
	doi = {10.1109/72.548176},
	abstract = {A model of a multivalued associative memory is presented. This memory has the form of a fully connected attractor neural network composed of multistate complex-valued neurons. Such a network is able to perform the task of storing and recalling gray-scale images. It is also shown that the complex-valued fully connected neural network may be considered as a generalization of a Hopfield network containing real-valued neurons. A computational energy function is introduced and evaluated in order to prove network stability for asynchronous dynamics. Storage capacity as related to the number of accessible neuron states is also estimated.},
	number = {6},
	journal = {IEEE Transactions on Neural Networks},
	author = {Jankowski, S. and Lozowski, A. and Zurada, J. M.},
	month = nov,
	year = {1996},
	keywords = {Associative memory, asynchronous dynamics, complex-valued multistate neural associative memory, computational energy function, Computer networks, content-addressable storage, fully connected attractor neural network, Gray-scale, gray-scale image recall, gray-scale image storage, Hopfield network, Hopfield neural networks, Image coding, Image recognition, multivalued associative memory, network stability, Neural networks, Neurons, Stability, State estimation, storage capacity},
	pages = {1491--1496},
	file = {IEEE Xplore Abstract Record:/Users/ivannazarov/Zotero/storage/DRI8RCY8/548176.html:text/html;IEEE Xplore Full Text PDF:/Users/ivannazarov/Zotero/storage/MJAQ75F6/Jankowski et al. - 1996 - Complex-valued multistate neural associative memor.pdf:application/pdf}
}

@article{monning_evaluation_2018-1,
	title = {Evaluation of {Complex}-{Valued} {Neural} {Networks} on {Real}-{Valued} {Classification} {Tasks}},
	url = {http://arxiv.org/abs/1811.12351},
	abstract = {Complex-valued neural networks are not a new concept, however, the use of real-valued models has often been favoured over complex-valued models due to difficulties in training and performance. When comparing real-valued versus complex-valued neural networks, existing literature often ignores the number of parameters, resulting in comparisons of neural networks with vastly different sizes. We find that when real and complex neural networks of similar capacity are compared, complex models perform equal to or slightly worse than real-valued models for a range of real-valued classification tasks. The use of complex numbers allows neural networks to handle noise on the complex plane. When classifying real-valued data with a complex-valued neural network, the imaginary parts of the weights follow their real parts. This behaviour is indicative for a task that does not require a complex-valued model. We further investigated this in a synthetic classification task. We can transfer many activation functions from the real to the complex domain using different strategies. The weight initialisation of complex neural networks, however, remains a significant problem.},
	urldate = {2019-06-03},
	journal = {arXiv:1811.12351 [cs, stat]},
	author = {Mönning, Nils and Manandhar, Suresh},
	month = nov,
	year = {2018},
	note = {arXiv: 1811.12351},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	file = {arXiv\:1811.12351 PDF:/Users/ivannazarov/Zotero/storage/HBAVPNXV/Mönning and Manandhar - 2018 - Evaluation of Complex-Valued Neural Networks on Re.pdf:application/pdf;arXiv.org Snapshot:/Users/ivannazarov/Zotero/storage/TEGL2NBS/1811.html:text/html}
}

@article{pav_moments_2015,
	title = {Moments of the log non-central chi-square distribution},
	url = {http://arxiv.org/abs/1503.06266},
	abstract = {The cumulants and moments of the log of the non-central chi-square distribution are derived. For example, the expected log of a chi-square random variable with v degrees of freedom is log(2) + psi(v/2). Applications to modeling probability distributions are discussed.},
	urldate = {2019-06-03},
	journal = {arXiv:1503.06266 [math, stat]},
	author = {Pav, Steven E.},
	month = mar,
	year = {2015},
	note = {arXiv: 1503.06266},
	keywords = {Mathematics - Probability, Statistics - Applications, 60E07},
	file = {arXiv\:1503.06266 PDF:/Users/ivannazarov/Zotero/storage/B38HCG2W/Pav - 2015 - Moments of the log non-central chi-square distribu.pdf:application/pdf;arXiv.org Snapshot:/Users/ivannazarov/Zotero/storage/HHNRLD9X/1503.html:text/html}
}

@phdthesis{wang_study_2017,
	title = {Study on complexity reduction of digital predistortion for power amplifier linearization},
	abstract = {This dissertation contributes to the linearization techniques of high power amplifier using digital predistortion method. High power amplifier is one of the most nonlinear components in radio transmitters. Baseband adaptive digital predistortion is a powerful technique to linearize the power amplifiers and allows to push the power amplifier operation point towards its high efficiency region. Linearization of power amplifiers using digital predistortion with low complexities is the focus of this dissertation. An algorithm is proposed to determine an optimal model structure of single-stage or multi-stage predistorter according to a trade-off between modeling accuracy and model complexity. Multi-stage cascaded digital predistortions are studied with different identification methods, which have advantages on complexity of model identification compared with single-stage structure. In terms of experimental implementations, this dissertation studies the impact of different gain choices on linearized power amplifier. All studies are evaluated with a Doherty power amplifier.},
	language = {en},
	school = {Universit{\'e} Paris-Est},
	author = {Wang, Siqi},
	month = dec,
	year = {2017},
	file = {Wang - Study on complexity reduction of digital predistor.pdf:/Users/ivannazarov/Zotero/storage/8UG9NXWV/Wang - Study on complexity reduction of digital predistor.pdf:application/pdf}
}

@misc{moore_introduction_2006,
	title = {An {Introduction} to {Iterative} {Learning} {Control} {Theory}},
	language = {en},
	author = {Moore, Kevin L},
	year = {2006},
	file = {Moore - 2006 - An Introduction to Iterative Learning Control Theo.pdf:/Users/ivannazarov/Zotero/storage/GSCEACYQ/Moore - 2006 - An Introduction to Iterative Learning Control Theo.pdf:application/pdf}
}

@phdthesis{chani-cahuana_digital_2015,
	address = {Department of Signals and Systems},
	title = {Digital {Predistortion} for the {Linearization} of {Power} {Amplifiers}},
	abstract = {High efficiency and linearity are indispensable requirements of power amplifiers. Unfortunately they are difficult to obtain simultaneously, since high efficiency PAs are nonlinear and linear PAs may have low efficiency. In order to satisfy the efficiency and linearity requirements, designers preferred to prioritize the efficiency of PAs in the design process and to later recover the linearity using external linearization techniques or architectures. Among the linearization techniques proposed in the literature, digital predistortion (DPD) has drawn the most attention of the industrial and academic sectors because it can provide a good compromise between linearity performance and implementation complexity. This thesis investigates digital predistortion techniques to suppress nonlinear distortion in radio transmitters.},
	language = {en},
	school = {Chalmers University of Technology},
	author = {Chani-Cahuana, Jessica},
	year = {2015},
	file = {Chani-Cahuana - Digital Predistortion for the Linearization of Pow.pdf:/Users/ivannazarov/Zotero/storage/5N9FB9E6/Chani-Cahuana - Digital Predistortion for the Linearization of Pow.pdf:application/pdf}
}

@misc{noauthor_bandpower_nodate,
	title = {Bandpower of an {EEG} signal},
	url = {https://raphaelvallat.com/bandpower.html},
	urldate = {2019-06-03},
	file = {Bandpower of an EEG signal:/Users/ivannazarov/Zotero/storage/A98FDWYL/bandpower.html:text/html}
}

@misc{karseras_caution:_2014,
	title = {Caution: {The} {Complex} {Normal} {Distribution} !},
	language = {en},
	author = {Karseras, Evripidis},
	month = may,
	year = {2014},
	file = {Karseras - Caution The Complex Normal Distribution !.pdf:/Users/ivannazarov/Zotero/storage/UMHXS9J7/Karseras - Caution The Complex Normal Distribution !.pdf:application/pdf}
}

@techreport{hunger_introduction_2007,
	title = {An {Introduction} to {Complex} {Differentials} and {Complex} {Differentiability}},
	abstract = {This technical report gives a brief introduction to some elements of complex function theory. First, general definitions for complex differentiability and holomorphic functions are presented. Since non-analytic functions are not complex differentiable, the concept of differentials is explained both for complex-valued and real-valued mappings. Finally, multivariate differentials and Wirtinger derivatives are investigated.},
	language = {en},
	number = {TUM-LNS-TR-07-06},
	institution = {Technische Universit{\:a}t M{\:u}nchen},
	author = {Hunger, Raphael},
	year = {2007},
	pages = {20},
	file = {Hunger - An Introduction to Complex Differentials and Compl.pdf:/Users/ivannazarov/Zotero/storage/QDSY8L7K/Hunger - An Introduction to Complex Differentials and Compl.pdf:application/pdf}
}

@inproceedings{cohen_group_2016,
	title = {Group {Equivariant} {Convolutional} {Networks}},
	url = {http://proceedings.mlr.press/v48/cohenc16.html},
	abstract = {We introduce Group equivariant Convolutional Neural Networks (G-CNNs), a natural generalization of convolutional neural networks that reduces sample complexity by exploiting symmetries. G-CNNs use ...},
	language = {en},
	urldate = {2019-06-03},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Cohen, Taco and Welling, Max},
	month = jun,
	year = {2016},
	pages = {2990--2999},
	file = {Full Text PDF:/Users/ivannazarov/Zotero/storage/238EEAPT/Cohen and Welling - 2016 - Group Equivariant Convolutional Networks.pdf:application/pdf;Snapshot:/Users/ivannazarov/Zotero/storage/JI5RDJJY/cohenc16.html:text/html}
}


@book{petersen_matrix_2012,
	title = {The {Matrix} {Cookbook}},
	abstract = {These pages are a collection of facts (identities, approximations, inequalities, relations, ...) about matrices and matters relating to them. It is collected in this form for the convenience of anyone who wants a quick desktop reference.},
	language = {en},
	author = {Petersen, Kaare Brandt and Pedersen, Michael Syskind},
	month = nov,
	year = {2012},
	file = {Petersen and Pedersen - [ httpmatrixcookbook.com ].pdf:/Users/ivannazarov/Zotero/storage/7MBQC6ED/Petersen and Pedersen - [ httpmatrixcookbook.com ].pdf:application/pdf}
}


@article{kingma_auto-encoding_2014,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	urldate = {2019-11-04},
	journal = {arXiv:1312.6114 [cs, stat]},
	author = {Kingma, Diederik P. and Welling, Max},
	month = may,
	year = {2014},
	note = {arXiv: 1312.6114},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/ivannazarov/Zotero/storage/HWZK2M3Y/Kingma and Welling - 2014 - Auto-Encoding Variational Bayes.pdf:application/pdf;arXiv.org Snapshot:/Users/ivannazarov/Zotero/storage/NLIRQZIH/1312.html:text/html}
}

@article{figurnov_implicit_2019,
	title = {Implicit {Reparameterization} {Gradients}},
	url = {http://arxiv.org/abs/1805.08498},
	abstract = {By providing a simple and efficient way of computing low-variance gradients of continuous random variables, the reparameterization trick has become the technique of choice for training a variety of latent variable models. However, it is not applicable to a number of important continuous distributions. We introduce an alternative approach to computing reparameterization gradients based on implicit differentiation and demonstrate its broader applicability by applying it to Gamma, Beta, Dirichlet, and von Mises distributions, which cannot be used with the classic reparameterization trick. Our experiments show that the proposed approach is faster and more accurate than the existing gradient estimators for these distributions.},
	urldate = {2019-11-04},
	journal = {arXiv:1805.08498 [cs, stat]},
	author = {Figurnov, Michael and Mohamed, Shakir and Mnih, Andriy},
	month = jan,
	year = {2019},
	note = {arXiv: 1805.08498},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/ivannazarov/Zotero/storage/ANEUWL3V/Figurnov et al. - 2019 - Implicit Reparameterization Gradients.pdf:application/pdf;arXiv.org Snapshot:/Users/ivannazarov/Zotero/storage/X2GZKD6M/1805.html:text/html}
}

@article{ranganath_operator_2018,
	title = {Operator {Variational} {Inference}},
	url = {http://arxiv.org/abs/1610.09033},
	abstract = {Variational inference is an umbrella term for algorithms which cast Bayesian inference as optimization. Classically, variational inference uses the Kullback-Leibler divergence to define the optimization. Though this divergence has been widely used, the resultant posterior approximation can suffer from undesirable statistical properties. To address this, we reexamine variational inference from its roots as an optimization problem. We use operators, or functions of functions, to design variational objectives. As one example, we design a variational objective with a Langevin-Stein operator. We develop a black box algorithm, operator variational inference (OPVI), for optimizing any operator objective. Importantly, operators enable us to make explicit the statistical and computational tradeoffs for variational inference. We can characterize different properties of variational objectives, such as objectives that admit data subsampling---allowing inference to scale to massive data---as well as objectives that admit variational programs---a rich class of posterior approximations that does not require a tractable density. We illustrate the benefits of OPVI on a mixture model and a generative model of images.},
	urldate = {2019-11-04},
	journal = {arXiv:1610.09033 [cs, stat]},
	author = {Ranganath, Rajesh and Altosaar, Jaan and Tran, Dustin and Blei, David M.},
	month = mar,
	year = {2018},
	note = {arXiv: 1610.09033},
	keywords = {Computer Science - Machine Learning, Statistics - Computation, Statistics - Machine Learning, Statistics - Methodology},
	file = {arXiv Fulltext PDF:/Users/ivannazarov/Zotero/storage/9E47IFZY/Ranganath et al. - 2018 - Operator Variational Inference.pdf:application/pdf;arXiv.org Snapshot:/Users/ivannazarov/Zotero/storage/ISJSAREL/1610.html:text/html}
}

@article{jang_categorical_2017,
	title = {Categorical {Reparameterization} with {Gumbel}-{Softmax}},
	url = {http://arxiv.org/abs/1611.01144},
	abstract = {Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.},
	urldate = {2019-11-04},
	journal = {arXiv:1611.01144 [cs, stat]},
	author = {Jang, Eric and Gu, Shixiang and Poole, Ben},
	month = aug,
	year = {2017},
	note = {arXiv: 1611.01144},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{yang_complex_2019,
	title = {Complex {Transformer}: {A} {Framework} for {Modeling} {Complex}-{Valued} {Sequence}},
	shorttitle = {Complex {Transformer}},
	url = {http://arxiv.org/abs/1910.10202},
	abstract = {While deep learning has received a surge of interest in a variety of fields in recent years, major deep learning models barely use complex numbers. However, speech, signal and audio data are naturally complex-valued after Fourier Transform, and studies have shown a potentially richer representation of complex nets. In this paper, we propose a Complex Transformer, which incorporates the transformer model as a backbone for sequence modeling; we also develop attention and encoder-decoder network operating for complex input. The model achieves state-of-the-art performance on the MusicNet dataset and an In-phase Quadrature (IQ) signal dataset.},
	urldate = {2019-11-04},
	journal = {arXiv:1910.10202 [cs, eess, stat]},
	author = {Yang, Muqiao and Ma, Martin Q. and Li, Dongyu and Tsai, Yao-Hung Hubert and Salakhutdinov, Ruslan},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.10202},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/ivannazarov/Zotero/storage/AVGG2K57/Yang et al. - 2019 - Complex Transformer A Framework for Modeling Comp.pdf:application/pdf;arXiv.org Snapshot:/Users/ivannazarov/Zotero/storage/2YGNBBLQ/1910.html:text/html}
}

@article{benvenuto_complex_1992,
	title = {On the complex backpropagation algorithm},
	volume = {40},
	issn = {1053-587X, 1941-0476},
	doi = {10.1109/78.127967},
	abstract = {A recursive algorithm for updating the coefficients of a neural network structure for complex signals is presented. Various complex activation functions are considered and a practical definition is proposed. The method, associated to a mean-square-error criterion, yields the complex form of the conventional backpropagation algorithm.{\textless}{\textgreater}},
	number = {4},
	journal = {IEEE Transactions on Signal Processing},
	author = {Benvenuto, N. and Piazza, F.},
	month = apr,
	year = {1992},
	keywords = {Backpropagation algorithms, coefficients updating, complex activation functions, complex backpropagation algorithm, complex signals, Equations, mean-square-error criterion, Multi-layer neural network, Multilayer perceptrons, neural nets, neural network, Neural networks, Neurons, Nonlinear filters, recursive algorithm, signal processing, Signal processing, Signal processing algorithms, Wiener filter},
	pages = {967--969},
	file = {IEEE Xplore Abstract Record:/Users/ivannazarov/Zotero/storage/CZVBGWES/127967.html:text/html}
}

@article{williams_simple_1992,
	title = {Simple statistical gradient-following algorithms for connectionist reinforcement learning},
	volume = {8},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/BF00992696},
	doi = {10.1007/BF00992696},
	abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
	language = {en},
	number = {3},
	urldate = {2019-11-05},
	journal = {Machine Learning},
	author = {Williams, Ronald J.},
	month = may,
	year = {1992},
	keywords = {connectionist networks, gradient descent, mathematical analysis, Reinforcement learning},
	pages = {229--256},
	file = {Springer Full Text PDF:/Users/ivannazarov/Zotero/storage/QWQ47BUX/Williams - 1992 - Simple statistical gradient-following algorithms f.pdf:application/pdf}
}

@article{hinton_improving_2012,
	title = {Improving neural networks by preventing co-adaptation of feature detectors},
	url = {http://arxiv.org/abs/1207.0580},
	abstract = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.},
	urldate = {2019-11-05},
	journal = {arXiv:1207.0580 [cs]},
	author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
	month = jul,
	year = {2012},
	note = {arXiv: 1207.0580},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/ivannazarov/Zotero/storage/VX75V4II/Hinton et al. - 2012 - Improving neural networks by preventing co-adaptat.pdf:application/pdf;arXiv.org Snapshot:/Users/ivannazarov/Zotero/storage/9LQQI6RK/1207.html:text/html}
}

@article{srivastava_dropout_2014,
	title = {Dropout: {A} {Simple} {Way} to {Prevent} {Neural} {Networks} from {Overfitting}},
	volume = {15},
	shorttitle = {Dropout},
	url = {http://jmlr.org/papers/v15/srivastava14a.html},
	urldate = {2019-11-05},
	journal = {Journal of Machine Learning Research},
	author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	year = {2014},
	pages = {1929--1958},
	file = {Full Text PDF:/Users/ivannazarov/Zotero/storage/2HLLYYJX/Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks f.pdf:application/pdf;Snapshot:/Users/ivannazarov/Zotero/storage/T5RDFIIE/srivastava14a.html:text/html}
}

@inproceedings{wan_regularization_2013,
	title = {Regularization of {Neural} {Networks} using {DropConnect}},
	url = {http://proceedings.mlr.press/v28/wan13.html},
	abstract = {We introduce DropConnect, a generalization of DropOut, for regularizing large fully-connected layers within neural networks. When training with Dropout, a randomly selected subset of activations ar...},
	language = {en},
	urldate = {2019-11-05},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Wan, Li and Zeiler, Matthew and Zhang, Sixin and Cun, Yann Le and Fergus, Rob},
	month = feb,
	year = {2013},
	pages = {1058--1066},
	file = {Full Text PDF:/Users/ivannazarov/Zotero/storage/2DU67PFA/Wan et al. - 2013 - Regularization of Neural Networks using DropConnec.pdf:application/pdf;Snapshot:/Users/ivannazarov/Zotero/storage/67X4MMXB/wan13.html:text/html}
}

@inproceedings{wang_fast_2013,
	title = {Fast dropout training},
	url = {http://proceedings.mlr.press/v28/wang13a.html},
	abstract = {Preventing feature co-adaptation by encouraging independent contributions from different features often improves classification and regression performance.  Dropout training (Hinton et al., 2012) d...},
	language = {en},
	urldate = {2019-11-05},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Wang, Sida and Manning, Christopher},
	month = feb,
	year = {2013},
	pages = {118--126},
	file = {Full Text PDF:/Users/ivannazarov/Zotero/storage/GFRB43XT/Wang and Manning - 2013 - Fast dropout training.pdf:application/pdf;Snapshot:/Users/ivannazarov/Zotero/storage/8A4QEX8B/wang13a.html:text/html}
}

@article{cheung_superposition_2019,
	title = {Superposition of many models into one},
	url = {http://arxiv.org/abs/1902.05522},
	abstract = {We present a method for storing multiple models within a single set of parameters. Models can coexist in superposition and still be retrieved individually. In experiments with neural networks, we show that a surprisingly large number of models can be effectively stored within a single parameter instance. Furthermore, each of these models can undergo thousands of training steps without significantly interfering with other models within the superposition. This approach may be viewed as the online complement of compression: rather than reducing the size of a network after training, we make use of the unrealized capacity of a network during training.},
	urldate = {2019-11-21},
	journal = {arXiv:1902.05522 [cs]},
	author = {Cheung, Brian and Terekhov, Alex and Chen, Yubei and Agrawal, Pulkit and Olshausen, Bruno},
	month = jun,
	year = {2019},
	note = {arXiv: 1902.05522},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/ivannazarov/Zotero/storage/KMQV67SY/Cheung et al. - 2019 - Superposition of many models into one.pdf:application/pdf;arXiv.org Snapshot:/Users/ivannazarov/Zotero/storage/PIAZRYKL/1902.html:text/html}
}

@inproceedings{titsias_doubly_2014,
	title = {Doubly {Stochastic} {Variational} {Bayes} for non-{Conjugate} {Inference}},
	url = {http://proceedings.mlr.press/v32/titsias14.html},
	abstract = {We propose a simple and effective variational inference algorithm based on stochastic optimisation   that can be widely applied for Bayesian non-conjugate inference in continuous parameter spaces. ...},
	language = {en},
	urldate = {2019-11-14},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Titsias, Michalis and Lázaro-Gredilla, Miguel},
	month = jan,
	year = {2014},
	pages = {1971--1979},
	file = {Full Text PDF:/Users/ivannazarov/Zotero/storage/AD3QK4W5/Titsias and Lázaro-Gredilla - 2014 - Doubly Stochastic Variational Bayes for non-Conjug.pdf:application/pdf;Snapshot:/Users/ivannazarov/Zotero/storage/YWSYXLEZ/titsias14.html:text/html}
}

@article{zuo_compression_2019,
	title = {On {Compression} of {Unsupervised} {Neural} {Nets} by {Pruning} {Weak} {Connections}},
	url = {http://arxiv.org/abs/1901.07066},
	abstract = {Unsupervised neural nets such as Restricted Boltzmann Machines(RBMs) and Deep Belif Networks(DBNs), are powerful in automatic feature extraction,unsupervised weight initialization and density estimation. In this paper,we demonstrate that the parameters of these neural nets can be dramatically reduced without affecting their performance. We describe a method to reduce the parameters required by RBM which is the basic building block for deep architectures. Further we propose an unsupervised sparse deep architectures selection algorithm to form sparse deep neural networks.Experimental results show that there is virtually no loss in either generative or discriminative performance.},
	urldate = {2019-11-15},
	journal = {arXiv:1901.07066 [cs]},
	author = {Zuo, Zhiwen and Zhao, Lei and Zuo, Liwen and Jiang, Feng and Xing, Wei and Lu, Dongming},
	month = jan,
	year = {2019},
	note = {arXiv: 1901.07066},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/ivannazarov/Zotero/storage/PC46U4UQ/Zuo et al. - 2019 - On Compression of Unsupervised Neural Nets by Prun.pdf:application/pdf;arXiv.org Snapshot:/Users/ivannazarov/Zotero/storage/EDTPL8GS/1901.html:text/html}
}

@inproceedings{he_amc:_2018,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{AMC}: {AutoML} for {Model} {Compression} and {Acceleration} on {Mobile} {Devices}},
	isbn = {978-3-030-01234-2},
	shorttitle = {{AMC}},
	doi = {10.1007/978-3-030-01234-2\_48},
	abstract = {Model compression is an effective technique to efficiently deploy neural network models on mobile devices which have limited computation resources and tight power budgets. Conventional model compression techniques rely on hand-crafted features and require domain experts to explore the large design space trading off among model size, speed, and accuracy, which is usually sub-optimal and time-consuming. In this paper, we propose AutoML for Model Compression (AMC) which leverages reinforcement learning to efficiently sample the design space and can improve the model compression quality. We achieved state-of-the-art model compression results in a fully automated way without any human efforts. Under 4××{\textbackslash}times FLOPs reduction, we achieved 2.7\% better accuracy than the hand-crafted model compression method for VGG-16 on ImageNet. We applied this automated, push-the-button compression pipeline to MobileNet-V1 and achieved a speedup of 1.53××{\textbackslash}times on the GPU (Titan Xp) and 1.95××{\textbackslash}times on an Android phone (Google Pixel 1), with negligible loss of accuracy.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {He, Yihui and Lin, Ji and Liu, Zhijian and Wang, Hanrui and Li, Li-Jia and Han, Song},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	keywords = {AutoML, CNN acceleration, Mobile vision, Model compression, Reinforcement learning},
	pages = {815--832},
	file = {Springer Full Text PDF:/Users/ivannazarov/Zotero/storage/8FKZZ96T/He et al. - 2018 - AMC AutoML for Model Compression and Acceleration.pdf:application/pdf}
}

@article{thickstun_learning_2017,
	title = {Learning {Features} of {Music} from {Scratch}},
	url = {http://arxiv.org/abs/1611.09827},
	abstract = {This paper introduces a new large-scale music dataset, MusicNet, to serve as a source of supervision and evaluation of machine learning methods for music research. MusicNet consists of hundreds of freely-licensed classical music recordings by 10 composers, written for 11 instruments, together with instrument/note annotations resulting in over 1 million temporal labels on 34 hours of chamber music performances under various studio and microphone conditions. The paper defines a multi-label classification task to predict notes in musical recordings, along with an evaluation protocol, and benchmarks several machine learning architectures for this task: i) learning from spectrogram features; ii) end-to-end learning with a neural net; iii) end-to-end learning with a convolutional neural net. These experiments show that end-to-end models trained for note prediction learn frequency selective filters as a low-level representation of audio.},
	urldate = {2019-11-24},
	journal = {arXiv:1611.09827 [cs, stat]},
	author = {Thickstun, John and Harchaoui, Zaid and Kakade, Sham},
	month = apr,
	year = {2017},
	note = {arXiv: 1611.09827},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Sound},
	file = {arXiv Fulltext PDF:/Users/ivannazarov/Zotero/storage/7YKJJ89W/Thickstun et al. - 2017 - Learning Features of Music from Scratch.pdf:application/pdf;arXiv.org Snapshot:/Users/ivannazarov/Zotero/storage/U9F348XZ/1611.html:text/html}
}

@article{aizenberg_multilayer_2007,
	title = {Multilayer feedforward neural network based on multi-valued neurons ({MLMVN}) and a backpropagation learning algorithm},
	volume = {11},
	abstract = {A multilayer neural network based on multi-valued neurons is considered in the paper. A multivalued neuron (MVN) is based on the principles of multiple-valued threshold logic over the field of the complex numbers. The most important properties of MVN are: the complex-valued weights, inputs and output coded by the k th roots of unity and the activation function, which maps the complex plane into the unit circle. MVN learning is reduced to the movement along the unit circle, it is based on a simple linear error correction rule and it does not require a derivative. It is shown that using a traditional architecture of multilayer feedforward neural network (MLF) and the high functionality of the multi-valued neuron, it is possible to obtain a new powerful neural network. Its training does not require a derivative of the activation function and its functionality is higher than the functionality of MLF containing the same number of layers and neurons. These advantages of MLMVN are confirmed by testing using parity n, two spirals and "sonar" benchmarks and the Mackey-Glass time series prediction.},
	number = {2},
	journal = {Soft Computing},
	author = {Aizenberg, Igor and Moraga, Claudio},
	year = {2007},
	pages = {2007},
	file = {Citeseer - Full Text PDF:/Users/ivannazarov/Zotero/storage/X8NCH3TB/Aizenberg and Moraga - 2007 - Multilayer feedforward neural network based on mul.pdf:application/pdf;Citeseer - Snapshot:/Users/ivannazarov/Zotero/storage/ET2S7LNP/summary.html:text/html}
}

@article{faijul_amin_single-layered_2009,
	series = {Brain {Inspired} {Cognitive} {Systems} ({BICS} 2006) / {Interplay} {Between} {Natural} and {Artificial} {Computation} ({IWINAC} 2007)},
	title = {Single-layered complex-valued neural network for real-valued classification problems},
	volume = {72},
	issn = {0925-2312},
	url = {http://www.sciencedirect.com/science/article/pii/S0925231208002439},
	doi = {10.1016/j.neucom.2008.04.006},
	abstract = {This paper presents a model of complex-valued neuron (CVN) for real-valued classification problems, introducing two new activation functions. In this CVN model, each real-valued input is encoded into a phase between 0 and π of a complex number of unity magnitude, and multiplied by a complex-valued weight. The weighted sum of inputs is then fed to an activation function. Both the proposed activation functions map complex values into real values, and their role is to divide the net-input (weighted sum) space into multiple regions representing the classes of input patterns. Gradient-based learning rules are derived for each of the activation functions. The ability of such CVN is discussed and tested with two-class problems, such as two- and three-input Boolean problems, and the symmetry detection in binary sequences. We show here that the CVN with both activation functions can form proper boundaries for these linear and nonlinear problems. For solving n-class problems, a complex-valued neural network (CVNN) consisting of n CVNs is also studied. We defined the one exhibiting the largest output among all the neurons as representing the output class. We tested such single-layered CVNNs on several real world benchmark problems. The results show that the classification ability of single-layered CVNN on unseen data is comparable to the conventional real-valued neural network (RVNN) having one hidden layer. Moreover, convergence of the CVNN is much faster than that of the RVNN in most cases.},
	language = {en},
	number = {4},
	urldate = {2019-11-22},
	journal = {Neurocomputing},
	author = {Faijul Amin, Md. and Murase, Kazuyuki},
	month = jan,
	year = {2009},
	keywords = {Activation function, Classification, Complex-valued neural networks, Generalization, Phase-encoding},
	pages = {945--955},
	file = {ScienceDirect Full Text PDF:/Users/ivannazarov/Zotero/storage/7PRZGAHF/Faijul Amin and Murase - 2009 - Single-layered complex-valued neural network for r.pdf:application/pdf;ScienceDirect Snapshot:/Users/ivannazarov/Zotero/storage/L3WKAPE7/S0925231208002439.html:text/html}
}

@inproceedings{wolter_complex_2018,
	address = {USA},
	series = {{NIPS}'18},
	title = {Complex {Gated} {Recurrent} {Neural} {Networks}},
	url = {http://dl.acm.org/citation.cfm?id=3327546.3327714},
	abstract = {Complex numbers have long been favoured for digital signal processing, yet complex representations rarely appear in deep learning architectures. RNNs, widely used to process time series and sequence information, could greatly benefit from complex representations. We present a novel complex gated recurrent cell, which is a hybrid cell combining complex-valued and norm-preserving state transitions with a gating mechanism. The resulting RNN exhibits excellent stability and convergence properties and performs competitively on the synthetic memory and adding task, as well as on the real-world tasks of human motion prediction.},
	urldate = {2019-11-22},
	booktitle = {Proceedings of the 32Nd {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Wolter, Moritz and Yao, Angela},
	year = {2018},
	note = {event-place: Montréal, Canada},
	pages = {10557--10567},
	file = {ACM Full Text PDF:/Users/ivannazarov/Zotero/storage/LKUFD8TC/Wolter and Yao - 2018 - Complex Gated Recurrent Neural Networks.pdf:application/pdf}
}

@inproceedings{hirose_complex-valued_2009,
	title = {Complex-valued neural networks: {The} merits and their origins},
	shorttitle = {Complex-valued neural networks},
	doi = {10.1109/IJCNN.2009.5178754},
	abstract = {This paper discusses what the merits of complex-valued neural networks (CVNNs) arise from. First we look back the mathematical history to elucidate the features of complex numbers, in particular to confirm the importance of the phase-and-amplitude viewpoint for designing and constructing CVNNs to enhance the features. The viewpoint is essential in general to deal with waves such as electromagnetic-wave and lightwave. Then we point out that, although we represent a complex number as an ordered pair of real numbers for example, we can reduce ineffective degree of freedom in learning or self-organization in CVNNs to achieve better generalization characteristics. This wave-oriented merit is useful widely for general signal processing with Fourier synthesis or in frequency-domain treatment through Fourier transform.},
	booktitle = {2009 {International} {Joint} {Conference} on {Neural} {Networks}},
	author = {Hirose, Akira},
	month = jun,
	year = {2009},
	note = {ISSN: 2161-4407},
	keywords = {signal processing, Neural networks, neural nets, complex numbers, complex-valued neural networks, electromagnetic-wave, Fourier synthesis, Fourier transform, Fourier transforms, frequency-domain analysis, frequency-domain treatment, general signal processing},
	pages = {1237--1244},
	file = {IEEE Xplore Abstract Record:/Users/ivannazarov/Zotero/storage/RC5XSTFG/5178754.html:text/html;IEEE Xplore Full Text PDF:/Users/ivannazarov/Zotero/storage/YYDZ4VQR/Hirose - 2009 - Complex-valued neural networks The merits and the.pdf:application/pdf}
}

@inproceedings{arjovsky_unitary_2016,
	title = {Unitary {Evolution} {Recurrent} {Neural} {Networks}},
	url = {http://proceedings.mlr.press/v48/arjovsky16.html},
	abstract = {Recurrent neural networks (RNNs) are notoriously difficult to train. When the eigenvalues of the hidden to hidden weight matrix deviate from absolute value 1, optimization becomes difficult due to ...},
	language = {en},
	urldate = {2019-11-22},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Arjovsky, Martin and Shah, Amar and Bengio, Yoshua},
	month = jun,
	year = {2016},
	pages = {1120--1128},
	file = {Full Text PDF:/Users/ivannazarov/Zotero/storage/AVWSC8VQ/Arjovsky et al. - 2016 - Unitary Evolution Recurrent Neural Networks.pdf:application/pdf;Snapshot:/Users/ivannazarov/Zotero/storage/S6AIYFXD/arjovsky16.html:text/html}
}

@incollection{wisdom_full-capacity_2016,
	title = {Full-{Capacity} {Unitary} {Recurrent} {Neural} {Networks}},
	url = {http://papers.nips.cc/paper/6327-full-capacity-unitary-recurrent-neural-networks.pdf},
	urldate = {2019-11-22},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 29},
	publisher = {Curran Associates, Inc.},
	author = {Wisdom, Scott and Powers, Thomas and Hershey, John and Le Roux, Jonathan and Atlas, Les},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	year = {2016},
	pages = {4880--4888},
	file = {NIPS Full Text PDF:/Users/ivannazarov/Zotero/storage/84WDH4SR/Wisdom et al. - 2016 - Full-Capacity Unitary Recurrent Neural Networks.pdf:application/pdf;NIPS Snapshot:/Users/ivannazarov/Zotero/storage/AHALCX4B/6327-full-capacity-unitary-recurrent-neural-networks.html:text/html}
}

@article{boeddeker_computation_2019,
	title = {On the {Computation} of {Complex}-valued {Gradients} with {Application} to {Statistically} {Optimum} {Beamforming}},
	url = {http://arxiv.org/abs/1701.00392},
	abstract = {This report describes the computation of gradients by algorithmic differentiation for statistically optimum beamforming operations. Especially the derivation of complex-valued functions is a key component of this approach. Therefore the real-valued algorithmic differentiation is extended via the complex-valued chain rule. In addition to the basic mathematic operations the derivative of the eigenvalue problem with complex-valued eigenvectors is one of the key results of this report. The potential of this approach is shown with experimental results on the CHiME-3 challenge database. There, the beamforming task is used as a front-end for an ASR system. With the developed derivatives a joint optimization of a speech enhancement and speech recognition system w.r.t. the recognition optimization criterion is possible.},
	urldate = {2019-12-02},
	journal = {arXiv:1701.00392 [cs]},
	author = {Boeddeker, Christoph and Hanebrink, Patrick and Drude, Lukas and Heymann, Jahn and Haeb-Umbach, Reinhold},
	month = feb,
	year = {2019},
	note = {arXiv: 1701.00392},
	keywords = {Computer Science - Computational Engineering, Finance, and Science, Mathematics - Numerical Analysis},
	file = {arXiv Fulltext PDF:/Users/ivannazarov/Zotero/storage/FFWWKDC7/Boeddeker et al. - 2019 - On the Computation of Complex-valued Gradients wit.pdf:application/pdf;arXiv.org Snapshot:/Users/ivannazarov/Zotero/storage/SQEB46XJ/1701.html:text/html}
}

@article{wolter_fourier_2019,
	title = {Fourier {RNNs} for {Sequence} {Prediction}},
	url = {http://arxiv.org/abs/1812.05645},
	abstract = {Fourier methods have a long and proven track record as an excellent tool in data processing. We propose to integrate Fourier methods into complex recurrent neural network architectures and show accuracy improvements on prediction tasks as well as computational load reductions. We predict synthetic data drawn from synthetic-equations as well as real world power load data.},
	urldate = {2019-12-02},
	journal = {arXiv:1812.05645 [cs, stat]},
	author = {Wolter, Moritz and Yao, Angela},
	month = may,
	year = {2019},
	note = {arXiv: 1812.05645},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/ivannazarov/Zotero/storage/UNN8IXRK/Wolter and Yao - 2019 - Fourier RNNs for Sequence Prediction.pdf:application/pdf;arXiv.org Snapshot:/Users/ivannazarov/Zotero/storage/X9W55VEW/1812.html:text/html}
}

@inproceedings{lin_fixed_2016,
	title = {Fixed {Point} {Quantization} of {Deep} {Convolutional} {Networks}},
	url = {http://proceedings.mlr.press/v48/linb16.html},
	abstract = {In recent years increasingly complex architectures for deep convolution networks (DCNs) have been proposed to boost the performance on image recognition tasks. However, the gains in performance hav...},
	language = {en},
	urldate = {2020-01-17},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Lin, Darryl and Talathi, Sachin and Annapureddy, Sreekanth},
	month = jun,
	year = {2016},
	pages = {2849--2858},
	file = {Full Text PDF:/Users/ivannazarov/Zotero/storage/HXX5HPBD/Lin et al. - 2016 - Fixed Point Quantization of Deep Convolutional Net.pdf:application/pdf;Snapshot:/Users/ivannazarov/Zotero/storage/TX4FJ2ZG/linb16.html:text/html}
}

@inproceedings{chen_fxpnet_2017,
	title = {{FxpNet}: {Training} a deep convolutional neural network in fixed-point representation},
	shorttitle = {{FxpNet}},
	doi = {10.1109/IJCNN.2017.7966159},
	abstract = {We introduce FxpNet, a framework to train deep convolutional neural networks with low bit-width arithmetics in both forward pass and backward pass. During training FxpNet further reduces the bit-width of stored parameters (also known as primal parameters) by adaptively updating their fixed-point formats. These primal parameters are usually represented in the full resolution of floating-point values in previous binarized and quantized neural networks. In FxpNet, during forward pass fixed-point primal weights and activations are first binarized before computation, while in backward pass all gradients are represented as low resolution fixed-point values and then accumulated to corresponding fixed-point primal parameters. To have highly efficient implementations in FPGAs, ASICs and other dedicated devices, FxpNet introduces Integer Batch Normalization (IBN) and Fixed-point ADAM (FxpADAM) methods to further reduce the required floating-point operations, which will save considerable power and chip area. The evaluation on CIFAR-10 dataset indicates the effectiveness that FxpNet with 12-bit primal parameters and 12-bit gradients achieves comparable prediction accuracy with state-of-the-art binarized and quantized neural networks.},
	booktitle = {2017 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Chen, Xi and Hu, Xiaolin and Zhou, Hucheng and Xu, Ningyi},
	month = may,
	year = {2017},
	note = {ISSN: 2161-4407},
	keywords = {12-bit gradients, 12-bit primal parameters, Acceleration, application specific integrated circuits, ASICs, backward pass, binarized neural networks, bit-width arithmetics, CIFAR-10 dataset, convolution, Convolution, deep convolutional neural network, field programmable gate arrays, Field programmable gate arrays, fixed point arithmetic, fixed-point ADAM, fixed-point primal parameters, fixed-point primal weights, fixed-point representation, floating point arithmetic, floating-point values, forward pass, FPGAs, FxpADAM, FxpNet, IBN, integer batch normalization, Kernel, low resolution fixed-point values, neural nets, Neural networks, Quantization (signal), quantized neural networks, Training},
	pages = {2494--2501},
	file = {IEEE Xplore Abstract Record:/Users/ivannazarov/Zotero/storage/TZJ7NK59/7966159.html:text/html;IEEE Xplore Full Text PDF:/Users/ivannazarov/Zotero/storage/FKJRRC3C/Chen et al. - 2017 - FxpNet Training a deep convolutional neural netwo.pdf:application/pdf}
}

@article{hinton_distilling_2015,
	title = {Distilling the {Knowledge} in a {Neural} {Network}},
	url = {http://arxiv.org/abs/1503.02531},
	abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
	urldate = {2020-01-17},
	journal = {arXiv:1503.02531 [cs, stat]},
	author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	month = mar,
	year = {2015},
	note = {arXiv: 1503.02531},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/ivannazarov/Zotero/storage/LQEPQVKJ/Hinton et al. - 2015 - Distilling the Knowledge in a Neural Network.pdf:application/pdf;arXiv.org Snapshot:/Users/ivannazarov/Zotero/storage/R2MDZY9Z/1503.html:text/html}
}

@article{uhlich_differentiable_2019,
	title = {Differentiable {Quantization} of {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1905.11452},
	abstract = {We propose differentiable quantization (DQ) for efficient deep neural network (DNN) inference where gradient descent is used to learn the quantizer's step size, dynamic range and bitwidth. Training with differentiable quantizers brings two main benefits: first, DQ does not introduce hyperparameters; second, we can learn for each layer a different step size, dynamic range and bitwidth. Our experiments show that DNNs with heterogeneous and learned bitwidth yield better performance than DNNs with a homogeneous one. Further, we show that there is one natural DQ parametrization especially well suited for training. We confirm our findings with experiments on CIFAR-10 and ImageNet and we obtain quantized DNNs with learned quantization parameters achieving state-of-the-art performance.},
	urldate = {2020-01-17},
	journal = {arXiv:1905.11452 [cs, stat]},
	author = {Uhlich, Stefan and Mauch, Lukas and Yoshiyama, Kazuki and Cardinaux, Fabien and Garcia, Javier Alonso and Tiedemann, Stephen and Kemp, Thomas and Nakamura, Akira},
	month = may,
	year = {2019},
	note = {arXiv: 1905.11452},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/ivannazarov/Zotero/storage/G6KB63EM/Uhlich et al. - 2019 - Differentiable Quantization of Deep Neural Network.pdf:application/pdf;arXiv.org Snapshot:/Users/ivannazarov/Zotero/storage/TSTRRHDE/1905.html:text/html}
}
